---
title: "9.2 Hypothesis Tests for Two Means"
author: "Dr. Lauren Perry"
output:
  ioslides_presentation: 
    widescreen: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 

What if we wanted to compare two means? We begin by discussing paired samples. This will feel very familiar, since it's essentially the same as hypothesis testing for a single mean. Then we will move on to independent samples, which will require a couple of adjustments. 

### Paired Samples

Sometimes there is a special correspondence between two sets of observations. We say that two sets of observations are **paired** if each observation has a natural connection with exactly one observation in the other data set. Consider the following data from 30 students given a pre- and post-test on a course concept:

| Student | Pre-Test | Post-Test |
| :-: | :-: | :-: | 
| 1 | 52 | 70 |
| 2 | 71 | 98 |
| 3 | 13 | 65 |
| $\dots$ | $\dots$ | $\dots$ |
| 30 | 48 | 81 |

The natural connection between "pre-test" and "post-test" is the student who took each test! Often, paired data will involve similar measures taken on the *same item or individual*. We *pair* these data because we want to compare two means, but we also want to account for the pairing. 

Why? Consider: If a student got a 13\% on the pre-test, I would love to see them get a 60\% on the post-test - that's a huge improvement! But if a student got an 82\% on the pre-test, I would *not* like to see them get a 60\% on the post-test. Pairing the data lets us account for this connection. 

So what do we do with paired data? Fortunately, this part is easy! We start by taking the difference between the two sets of observations. In the pre- and post-test example, I will take the pre-test score and subtract the post-test score:

| Student | Pre-Test | Post-Test | **Difference** | 
| :-: | :-: | :-: | :-: |
| 1 | 52 | 70 | **18** |
| 2 | 71 | 98 | **27** |
| 3 | 13 | 65 | **52** |
| $\dots$ | $\dots$ | $\dots$ | $\dots$
| 30 | 48 | 81 | **33** |

Then, we do a test of a *single mean* on the differences where 

- $H_0: \mu_{\text{d}} = 0$
- $H_A: \mu_{\text{d}} \ne 0$

Note that the subscript "d" denotes "difference". We will use the exact same test(s) as in the previous sections:

- **Large Sample Setting**: $\mu_{\text{d}}$ is target parameter, $n_{\text{d}} \ge 30$, $$z = \frac{\bar{x}_{\text{d}}}{s_{\text{d}}/\sqrt{n_{\text{d}}}}$$ and the p-value is $$2P(Z > |z|)$$ where $z$ is the test statistic.

- **Small Sample Setting**: $\mu_{\text{d}}$ is target parameter, $n_{\text{d}} < 30$, $$t = \frac{\bar{x}_{\text{d}}}{s_{\text{d}}/\sqrt{n_{\text{d}}}}$$ and the p-value is $$2P(t_{df} > |t|)$$ where $t$ is the test statistic.

Here, $n_{\text{d}}$ is the number of pairs.

Steps:

1. State the null and alternative hypotheses.
2. Determine the significance level $\alpha$. Check assumptions (decide which setting to use).
3. Compute the value of the test statistic.
4. Determine the critical values or p-value.
5. For the *critical value approach*: If the test statistic is in the rejection region, reject the null hypothesis. For the *p-value approach*: If $\text{p-value} < \alpha$, reject the null hypothesis. Otherwise, do not reject.
6. Interpret results. 

### Independent Samples

In **independent samples**, the sample from one population does not impact the sample from the other population. In short, we take two *separate samples* and compare them. 

- $H_0: \mu_1 = \mu_2 \quad \rightarrow \quad H_0: \mu_1 - \mu_2 = 0$
- $H_A: \mu_1 \ne \mu_2 \quad \rightarrow \quad H_A: \mu_1 - \mu_2 \ne 0$

If we use $\bar{x}$ to estimate $\mu$, intuitively we might use $\bar{x}_1-\bar{x}_2$ to estimate $\mu_1 - \mu_2$. To do this, we need to know something about the sampling distribution of $\bar{x}_1-\bar{x}_2$. 

Consider: if $X_1$ is Normal($\mu_1$, $\sigma_1$) and $X_2$ is Normal($\mu_2$,$\sigma_2$) with $\sigma_1$ and $\sigma_2$ are known, then for independent samples of size $n_1$ and $n_2$, 

- $\bar{X}_1-\bar{X}_2$ is Normal($\mu_{\bar{X}_1-\bar{X}_2}$, $\sigma_{\bar{X}_1-\bar{X}_2}$).
- $\mu_{\bar{X}_1-\bar{X}_2} = \mu_1 - \mu_2$
- $\sigma_{\bar{X}_1-\bar{X}_2} = \sigma_1 - \sigma_2$

so then $$Z = \frac{(\bar{X}_1 - \bar{X}_2) - (\mu_1 - \mu_2)}{\sqrt{\sigma_1/n_1 - \sigma_2/n_2}}$$ has a standard normal distribution. But, as we mentioned earlier, we rarely work in that setting where the population standard deviation is known. Instead, we will use $s_1$ and $s_2$ to estimate $\sigma_1$ and $\sigma_2$. For independent samples of size $n_1$ and $n_2$, $$t = \frac{(\bar{X}_1 - \bar{X}_2) - (\mu_1 - \mu_2)}{\sqrt{s_1/n_1 - s_2/n_2}}$$ has a t-distribution with degrees of freedom $$\Delta = \frac{[(s_1^2/n_1) + (s_2^2/n_2)]^2}{\frac{(s_1^2/n_1)^2}{n_1-1} + \frac{(s_2^2/n_2)^2}{n_2-1}}$$ rounded *down* to the nearest whole number. (Note that $\Delta$ is the uppercase Greek letter, "delta".) If $n_1 = n_2$, this simplifies to $$\Delta = (n-1)\left(\frac{(s_1^2 + s_2^2)^2}{s_1^4 + s_2^4}\right)$$

**Tip:** Generally, people do not calculate $\Delta$ by hand. Instead, we use a computer to do these kinds of tests. 

<center><font size='4'>**The Two-Sample T-Test**</center></font>

Assumptions:

- Simple random samples.
- Independent samples.
- Normal populations or large ($n \ge 30$) samples.

**Steps for Critical Value Approach**:

1. $H_0: \mu_1 - \mu_2 = 0$ and $H_A: \mu_1 - \mu_2 \ne 0$
2. Check assumptions; select the significance level $\alpha$.
3. Compute the test statistic $$t = \frac{\bar{x}_1 - \bar{x}_2}{\sqrt{s_1/n_1 - s_2/n_2}}$$ Note that we assume under the null hypothesis that $\mu_1 - \mu_2 = 0$, which is why we replace this quantity with $0$ in the test statistic.
4. The critical value is $\pm t_{df, \alpha/2}$ with $df = \Delta$. 
5. If the test statistic falls in the rejection region, reject the null hypothesis.
6. Interpret in the context of the problem. 

**Steps for P-Value Approach**:

1. $H_0: \mu_1 - \mu_2 = 0$ and $H_A: \mu_1 - \mu_2 \ne 0$
2. Check assumptions; select the significance level $\alpha$.
3. Compute the test statistic $$t = \frac{\bar{x}_1 - \bar{x}_2}{\sqrt{s_1/n_1 - s_2/n_2}}$$ Note that we assume under the null hypothesis that $\mu_1 - \mu_2 = 0$, which is why we replace this quantity with $0$ in the test statistic.
4. The p-value is $2P(t_{df} > |t|)$ with $df = \Delta$.
5. If $\text{p-value}<\alpha$, reject the null hypothesis.
6. Interpret in the context of the problem. 

Notice that the only difference between the critical value and p-value approaches are steps 4 and 5. 

> **Example**: Researchers wanted to detemine whether a dymanic or static approach would impact the time needed to complete neurosurgeries. The experiment resulted in the following data from simple random samples of patients:
>
> | Dynamic | Static |
> | :-: | :-: |
> | $\bar{x}_1 = 394.6$ | $\bar{x}_2 = 468.3$ |
> | $s_1 = 84.7$ | $s_2 = 38.2$ |
> | $n_1 = 14$ | $n_2 = 6$
>
> Times are measured in minutes. Assume $X_1$ and $X_2$ are reasonably normal. 
>
> 1. $H_0: \mu_1 = \mu_2$ and $H_A: \mu_1\ne\mu_2$
> 2. Let $\alpha=0.05$ (this will be our default when a significance level is not given)
>    - We are told these are simple random samples.
>    - There's no reason that time for a neurosurgery with the dynamic system would impact time for the static system (or vice versa), so it's reasonable to assume these samples are independent.
>    - We are told to assume that $X_1$ and $X_2$ are reasonably normal.
> 3. The test statistic is $$t = \frac{394.6-468.3}{84.7^2/14 + 38.2^2/6} = -2.681$$
> 4. Then $$df = \Delta = \frac{(84.7^2/14) + (38.2^2/6)^2}{\frac{(84.7^2/14)^2}{14-1} + \frac{(38.2^2/6)^2}{6-1}} = 17$$ when rounded down. The critical value is $$t_{17, 0.025} = 2.110$$ and the p-value is $$2P(t_{17}>|-2.681|)=2(0.0079)=0.0158$$
> 5. For the critical value approach, 

```{r}
x <- seq(-4,4,length.out=1000)
y <- dt(x,18)
x1 <- -4; x2 <- -qt(0.975,18); x3 <- qt(0.975,18); x4 <- 4
ts <- -2.681
x.coord <- c(x[x > x1 & x < x2])
y.coord <- y[x > x1 & x < x2]
x.coord2 <- c(x[x > x3 & x < x4])
y.coord2 <- y[x > x3 & x < x4]
plot(x,y,type='l', xaxt='n', yaxt='n', ylab="",xlab="", ylim=c(-0.02,max(y)+0.01)) #
abline(h=0)
polygon(x=x.coord, y= c(0, y.coord[2:(length(y.coord)-1)], 0), col="mistyrose")
polygon(x=x.coord2, y= c(0, y.coord2[2:(length(y.coord2)-1)], 0), col="mistyrose")
text(-2.7,0.05,"Reject"); text(2.7,0.05,"Reject"); text(0,0.1,"Do Not Reject")
text(x2, -0.015, "-2.11"); text(x3, -0.015, "2.11"); text(ts-0.1, -0.015, "-2.68")
```

> Since the test statistic is in the rejection region, we reject the null hypothesis. For the p-value approach, since $\text{p-value}=0.158 < \alpha =0.05$, reject the null hypothesis.
>
> 6. At the 0.05 level of significance, the data provide sufficient evidence to conclude that the mean time for the dynamic system is less than the mean time for the static system. 

We can also construct a **$(1-\alpha)100\%$ confidence interval** for the difference of the two population means: $$(\bar{x}_1-\bar{x}_2) \pm t_{df, \alpha/2}\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}$$ which we interpret as we interpret other confidence intervals, including in our interpretation that we are now considering the **difference of two means**. 