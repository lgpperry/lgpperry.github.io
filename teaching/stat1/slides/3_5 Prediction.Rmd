---
title: "3.5 Prediction: A Cautionary Tale"
author: "Dr. Lauren Perry"
output:
  ioslides_presentation: 
    widescreen: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

***

It's important to stop and think about our predictions.

- Sometimes, the numbers just don't make sense.
- Other times it's harder to tell something's wrong!

## Extrapolation

*Extrapolation* is applying a model estimate for values outside of the data's range for $x$. 

- Our linear model is only an approximation.
    - We don't know anything about the relationship outside of the scope of our data.

## Example

```{r}
set.seed(1)

x.dat <- runif(100,5,20)
y.dat <- 2*log(x.dat) + rnorm(100,0,0.4)
mod <- lm(y.dat ~ x.dat)

plot(x.dat,y.dat, pch=16, xlim=c(5,20), ylab="y", xlab="x"); abline(mod$coefficients[1], mod$coefficients[2], col='red', lwd=3); 
```

## Example

The best fit line is $$\hat{y} = 2.69 + 0.179x$$ 

- The correlation is $R=0.877$.
- So the coefficient of determination is $R^2 = 0.767$.
    - (think: a C grade)
    
***

Now suppose we wanted to predict the value of $y$ when $x=0.1$: $$\hat{y} = 2.66 + 0.181\times0.1 = 2.67$$ 

- Seems reasonable... but the true (population) best-fit model looks like this:

```{r}
set.seed(1)
x <- seq(from=0.1, to=25, by=0.1)
y <- 2*log(x)

x.dat <- runif(100,5,20)
y.dat <- 2*log(x.dat) + rnorm(100,0,0.5)
mod <- lm(y.dat ~ x.dat)

plot(x,y,type='l', lwd=3, col='red', ylim=c(min(y), max(y.dat))); points(x.dat,y.dat, pch=16); abline(v=5); abline(v=20); 
  points(x=0.1, y=mod$coefficients[1]+0.1* mod$coefficients[2], pch=16, col='blue'); abline(v=0.1, lty=2)
```
