% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Introduction to Statistics},
  pdfauthor={Lauren Cappiello},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Introduction to Statistics}
\author{Lauren Cappiello}
\date{}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{welcome-to-statistics}{%
\chapter*{Welcome to Statistics!}\label{welcome-to-statistics}}
\addcontentsline{toc}{chapter}{Welcome to Statistics!}

\hypertarget{for-the-student}{%
\section*{For the Student}\label{for-the-student}}
\addcontentsline{toc}{section}{For the Student}

There are a lot of ways to approach an introductory statistics class. Historically, the topics found in this text have been taught in a way that emphasizes hand calculations and the use of tables full of numbers.

My philosophy is a little different. This class is designed for students who will need to read statistical results and may need to produce basic statistics using a computer. If you go on to be a scientist and need more statistical know how, this course will give you enough background knowledge to take the inevitable next course in statistics. There is plenty of math in this text, but none of these situations require the ability to do that math by hand.

In many sections, the math is provided and explained but not emphasized. This is intentional. Instead, we focus on the ``why''\ldots{} Why do we care about this topic? Why is this concept important? Why do I run this test when I have that kind of data? \ldots and we focus on the interpretation. What does this number tell us about an experiment? What can we conclude based on these statistical results?

We see statistics all the time in the media - in the form of graphs, tables, averages, predictions about elections or sports, you name it! Hopefully, by learning the whys and the interpretation, you will finish this text feeling like you can read and understand statistical results when you run into them in the real world.

\hypertarget{r-programming}{%
\subsection*{R Programming}\label{r-programming}}
\addcontentsline{toc}{subsection}{R Programming}

This text is designed to teach you introductory statistics with the option to learn some R (a statistical programming language) along the way. As a result, some sections have some introductory material on R. R is an incredibly powerful tool, but we're going to keep it relatively simple. Using R will save us the headache of doing a lot of calculations by hand.

Since we are only going to use R for a few simple commands, we will run it completely online at the website rdrr.io/snippets (bookmark this website!)

For now, you can run R right here in the course notes! This is exactly what you will see on the rdrr.io website. Type in your command and click the green ``Run'' button. Try running the command \texttt{print("Welcome\ to\ Statistics!")}.

If it prints out ``Sorry, something went wrong. All I know is:'', just press the ``Run'' button again.

\hypertarget{for-the-instructor}{%
\section*{For the Instructor}\label{for-the-instructor}}
\addcontentsline{toc}{section}{For the Instructor}

Thanks for checking out my Introduction to Statistics text! Sections are designed to be short, easy-to-read introductions to each concept. Some of the more conceptual sections do not have section exercises, but I am working on adding exercises wherever it seems appropriate. The topics and course ordering reflect the department syllabus for the 3-unit Introduction to Statistics at Sacramento State. I am sure there are topics we've left out, but there are only so many things one can cover in 15 weeks.

Each Chapter is designed to take approximately two weeks of class time. In an ideal world, I would cover at least the first eight in a 15 week semester. However, with assessment, activities, student questions, holidays, etc., I usually get through the first seven. Rarely do I get to ANOVA. Despite the time constraints, I am working on including additional topics.

This text is a work in progress and gets updated every semester that I each Introduction to Statistics (which is very nearly every semester) and sometimes during winter and summer breaks.

Currently, I am working on

\begin{itemize}
\tightlist
\item
  overhauling the entire thing to remove some of the examples borrowed from OpenIntro (another great resource) and from Weiss' \emph{Introductory Statistics} from when this was just the typed version of my course notes.
\item
  adding section exercises and additional topics.
\item
  including subsections with brief introductions to R programming.
\end{itemize}

Note that the problems with the pdf or epub downloads are known issues. I will tackle those when the text is in a more finished state. Currently, it is designed to be viewed fully online (or you can right click and ``print page'' - save to pdf - to download each chapter individually).

Slides for many of the sections are available on my website, with more being added throughout the Spring 2023 semester: lgpcappiello.github.io/teaching/stat1/

Please feel free to reach out to me with any questions, comments, or concerns by emailing me at \href{mailto:cappiello@csus.edu}{\nolinkurl{cappiello@csus.edu}}

\hypertarget{course-learning-outcomes}{%
\section*{Course Learning Outcomes}\label{course-learning-outcomes}}
\addcontentsline{toc}{section}{Course Learning Outcomes}

The CLOs for Stat 1: Introduction to Statistics at Sacramento State are as follows.

Students will be able to:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Organize, summarize, and interpret data in tabular, graphical, and pictorial formats.
\item
  Organize and interpret bivariate data and learn simple linear regression and correlation.
\item
  Understand the basic rules of probability.
\item
  Use the binomial distribution as a model for discrete variables.
\item
  Use the normal distribution as a model for continuous variables.
\item
  Apply statistical inference techniques of parameter estimation such as point estimation and confidence interval estimation.
\item
  Apply techniques of testing various statistical hypotheses concerning population parameters.
\end{enumerate}

Each chapter also has chapter-specific learning outcomes and their corresponding CLOs.

\hypertarget{introduction-to-data}{%
\chapter{Introduction to Data}\label{introduction-to-data}}

\hypertarget{chapter-overview}{%
\section{Chapter Overview}\label{chapter-overview}}

What is statistics? There are two ways to think about this:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Facts and data, organized or summarized in such a way that they provide useful information about something.
\item
  The science of analyzing, organizing, and summarizing data.
\end{enumerate}

As a field, Statistics provides tools for scientists, practitioners, and laypeople to better understand data. You may find yourself using knowledge from this course in a research lab, while reading a research report, or even while watching the news!

\textbf{Chapter Learning Objectives/Outcomes}

After completing Chapter 1, you will:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Understand basic statistical terminology.
\item
  Produce data using sampling and experimental design techniques.
\item
  Organize and visualize data using techniques for exploratory data analysis.
\item
  Identify the shape of a data set.
\item
  Understand and interpret graphical displays.
\end{enumerate}

\textbf{R objectives}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Manually enter data.
\item
  Generate random numbers.
\item
  Create histograms.
\end{enumerate}

This chapter's outcomes correspond to course outcomes (1) organize, summarize, and interpret data in tabular, graphical, and pictorial formats and (2) organize and interpret bivariate data and learn simple linear regression and correlation.

\hypertarget{statistics-terminology}{%
\section{Statistics Terminology}\label{statistics-terminology}}

There are two ways to think about statistics:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Descriptive statistics} are methods for \emph{describing} information.
\end{enumerate}

For example, 66\% of eligible voters voted in the 2020 presidential election (the highest turnout since 1900!).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Inferential statistics} are methods for \emph{drawing inference} (making decisions about something we are uncertain about).
\end{enumerate}

For example, a poll suggests that 75\% of voters will select a Candidate A. People haven't voted yet, so we don't know what will happen, but we could reasonably conclude that Candidate A will win the election.

\textbf{Data} is factual information. We collect data from a \textbf{population}, the collection of all individuals or items a researcher is interested in.

\begin{itemize}
\tightlist
\item
  Collecting data from an entire population is called a \textbf{census}.

  \begin{itemize}
  \tightlist
  \item
    This is complicated and expensive! There's a reason the United States only does a census every 10 years.
  \end{itemize}
\item
  We can also take a \textbf{sample}, a subset of the population we get data from.

  \begin{itemize}
  \tightlist
  \item
    If you think of the population as a pie, the sample is a small slice. Whether it's a pumpkin pie, a cherry pie, or a savory pie, the small slice will tell you that. We don't need to eat the entire pie to learn a lot about it!
  \end{itemize}
\end{itemize}

Data are often organized in what we call a \textbf{data matrix}. If you've ever seen data in a spreadsheet, that's a data matrix!

Age

Gender

Smoker

Marital Status

Person 1

45

Male

yes

married

Person 2

23

Female

no

single

Person 3

36

Other

no

married

Person 4

29

Female

no

single

Each row (horizontal) represents one \textbf{observation} (also called \textbf{observational units}, \textbf{cases}, or \textbf{subjects}). These are the individuals or items in the sample.

Each column (vertical) represents a \textbf{variable}, the characteristic or thing being measured. Think of variables as measurements that can \emph{vary} from one observation to the next.

There are two types of variable:

\textbf{Numeric} or \textbf{quantitative} variables take \emph{numeric} values AND it is sensible to do math with those values.

\textbf{Discrete numeric} variables take numeric values with jumps. Typically, this means they can only take whole number values. These are often counts of something - for example, counting the number of pets you have.

\textbf{Continuous numeric} variables take values ``between the jumps''. Typically, this means they can take decimal values.

\textbf{Categorical} or \textbf{qualitative} variables take values that are \emph{categories}.

The ``Does it make sense''? Test

\begin{itemize}
\tightlist
\item
  Sometimes, categories can be represented by numbers. Ask yourself if it makes sense to do math with those numbers. If it doesn't make sense, it's probably a categorical variable. (Ex: zip codes)
\item
  If you're unsure whether a variable is discrete or continuous, pick a number with some decimal places - like 1.83 - and ask yourself if that value makes sense. If it doesn't, it's probably discrete. (Ex: number of siblings)
\end{itemize}

\hypertarget{r-entering-data}{%
\subsection*{R: Entering Data}\label{r-entering-data}}
\addcontentsline{toc}{subsection}{R: Entering Data}

We can work with data in R by reading it in from a file or by entering it manually. To enter numeric data manually, we use the \texttt{c} command.

The following line of code saves the \texttt{age} data from the data matrix example above:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{age }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{45}\NormalTok{, }\DecValTok{23}\NormalTok{, }\DecValTok{36}\NormalTok{, }\DecValTok{29}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Notice that we set \texttt{age} equal to \texttt{c()} with the numbers in the parentheses, separated by commas. Also notice that the numbers are in the same order as in the data. If I want to use the \texttt{age} variable later, I can refer to it directly in R and it will print out the values in that variable:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{age }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{45}\NormalTok{, }\DecValTok{23}\NormalTok{, }\DecValTok{36}\NormalTok{, }\DecValTok{29}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To enter categorical data in R, we do the same thing, but with the addition of quotation marks:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gender }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"Male"}\NormalTok{, }\StringTok{"Female"}\NormalTok{, }\StringTok{"Other"}\NormalTok{, }\StringTok{"Female"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{section-exercises}{%
\subsection*{Section Exercises}\label{section-exercises}}
\addcontentsline{toc}{subsection}{Section Exercises}

The following table shows part of the data matrix from a Stat 1 course survey.

\textbf{Age}

\textbf{Year in college}

\textbf{What is your major?}

\textbf{Units this semester}

1

19

Sophomore

Health Sciences

15

2

19

Sophomore

Business

15

3

19

Sophomore

Undecided

14

\(\vdots\)

\(\vdots\)

\(\vdots\)

\(\vdots\)

\(\vdots\)

29

21

Junior

Business

15

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What does each row of the data matrix represent?
\item
  What does each column of the data matrix represent?
\item
  Indicate whether each variable is discrete numeric, continuous numeric, or categorical.
\end{enumerate}

\hypertarget{dig-deeper}{%
\subsubsection*{Dig Deeper}\label{dig-deeper}}
\addcontentsline{toc}{subsubsection}{Dig Deeper}

Read the article, \href{https://www.nytimes.com/2018/03/27/us/politics/census-citizenship-question.html}{Here's Why an Accurate Census Count Is So Important} from the New York Times. (If you can't access the article, try a Google search for ``why an accurate census count is important''.) Take a moment to write down your thoughts on the relationship between how we collect data (for example - the questions asked in the census) and the power data has over people's lives. As researchers, scientists, and consumers of media, what are some reasons this is important to think about?

\hypertarget{sampling-and-design}{%
\section{Sampling and Design}\label{sampling-and-design}}

\hypertarget{statistical-sampling}{%
\subsection{Statistical Sampling}\label{statistical-sampling}}

How do we get samples? We want a sample that represents our population. \textbf{Representative samples} reflect the relevant characteristics of our population.

In general, we get representative samples by selecting our samples \emph{at random} and with an adequate sample size.

A non-representative sample is said to be \textbf{biased}. For example, if we used a sample of chihuahuas to represent all dogs, we probably wouldn't get very good information; that sample would be \emph{biased}.

These can be a result of \textbf{convenience sampling}, choosing a sample based on ease.

In our daily lives, common sources of biases are \emph{anecdotal evidence} and \emph{availability bias}. Anecdotal evidence is data based on personal experience or observation. Typically this consists of only one or two observations and is NOT representative of the population.

\begin{quote}
\emph{Example}: anecdotal evidence. A friend tells you their grandpa smoked a pack of cigarettes a day and lived to be 100. Does this mean that cigarettes will help you live to 100? no!
\end{quote}

Availability bias is your brain's tendency to think that examples of things that come readily to mind are more representative than is actually the case.

\begin{quote}
\emph{Example}: availability bias. Shark attacks. Shark attacks are actually extremely uncommon, but the media tends to report on extreme anecdotes, making us more prone to this kind of bias!
\end{quote}

We avoid bias by taking random samples. One type of random sample is a \textbf{simple random sample}. We can think of this as ``raffle sampling'', like drawing names out of a hat. Each case (or each possible sample) has an equal chance of being selected. Knowing that A is selected doesn't tell us anything about whether B is selected. Instead of literally drawing from a hat, we usually use a \textbf{random number generator} from a computer.

\hypertarget{experimental-design}{%
\subsection{Experimental Design}\label{experimental-design}}

When we do research, we have two options:

Conduct an \textbf{experiment}, where researchers assign treatments to cases.

\textbf{Treatments} are experimental conditions.

In an experiment, cases may also be called \textbf{experimental units} (items or individuals on which the experiment is performed).

Conduct an \textbf{observational study}, where no conditions are assigned. These are often done for ethical reasons, like examining the impacts of smoking cigarettes.

Experiments allow us to infer causation. Observational studies do not.

Experimental design principles:

\textbf{Control}: two or more treatments are compared.

\textbf{Randomization}: experimental units are assigned to treatment groups (usually and preferably at random).

\textbf{Replication}: a large enough sample size is used to test each treatment many times (on many different experimental units).

\textbf{Blocking}: if variables other than treatment are likely to have an impact on study outcome, we use blocks.

For example, I might separate patients in a medical study into ``high risk'' and ``low risk'' blocks. I would randomly assign all of the high risk patients to a treatment and then randomly assign all of the low risk patients to a treatment. This helps ensure an even distribution of high/low risk patients in each treatment group.

An experiment without blocking has a completely randomized design; an experiment with blocking has a randomized block design.

In an experimental setting, we talk about

Response variable: the characteristic of the experimental outcome being measured or observed.

Factor: a variable whose impact on the response variable is of interest in the experiment.

Levels: the possible values of a factor.

Treatments: experimental conditions (based on combinations of factor levels).

In human subjects research, we do a little extra work:

If subjects do not know what treatment group they are in, the study is called blind.

We use a placebo (fake treatment) to achieve this.

If neither the subjects nor the researchers who interact with them know the treatment group, it is called double blind.

This helps avoid bias caused by placebo effect, doctor's expectations for outcome, etc.!

\hypertarget{r-random-number-generation}{%
\subsection*{R: Random Number Generation}\label{r-random-number-generation}}
\addcontentsline{toc}{subsection}{R: Random Number Generation}

To generate a random whole number using R, we can use the \texttt{sample} command. We use the \texttt{sample} command like \texttt{sample(minimum:maximum,\ size\ =\ n)}, replacing \texttt{minimum} with the minimum value (often the number 1), \texttt{maximum} with the maximum value, and \texttt{n} with the sample size.

The following command takes a random sample of size 1 from the values 1 through 10 (1, 2, 3, 4, 5, 6, 7, 8, 9, 10):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{, }\AttributeTok{size =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 6
\end{verbatim}

\hypertarget{section-exercises-1}{%
\subsection*{Section Exercises}\label{section-exercises-1}}
\addcontentsline{toc}{subsection}{Section Exercises}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  A study published in 2009 sought to examine whether supplementing with chia seeds contributed to weight loss. Researchers recruited 76 individuals and randomly assigned them into either a treatment group or a control group. The treatment group was given a set quantity of daily chia seeds; the control group was given a placebo. At the end of the 12-week study, they found no difference in average weight lost between the treatment and control group.

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    Is this an observational study or an experiment? Explain.
  \item
    Identify the (i) cases and (ii) response variable.
  \end{enumerate}
\end{enumerate}

\hypertarget{frequency-distributions}{%
\section{Frequency Distributions}\label{frequency-distributions}}

\hypertarget{qualitative-variables}{%
\subsection{Qualitative Variables}\label{qualitative-variables}}

\textbf{Frequency} (\textbf{count}): the number of times a particular value occurs.

A \textbf{frequency distribution} lists each distinct value with its frequency.

\begin{longtable}[]{@{}cc@{}}
\toprule()
Class & Frequency \\
\midrule()
\endhead
freshman & 12 \\
sophomore & 10 \\
junior & 3 \\
senior & 5 \\
\bottomrule()
\end{longtable}

A \textbf{bar plot} is a graphical representation of a frequency distribution. Each bar's height is based on the frequency of the corresponding category.

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-5-1.pdf}

The bar plot above shows the class level breakdown for students in an Introductory Statistics course. Take a moment to notice how the bars match up with the frequency distribution above.

\textbf{Relative frequency} is the ratio of the frequency to the total number of observations.

\[
  \text{relative frequency} = \frac{\text{frequency}}{\text{number of observations}}
\]

This is also called the \textbf{proportion}. The \textbf{percentage} can be obtained by multiplying the proportion by 100.

A \textbf{relative frequency distribution} lists each distinct value with its relative frequency.

\begin{longtable}[]{@{}cccc@{}}
\toprule()
Class & Frequency & Relative Frequency & Percent \\
\midrule()
\endhead
freshman & 12 & \(12/30 = 0.4\) & 40\% \\
sophomore & 10 & \(10/30 \approx 0.3333\) & 33.33\% \\
junior & 3 & \(3/30 = 0.1\) & 10\% \\
senior & 5 & \(5/30 \approx 0.1667\) & 16.67\% \\
\bottomrule()
\end{longtable}

\hypertarget{quantitative-variables}{%
\subsection{Quantitative Variables}\label{quantitative-variables}}

We can also apply this concept to numeric data. A \textbf{dot plot} is one graphical representation of this. A dot plot shows a number line with dots drawn above the line. Each dot represents a single point.

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-6-1.pdf}

For example, the dot plot above shows a sample where the value 1 appears three times, the value 5 appears six times, etc.

We would also like to be able to visualize larger, more complex data sets. This is hard to do using a dot plot! Instead, we can do this using \textbf{bins}, which group numeric data into equal-width consecutive intervals.

\begin{quote}
\emph{Example}: A random sample of weights (in lbs) from 12 cats:

\[\quad 6.2 \quad 11.6 \quad 7.2 \quad 17.1 \quad 15.1 \quad 8.4 \quad    7.7 \quad   13.9 \quad  21.0 \quad  5.5 \quad 9.1 \quad 7.3 \]

The \textbf{minimum} (smallest value) is 5.5 and the \textbf{maximum} (largest value) is 21. There are lots of ways to break these into ``bins'', but what about\ldots{}

\begin{itemize}
\tightlist
\item
  5 - 10
\item
  10 - 15
\item
  15 - 20
\item
  20 - 25
\end{itemize}

Each bin has an equal width of 5, but if we had a cat with a weight of exactly 15 lbs, would we use the second or third bin?? It's unclear. To make this clear, we need there to be no overlap. Instead, we could use:

\begin{longtable}[]{@{}cc@{}}
\toprule()
Weight & Count \\
\midrule()
\endhead
5 - \textless10 & 7 \\
10 - \textless15 & 2 \\
15 - \textless20 & 2 \\
20 - \textless25 & 1 \\
\bottomrule()
\end{longtable}

Now, a cat with a weight of 15.0 lbs would be placed in the third bin (but not the second).
\end{quote}

We will visualize this using a \textbf{histogram}, which is a lot like a bar plot but for numeric data:

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-7-1.pdf}

This is what we call a \textbf{frequency histogram} because each bar height reflects the frequency of that bin. We can also create a \textbf{relative frequency histogram} which displays the relative frequency instead of the frequency:

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-8-1.pdf}

Notice that these last two histograms look the same \emph{except for the numbers on the vertical axis}! This gives us insight into the shape of the data \textbf{distribution}, literally how the values are distributed across the bins. The part of the distribution that ``trails off'' to one or both sides is called a \textbf{tail} of the distribution.

When a histogram trails off to one side, we say it is \textbf{skewed} (right-skewed if it trails off to the right, left-skewed if it trails off to the left). Data sets with roughly equal tails are \textbf{symmetric}.

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-9-1.pdf}

We can also use a histogram to identify \textbf{modes}. For numeric data, especially continuous variables, we think of modes as \emph{prominent peaks}.

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-10-1.pdf}

\begin{itemize}
\tightlist
\item
  \textbf{Unimodal}: one prominent peak.
\item
  \textbf{Bimodal}: two prominent peaks.
\item
  \textbf{Multimodal}: three or more prominent peaks.
\end{itemize}

Finally, we can also ``smooth out'' these histograms and use a smooth curve to examine the shape of the distribution. Below are the smooth curve versions of the distributions shown in the four histograms used to demonstrate skew and symmetry.

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-11-1.pdf}

\hypertarget{r-histograms}{%
\subsection*{R: Histograms}\label{r-histograms}}
\addcontentsline{toc}{subsection}{R: Histograms}

There is a built-in dataset in R called \texttt{Loblolly}, which contains the variables \texttt{height} and \texttt{age} of some Loblolly pine trees. I can refer to this data by typing in \texttt{Loblolly} directly. To view just the first few observations (out of the 84 total in the data), I can use the \texttt{head} command:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(Loblolly)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   height age
## 1   4.51   3
## 2  10.89   5
## 3  28.72  10
## 4  41.74  15
## 5  52.70  20
## 6  60.92  25
\end{verbatim}

The information that appears next to each \#\# is what R prints out for us.

In order to refer to the variables in \texttt{Loblolly} directly, I will need to use the \texttt{attach} command. This tells R that when I say \texttt{age} I mean the age variable from the \texttt{Loblolly} dataset (and not from some other dataset).

I want to create a histogram to visualize the ratio of tree height to age. First, I need to find this ratio for each observation. I can do this easily in R by dividing \texttt{height} by \texttt{age}. I will save this as a new variable called \texttt{htage\_ratio}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{htage\_ratio }\OtherTok{=}\NormalTok{ height}\SpecialCharTok{/}\NormalTok{age}
\end{Highlighting}
\end{Shaded}

Then to create a histogram of the height to age ratio, we will use the command \texttt{hist} on the variable \texttt{htage\_ratio}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(htage\_ratio)}
\end{Highlighting}
\end{Shaded}

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-16-1.pdf}

I can clean up this graph by taking advantage of additional \emph{arguments} in the \texttt{hist} command:

\begin{itemize}
\tightlist
\item
  \texttt{main} is where I can give the plot a new title. (Make sure to put the title in quotes!)
\item
  \texttt{xlab} is the x-axis (horizontal axis) title.
\item
  \texttt{ylab} is the y-axis (vertical axis) title.
\item
  \texttt{freq} allows us to create either frequency or \emph{relative frequency} histograms.

  \begin{itemize}
  \tightlist
  \item
    If we set it equal to \texttt{TRUE} it will produce a frequency histogram. (This is the default if we don't give R any instructions.)
  \item
    If we set it equal to \texttt{FALSE} it will produce a relative frequency histogram.
  \end{itemize}
\item
  \texttt{col} allows us to give R a specific color for the bars.
\end{itemize}

Notice that each argument is separated by a comma.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(htage\_ratio, }
     \AttributeTok{main =} \StringTok{"Histogram of Height{-}to{-}Age Ratio"}\NormalTok{, }
     \AttributeTok{xlab =} \StringTok{"Height{-}to{-}Age Ratio (feet/year)"}\NormalTok{, }
     \AttributeTok{ylab =} \StringTok{"Relative Frequency"}\NormalTok{, }
     \AttributeTok{freq =} \ConstantTok{FALSE}\NormalTok{,}
     \AttributeTok{col =} \StringTok{\textquotesingle{}pink\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-17-1.pdf}

When I am done, I will use the \texttt{detatch} command to tell R that I am not working with the \texttt{Loblolly} data anymore.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{detach}\NormalTok{(Loblolly)}
\end{Highlighting}
\end{Shaded}

\hypertarget{descriptive-measures}{%
\chapter{Descriptive Measures}\label{descriptive-measures}}

\hypertarget{chapter-overview-1}{%
\section{Chapter Overview}\label{chapter-overview-1}}

In the previous chapter, we thought about descriptive statistics using tables and graphs. Next, we summarize data by computing numbers. Some of these numbers you may already be familiar with, such as averages and percentiles. Numbers used to describe data are called \emph{descriptive measures}.

\textbf{Chapter Learning Objectives/Outcomes}

After completing Chapter 2, you will be able to:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Calculate and interpret measures of center.
\item
  Calculate and interpret measures of variation.
\item
  Find and interpret measures of position.
\item
  Summarize data using boxplots.
\end{enumerate}

\textbf{R objectives}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Generate measures of center.
\item
  Generate measures of variability.
\item
  Generate measures of position.
\item
  Create box plots.
\end{enumerate}

This chapter's outcomes correspond to course outcomes (1) organize, summarize, and interpret data in tabular, graphical, and pictorial formats, (2) organize and interpret bivariate data and learn simple linear regression and correlation, and (6) apply statistical inference techniques of parameter estimation such as point estimation and confidence interval estimation.

\hypertarget{measures-of-central-tendency}{%
\section{Measures of Central Tendency}\label{measures-of-central-tendency}}

One research question we might ask is : what values are most common or most likely?

\textbf{Mode}: the most commonly occurring value. We can use this for numeric variables, but typically we use the mode when talking about categorical data.

\textbf{Mean}: this is what we usually think of as the ``average''. Denoted \(\bar{x}\). Add up all of the values and divide by the number of observations (\(n\)):
\[
  \bar{x} = \frac{x_1 + x_2 + \dots + x_n}{n} = \sum_{i=1}^n \frac{x_i}{n}
\]
where \(x_i\) denotes the \(i\)th observation and \(\sum_{i=1}^n\) is the sum of all observations from 1 through \(n\). This is called \emph{summation notation}.

\textbf{Median}: the middle number when the data are ordered from smallest to largest.

\begin{itemize}
\tightlist
\item
  If there are an odd number of observations, this will be the number in the middle:

  \{1, 3, \textbf{7}, 9, 9\} has median 7
\item
  If there are an even number of observations, there will be two numbers in the middle. The median will be their average.

  \{1, 2, \textbf{4, 7}, 9, 9\} has median \(\frac{4+7}{2}=5.5\)
\end{itemize}

The mean is sensitive to extreme values and skew. The median is not!

\(x\): 1, 3, 7, 9, 9

\(y\): 1, 3, 7, 9, 45

\textbf{Median}

\(\text{median} = 7\)

\(\text{median} = 7\)

\textbf{Mean}

\(\bar{x} = \frac{29}{5} = 5.8\)

\(\bar{y} = \frac{65}{5} = 13\)

Notice how changing that 9 out for a 45 changes the \emph{mean} a lot! But the \emph{median} is 7 for both \(x\) and \(y\).

Because the median is not affected by extreme observations or skew, we say it is a \textbf{resistant measure} or that it is \textbf{robust}.

Which measure should we use?

\begin{itemize}
\tightlist
\item
  Mean: symmetric, numeric data
\item
  Median: skewed, numeric data
\item
  Mode: categorical data
\end{itemize}

Note: If the mean and median are roughly equal, it is reasonable to assume the distribution is roughly symmetric.

\hypertarget{r-finding-measures-of-center}{%
\subsection*{R: Finding Measures of Center}\label{r-finding-measures-of-center}}
\addcontentsline{toc}{subsection}{R: Finding Measures of Center}

To find a mean in R, we use the command \texttt{mean}. Let's use the \texttt{Loblolly} pine data again and find the \texttt{mean} tree height.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{attach}\NormalTok{(Loblolly)}
\FunctionTok{mean}\NormalTok{(height)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 32.3644
\end{verbatim}

From R, we can see that the sample mean height of the Loblolly pines is 32.36 feet. (Note that R will often print things out with \#\# and a number in square brackets. This is just to help us keep track of things if we write a lot of lines of code. You can ignore that number in the brackets!)

Is a mean the appropriate measure of center? We can quickly check the skew with a histogram:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(height)}
\end{Highlighting}
\end{Shaded}

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-21-1.pdf}

It might be a little bit hard to tell with this histogram, but it does not look particularly skewed. Let's use our other trick: if the mean and median are approximatly equal, we can say the distribution is approximately symmetric (and therefore the mean is an appropriate measure of center). To calculate the median, we use the command \texttt{median}:

\begin{verbatim}
## [1] 34
\end{verbatim}

The sample median of the Loblolly pine heights is 34 feet. Since the mean and median are approximately equal, it would be reasonable to use the mean in this case. To find a mode (for a categorical variable), we would repeat the same process but with the command \texttt{mode}.

\hypertarget{measures-of-variability}{%
\section{Measures of Variability}\label{measures-of-variability}}

How much do the data vary?

Should we care? Yes! The more variable the data, the harder it is to be confident in our measures of center!

If you live in a place with extremely variable weather, it is going to be much harder to be confident in how to dress for tomorrow's weather\ldots{} but if you live in a place where the weather is always the same, it's much easier to be confident in what you plan to wear.

We want to think about how far observations are from the measure of center.

One easy way to think about variability is the \textbf{range} of the data: \[\text{range} = \text{maximum} - \text{minimum}\] This is quick and convenient, but it is \emph{extremely} sensitive to outliers! It also takes into account only two of the observations - we would prefer a measure of variability that takes into account \emph{all} the observations.

\textbf{Deviation} is the distance of an observation from the mean: \(x - \bar{x}\). If we want to think about how far - on average - a typical observation is from the center, our intuition might be to take the average deviance\ldots{} but it turns out that summing up the deviances will \emph{always} result in 0! Conceptually, this is because the stuff below the mean (negative numbers) and the stuff above the mean (positive numbers) end up canceling each other out until we end up at 0. (If you are interested, \emph{Appendix A: Average Deviance} has a mathematical proof of this using some relatively straightforward algebra.)

One way to deal with this is to make all of the numbers positive, which we accomplish by squaring the deviance.

\begin{longtable}[]{@{}ccc@{}}
\toprule()
& Deviance & Squared Deviance \\
\midrule()
\endhead
\(x\) & \(x - \bar{x}\) & \((x - \bar{x})^2\) \\
2 & -1.2 & 1.44 \\
5 & 1.8 & 3.24 \\
3 & -0.2 & 0.04 \\
4 & 0.8 & 0.64 \\
2 & -1.2 & 1.44 \\
\(\bar{x}=3.2\) & Total = 0 & Total = 6.8 \\
\bottomrule()
\end{longtable}

\textbf{Variance} (denoted \(s^2\)) is the average squared distance from the mean:
\[
  s^2 = \frac{(x_1-\bar{x})^2 + (x_2-\bar{x})^2 + \dots + (x_n-\bar{x})^2}{n-1} = \frac{1}{n-1}\sum_{i=1}^n (x_i - \bar{x})^2
\]
where \(n\) is the sample size. Notice that we divide by \(n-1\) and NOT by \(n\). There are some mathematical reasons why we do this, but the short version is that it'll be a better estimate when we talk about inference.

Finally, we come to \textbf{standard deviation} (denoted \(s\)). \[s = \sqrt{s^2}\] The standard deviation is the square root of the variance. We say that a ``typical'' observation is within about one standard deviation of the mean (between \(\bar{x}-s\) and \(\bar{x}+s\)).

In practice (including in this class), we will use a computer to calculate the variance and standard deviation.

We will think about one more measure of variability, the interquartile range, in the next section.

\hypertarget{r-finding-measures-of-variability}{%
\subsection*{R: Finding Measures of Variability}\label{r-finding-measures-of-variability}}
\addcontentsline{toc}{subsection}{R: Finding Measures of Variability}

To find a range in R, we get started using the command \texttt{range}. Let's keep using the \texttt{Loblolly} pine data again and find the \texttt{mean} tree height.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{attach}\NormalTok{(Loblolly)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{range}\NormalTok{(height)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  3.46 64.10
\end{verbatim}

This command gives us the minimum and maximum values. Then to calculate the range, we would find \(64.10 - 3.46\). We can also do this in R, because R doubles as a calculator!

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{64.1} \SpecialCharTok{{-}} \FloatTok{3.46}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 60.64
\end{verbatim}

The sample range of the Loblolly pine heights is 60.64 feet.

To find the variance and standard deviation, we use the commands \texttt{var} and \texttt{sd}, respectively:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{var}\NormalTok{(height)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 427.3979
\end{verbatim}

The sample variance of the Loblolly pine heights is 427.4.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sd}\NormalTok{(height)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 20.6736
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{detach}\NormalTok{(Loblolly)}
\end{Highlighting}
\end{Shaded}

The sample standard deviation of the Loblolly pine heights is 20.67 feet.

\hypertarget{measures-of-position}{%
\section{Measures of Position}\label{measures-of-position}}

The \textbf{interquartile range} (\textbf{IQR}) represents the middle 50\% of the data.

Recall that the \emph{median} cut the data in half: 50\% of the data is below and 50\% is above the median. This is also called the \textbf{50th percentile}. The \textbf{\(p\)th percentile} is the value for which \(p\)\% of the data is below it.

To get the middle 50\%, we will split the data into four parts:

\begin{longtable}[]{@{}cccc@{}}
\toprule()
1 & 2 & 3 & 4 \\
\midrule()
\endhead
25\% & 25\% & 25\% & 25\% \\
\bottomrule()
\end{longtable}

The 25th and 75th percentiles, along with the median, divide the data into four parts. We call these three measurements the \textbf{quartiles}:

\begin{itemize}
\tightlist
\item
  \textbf{Q1}, the first quartile, is the median of the lower 50\% of the data.
\item
  \textbf{Q2}, the second quartile, is the median.
\item
  \textbf{Q3}, the third quartile, is the median of the upper 50\% of the data.
\end{itemize}

\begin{quote}
\emph{Example:} Consider \{1, 2, 3, 4, 5, 6, 7, 8, 9, 10\}

\begin{itemize}
\tightlist
\item
  Cutting the data in half: \{1, 2, 3, 4, 5 \textbar{} 6, 7, 8, 9, 10\}, the median (Q2) is \(\frac{5+6}{2}=5.5\).
\item
  Q1 is the median of \{1, 2, 3, 4, 5\}, or 3
\item
  Q3 is the median of \{6, 7, 8, 9, 10\}, or 8
\end{itemize}
\end{quote}

\textbf{Note:} this is a ``quick and dirty'' way of finding quartiles. A computer will give a more exact result.

Then the interquartile range is
\[
  \text{IQR} = \text{Q3}-\text{Q1}
\]

This is another measure of variability and is resistant to extreme values. In general, we prefer the mean and standard deviation when the data are symmetric and we prefer the median and IQR when the data are skewed.

\hypertarget{box-plots}{%
\subsection{Box Plots}\label{box-plots}}

Our measures of position are the foundation for constructing what we call a box plot, which summarizes the data with 5 statistics plus extreme observations:

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-29-1.pdf}

Drawing a box plot:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Draw the vertical axis to include all possible values in the data.
\item
  Draw a horizontal line at the median, at \(\text{Q1}\), and at \(\text{Q3}\). Use these to form a box.
\item
  Draw the \textbf{whiskers}. The whiskers' upper limit is \(\text{Q3}+1.5\times\text{IQR}\) and the lower limit is \(\text{Q1}-1.5\times\text{IQR}\). The actual whiskers are then drawn \emph{at the next closest data points within the limits}.
\item
  Any points outside the whisker limits are included as individual points. These are \textbf{potential outliers}.
\end{enumerate}

(Potential) outliers can help us\ldots{}

\begin{itemize}
\tightlist
\item
  examine skew (outliers in the negative direction suggest left skew; outliers in the positive direction suggest right skew).
\item
  identify issues with data collection or entry, especially if the value of the outliers doesn't make sense.
\end{itemize}

As with most things in this text, we won't draw a lot of boxplots by hand. However, understanding how they are drawn will help us understand how to interpret them!

\hypertarget{r-measures-of-position}{%
\subsection*{R: Measures of Position}\label{r-measures-of-position}}
\addcontentsline{toc}{subsection}{R: Measures of Position}

Let's continue using the Loblolly pine data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{attach}\NormalTok{(Loblolly)}
\end{Highlighting}
\end{Shaded}

We can get the quartiles quickly using the \texttt{summary} command. This will also give us the minimum, mean, and maximum. That's fine!

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(height)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    3.46   10.47   34.00   32.36   51.36   64.10
\end{verbatim}

So \(Q1 = 10.46\), \(Q2 = \text{Median} = 34\), and \(Q3 = 51.35\). We can also quickly get the interquartile range using the \texttt{IQR} command.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{IQR}\NormalTok{(height)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 40.895
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{detach}\NormalTok{(Loblolly)}
\end{Highlighting}
\end{Shaded}

\hypertarget{r-box-plots}{%
\subsection*{R: Box Plots}\label{r-box-plots}}
\addcontentsline{toc}{subsection}{R: Box Plots}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{attach}\NormalTok{(Loblolly)}
\end{Highlighting}
\end{Shaded}

To create a boxplot in R, we use the command \texttt{boxplot}. We can use some of the same arguments we used with the \texttt{hist} command to give it titles and color:

\begin{itemize}
\tightlist
\item
  \texttt{main} is where I can give the plot a new title. (Make sure to put the title in quotes!)
\item
  \texttt{ylab} is the y-axis (vertical axis) title.
\item
  \texttt{col} allows us to give R a specific color for the bars. Notice that I also have to put the color in quotes.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{boxplot}\NormalTok{(height,}
        \AttributeTok{main =} \StringTok{"Boxplot of Loblolly Pine Heights"}\NormalTok{, }
        \AttributeTok{ylab =} \StringTok{"Height (feet)"}\NormalTok{,}
        \AttributeTok{col =} \StringTok{"green"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-37-1.pdf}

R will automatically create the entire box plot, including showing any outliers. (Which we should note that there aren't any in the \texttt{height} variable!)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{detach}\NormalTok{(Loblolly)}
\end{Highlighting}
\end{Shaded}

\hypertarget{descriptive-measures-for-populations}{%
\section{Descriptive Measures for Populations}\label{descriptive-measures-for-populations}}

So far, we've thought about calculating various descriptive statistics from a sample, but our long-term goal is to estimate descriptive information about a population. At the population level, these values are called \textbf{parameters}.

When we find a measure of center, spread, or position, we use a sample to calculate a single value. These single values are called \textbf{point estimates} and they are used to \emph{estimate} the corresponding population parameter. For example, we use \(\bar{x}\) to estimate the population mean, denoted \(\mu\) (Greek letter ``mu'') and \(s\) to estimate the population standard deviation, denoted \(\sigma\) (Greek letter ``sigma'').

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Point Estimate
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Parameter
\end{minipage} \\
\midrule()
\endhead
sample mean: \(\bar{x}\) & population mean: \(\mu\) \\
sample standard deviation: \(s\) & population standard deviation: \(\sigma\) \\
\bottomrule()
\end{longtable}

\ldots and so on and so forth. For each quantity we calculate from a sample (point estimate), there is some corresponding unknown population level value (parameter) that we wish to estimate.

We will discuss this in more detail when we discuss Random Variables and Statistical Inference.

\hypertarget{regression-and-correlation}{%
\chapter{Regression and Correlation}\label{regression-and-correlation}}

\hypertarget{chapter-overview-2}{%
\section{Chapter Overview}\label{chapter-overview-2}}

We will extend our conversation on descriptive measures for quantitative variables to include the relationship between two variables.

\textbf{Chapter Learning Outcomes/Objectives}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Calculate and interpret a correlation coefficient.
\item
  Calculate and interpret a regression line.
\item
  Use a regression line to make predictions.
\end{enumerate}

\hypertarget{linear-equations}{%
\section{Linear Equations}\label{linear-equations}}

From your previous math classes, you should have a passing familiarity with linear equations like \(y=mx+b\). In statistics, we write these as \[y=b_0 + b_1x\] where \(b_0\) and \(b_1\) are constants, \(x\) is the independent variable, and \(y\) is the dependent variable. The graph of a linear function is always a (straight) line.

The \textbf{y-intercept} is \(b_0\), the value the dependent variable takes when the independent variable \(x=0\). The \textbf{slope} is \(b_1\), the change in \(y\) for a 1-unit change in \(x\).

A \textbf{scatterplot} shows the relationship between two (numeric) variables.

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-39-1.pdf}

At a glance, we can see that (in general) heavier cars have lower MPG. We call this type of data \textbf{bivariate data}. Now consider

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-40-1.pdf}

This relationship can be modeled perfectly with a straight line:
\[
y = 8 + 3.25x
\]
When we can do this - model a relationship perfectly - we know the exact value of \(y\) whenever we know the value of \(x\). This is nice (we would love to be able to do this all the time!) but typically data is more complex than this.

Linear regression takes the idea of fitting a line and allows the relationship to be imperfect. Imagine in the previous scenario that you buy an \$8 pound of coffee each month and individual coffees cost \$3.25\ldots{} but what if your pound of coffee didn't always cost \$8? Or your coffee drinks didn't always cost \$3.25? In this case, you might get a plot that looks something like this:

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-41-1.pdf}

The linear regression line looks like
\[
y = \beta_0 + \beta_1x + \epsilon
\]

\begin{itemize}
\tightlist
\item
  \(\beta\) is the Greek letter ``beta''.
\item
  \(\beta_0\) and \(\beta_1\) are constants.
\item
  Error (the fact that the points don't all line up perfectly) is represented by \(\epsilon\).
\end{itemize}

Think of this as the 2-dimensional version of a point estimate!

We estimate \(\beta_0\) and \(\beta_1\) using data and denote the estimated line by
\[
\hat{y} = b_0 + b_1x
\]

\begin{itemize}
\tightlist
\item
  \(\hat{y}\), ``y-hat'', is the estimated value of \(y\).
\item
  \(b_0\) is the estimate for \(\beta_0\).
\item
  \(b_1\) is the estimate for \(\beta_1\).
\end{itemize}

We drop the error term \(\epsilon\) when we estimate the constants for a regression line; we assume that the mean error is 0, so \emph{on average} we can ignore this error.

We use a regression line to make predictions about \(y\) using values of \(x\).

\begin{itemize}
\tightlist
\item
  \(y\) is the \textbf{response variable}.
\item
  \(x\) is the \textbf{predictor variable}.
\end{itemize}

\begin{quote}
\emph{Example}: (from OpenIntro Statistics 8.1.2) Researchers captured 104 brushtail possums and took a variety of body measurements on each before releasing them back into the wild. We consider two measurements for each possum: total body length and head length.
\end{quote}

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-42-1.pdf}

\begin{quote}
Clearly, the relationship isn't perfectly linear, but there does appear to be some kind of linear relationship (as body length increases, head length also increases). We want to try to use body length (\(x\)) to predict head length (\(y\)).

The regression model for these data is \[\hat{y}=42.7 + 0.57x\]
\end{quote}

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-43-1.pdf}

\begin{quote}
To predict the head length for a possum with a body length of 80cm, we just need to plug in 80 for body length (\(x\)): \[\hat{y}=42.7 + 0.57(80) = 88.3\text{mm}.\] Note: because the regression line is built using the data's original units (cm for body length, mm for head length), the regression line will preserve those units. That means that when we plugged in a value in cm, the equation spit out a predicted value in mm.
\end{quote}

\hypertarget{scatterplots-in-r}{%
\subsection*{Scatterplots in R}\label{scatterplots-in-r}}
\addcontentsline{toc}{subsection}{Scatterplots in R}

Let's create a scatterplot from the \texttt{faithful} dataset in R, which contains measurements on waiting time and eruption duration for Old Faithful geyser in Yellowstone National Park. We can use some of the same arguments we used with the \texttt{hist} and \texttt{boxplot} commands to give it titles:

\begin{itemize}
\tightlist
\item
  \texttt{x} is the variable I want represented on the x-axis (horizontal axis).
\item
  \texttt{y} is the variable I want represented on the y-axis (vertical axis).
\item
  \texttt{main} is where I can give the plot a new title. (Make sure to put the title in quotes!)
\item
  \texttt{xlab} is the x-axis title.
\item
  \texttt{ylab} is the y-axis title.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(}\StringTok{"faithful"}\NormalTok{)}
\FunctionTok{attach}\NormalTok{(faithful)}
\FunctionTok{plot}\NormalTok{(}\AttributeTok{x =}\NormalTok{ waiting, }\AttributeTok{y =}\NormalTok{ eruptions,}
     \AttributeTok{main =} \StringTok{"Scatterplot of Old Faithful Geyser Data"}\NormalTok{,}
     \AttributeTok{xlab =} \StringTok{"Waiting Time (minutes)"}\NormalTok{,}
     \AttributeTok{ylab =} \StringTok{"Eruption Duration (minutes)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-44-1.pdf}

\hypertarget{correlation}{%
\section{Correlation}\label{correlation}}

We've talked about the strength of linear relationships, but it would be nice to formalize this concept. The \textbf{correlation} between two variables describes the strength of their linear relationship. It always takes values between -1 and 1. We denote the correlation (or correlation coefficient) by \(R\): \[R = \frac{1}{n-1}\sum_{i=1}^n\left(\frac{x_i - \bar{x}}{s_x}\times\frac{y_i - \bar{y}}{s_y}\right)\] where \(s_x\) and \(s_y\) are the respective standard deviations for \(x\) and \(y\). The sample size \(n\) is the total number of \((x,y)\) pairs.

\begin{quote}
\emph{Example}: Consider

\begin{longtable}[]{@{}cc@{}}
\toprule()
\(x\) & \(y\) \\
\midrule()
\endhead
1 & 3 \\
2 & 3 \\
3 & 4 \\
\(\bar{x} = 2\) & \(\bar{y} = 3.333\) \\
\(s_x = 1\) & \(s_y = 0.577\) \\
\bottomrule()
\end{longtable}

Like we did with variance/standard deviation, I recommend using a table to calculate the correlation between \(x\) and \(y\):

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\centering
\(x - \bar{x}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(\frac{x - \bar{x}}{s_x}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(y - \bar{y}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(\frac{y - \bar{y}}{s_y}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(\frac{x - \bar{x}}{s_x}\times\frac{y - \bar{y}}{s_y}\)
\end{minipage} \\
\midrule()
\endhead
-1 & -1 & -0.333 & -0.577 & 0.577 \\
0 & 0 & -0.333 & -0.577 & 0.000 \\
1 & 1 & 0.667 & 1.155 & 1.155 \\
& & & & sum = 1.732 \\
\bottomrule()
\end{longtable}

So \(R = \frac{1}{3-1}(1.732) = 0.866\)
\end{quote}

Correlations

\begin{itemize}
\tightlist
\item
  close to -1 suggest strong, negative linear relationships.
\item
  close to +1 suggest strong, positive linear relationships.
\item
  close to 0 have little-to-no linear relationship.
\end{itemize}

Note: the sign of the correlation will match the sign of the slope!

\begin{itemize}
\tightlist
\item
  If \(R < 0\), there is a downward trend and \(b_1 < 0\).
\item
  If \(R > 0\), there is an upward trend and \(b_1 > 0\).
\item
  If \(R \approx 0\), there is no relationship and \(b_1 \approx 0\).
\end{itemize}

A final note: correlations only represent \emph{linear} trends. Consider the following scatterplot:

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-45-1.pdf}

Obviously there's a strong relationship between \(x\) and \(y\). In fact, there's a perfect relationship here: \(y = x^2\). But the \emph{correlation} between \(x\) and \(y\) is 0! This is one reason why it's important to examine the data both through visual and numeric measures.

\hypertarget{correlation-in-r}{%
\subsection*{Correlation in R}\label{correlation-in-r}}
\addcontentsline{toc}{subsection}{Correlation in R}

To find the correlation between two variables \texttt{x} and \texttt{y}, we use the command \texttt{cor(x,y)}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(}\AttributeTok{x =}\NormalTok{ waiting, }\AttributeTok{y =}\NormalTok{ eruptions)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9008112
\end{verbatim}

The correlation between waiting time and eruption duration for Old Faithful Geyser is \(-0.645\).

\hypertarget{finding-a-regression-line}{%
\section{Finding a Regression Line}\label{finding-a-regression-line}}

\textbf{Residuals} are the leftover \emph{stuff} (variation) in the data after accounting for model fit: \[\text{data} = \text{prediction} + \text{residual}\] Each observation has its own residual. The residual for an observation \((x,y)\) is the difference between observed (\(y\)) and predicted (\(\hat{y}\)): \[e = y - \hat{y}\] We denote the residuals by \(e\) and find \(\hat{y}\) by plugging \(x\) into the regression equation. If an observation lands above the regression line, \(e > 0\). If below, \(e < 0\).

When we estimate the parameters for the regression, our goal is to get each residual as close to 0 as possible. We might think to try minimizing \[\sum_{i=1}^n e_i = \sum_{i=1}^n (y_i - \hat{y}_i)\] but that would just give us very large negative residuals. As with the variance, we will use squares to shift the focus to magnitude:
\begin{align}
\sum_{i=1}^n e_i^2 &= \sum_{i=1}^n (y_i - \hat{y}_i)^2 \\
& = \sum_{i=1}^n [y_i - (b_0 + b_1 x_i)]^2
\end{align}
This will allow us to shrink the residuals toward 0: the values \(b_0\) and \(b_1\) that minimize this will make up our regression line.

This is a calculus-free course, so we'll skip the proof of the minimization part. The slope can be estimated as \[b_1 = \frac{s_y}{s_x}\times R\] and the intercept as \[b_0 = \bar{y} - b_1 \bar{x}\]

\hypertarget{coefficient-of-determination}{%
\subsection{Coefficient of Determination}\label{coefficient-of-determination}}

With the correlation and regression line in hand, we will add one last piece for considering the fit of a regression line. The \textbf{coefficient of determination}, \(R^2\), is the square of the correlation coefficient. This value tells us how much of the variability around the regression line is accounted for by the regression. An easy way to interpret this value is to assign it a letter grade. For example, if \(R^2 = 0.84\), the predictive capabilities of the regression line get a B.

\begin{quote}
\emph{Example}: Consider two measurements taken on the Old Faithful Geyser in Yellowstone National Park: \texttt{eruptions}, the length of each eruption and \texttt{waiting}, the time between eruptions. Each is measured in minutes.
\end{quote}

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-47-1.pdf}

\begin{quote}
There does appear to be some kind of linear relationship here, so we will see if we can use the wait time to predict the eruption duration. The sample statistics for these data are

\begin{longtable}[]{@{}ccc@{}}
\toprule()
& \texttt{waiting} & \texttt{eruptions} \\
\midrule()
\endhead
mean & \(\bar{x}=70.90\) & \(\bar{y}=3.49\) \\
sd & \(s_x=13.60\) & \(s_y=1.14\) \\
& & \(R = 0.90\) \\
\bottomrule()
\end{longtable}

Since we want to use wait time to predict eruption duration, wait time is \(x\) and eruption duration is \(y.\) Then \[b_1 = \frac{1.14}{13.60}\times 0.90 \approx 0.076 \] and \[b_0 = 3.49 - 0.076\times 70.90 \approx -1.87\] so the estimated regression line is \[\hat{y} = -1.87 + 0.076x\]

To interpret \(b_1\), the slope, we would say that for a one-minute increase in waiting time, we would predict a 0.076 minute increase in eruption duration. The intercept is a little bit trickier. Plugging in 0 for \(x\), we get a predicted eruption duration of \(-1.87\) minutes. There are two issues with this. First, a negative eruption duration doesn't make sense\ldots{} but it also doesn't make sense to have a waiting time of 0 minutes.
\end{quote}

It's important to stop and think about our predictions. Sometimes, the numbers don't make sense and it's easy to see that there's something wrong with the prediction. Other times, these issues are more insidious. Usually, all of these issues result from what we call \emph{extrapolation}, applying a model estimate for values outside of the data's range for \(x\). Our linear model is only an approximation, and we don't know anything about how the relationship outside of the scope of our data!

Consider the following data with the best fit line drawn on the scatterplot.

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-48-1.pdf}

The best fit line is \[\hat{y} = 2.69 + 0.179x\] and the correlation is \(R=0.877\). Then the coefficient of determination is \(R^2 = 0.767\) (think: a C grade), so the model has decent predictive capabilities. More precisely, the model accounts for 76.7\% of the variability about the regression line. Now suppose we wanted to predict the value of \(y\) when \(x=0.1\): \[\hat{y} = 2.66 + 0.181\times0.1 = 2.67\] This seems like a perfectly reasonable number\ldots{} But what if I told you that I generated the data using the model \(y = 2\ln(x) + \text{random error}\)? (If you're not familiar with the natural log, \(\ln\), don't worry about it! You won't need to use it.) The true (population) best-fit model would look like this:

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-49-1.pdf}

The vertical lines at \(x=5\) and \(x=20\) show the bounds of our data. The blue dot at \(x=0.1\) is the predicted value \(\hat{y}\) based on the linear model. The dashed horizontal line helps demonstrate just how far this estimate is from the true population value! This does \emph{not} mean there's anything inherently wrong with our model. If it works well from \(x=5\) to \(x=20\), great, it's doing its job!

\hypertarget{finding-a-regression-line-in-r}{%
\subsection*{Finding a Regression Line in R}\label{finding-a-regression-line-in-r}}
\addcontentsline{toc}{subsection}{Finding a Regression Line in R}

To find a regression line using R, we use the command \texttt{lm}, which stands for ``linear model''. The necessary argument is the \texttt{formula}, which takes the form \texttt{y\ \textasciitilde{}\ x}. For example, to find the regression line for the Old Faithful geyser data with waiting time predicting eruption duration, \[\text{eruptions} = b_0 + b_1\times\text{waiting}\] we would use \texttt{formula\ =\ eruptions\ \textasciitilde{}\ waiting}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ eruptions }\SpecialCharTok{\textasciitilde{}}\NormalTok{ waiting)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = eruptions ~ waiting)
## 
## Coefficients:
## (Intercept)      waiting  
##    -1.87402      0.07563
\end{verbatim}

The \texttt{lm} command prints out the intercept, \(b_0\) and the slope (waiting), \(b_1\). So the model is \[\text{eruptions} = 7.31 - 0.05\times\text{waiting}\]

To get the coefficient of determination, we can simply find and square the correlation coefficient. I will do this by putting the \texttt{cor()} command in parentheses and squaring it by adding \texttt{\^{}2} at the end:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\FunctionTok{cor}\NormalTok{(}\AttributeTok{x =}\NormalTok{ waiting, }\AttributeTok{y =}\NormalTok{ eruptions))}\SpecialCharTok{\^{}}\DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.8114608
\end{verbatim}

We might also be interested in adding a regression line to a scatterplot. Now that we know how to find both separately, we can put them together. I can add a line to a plot in R by using the command \texttt{abline}. This command takes two primary arguments, along with some optional arguments for adjusting line color and thickness:

\begin{itemize}
\tightlist
\item
  \texttt{a}: the intercept (\(b_0\))
\item
  \texttt{b}: the slope (\(b_1\))
\item
  \texttt{col}: the line color
\item
  \texttt{lwd}: the line width
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(}\AttributeTok{x =}\NormalTok{ waiting, }\AttributeTok{y =}\NormalTok{ eruptions,}
     \AttributeTok{main =} \StringTok{"Scatterplot of Old Faithful Geyser Data"}\NormalTok{,}
     \AttributeTok{xlab =} \StringTok{"Waiting Time (minutes)"}\NormalTok{,}
     \AttributeTok{ylab =} \StringTok{"Eruption Duration (minutes)"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{a =} \FloatTok{7.313}\NormalTok{, }\AttributeTok{b =} \SpecialCharTok{{-}}\FloatTok{0.053}\NormalTok{,}
       \AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-52-1.pdf}

\hypertarget{probability-concepts}{%
\chapter{Probability Concepts}\label{probability-concepts}}

\hypertarget{chapter-overview-3}{%
\section{Chapter Overview}\label{chapter-overview-3}}

In previous chapters, we discussed ways to describe variables and the relationships between them. From here, we want to start asking inferential statistics questions like ``If my sample mean is 10, how likely is it that the population mean is actually 11?''. Probability is going to start us on this path.

Probability theory is the science of uncertainty and it is really interesting! But it can also be quite challenging. I try to frame probability around things most of us can do at home: flipping a coin, rolling a die, drawing from a deck of cards. You certainly don't need any of these things to get through this chapter, but you may find it helpful to have a coin/die/deck of cards on hand as you read through the examples.

Take your time running practice problems and going through the examples, using a tactile approach like sorting through your deck of cards whenever it seems helpful.

\textbf{Chapter Learning Objectives/Outcomes}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Find and interpret probabilities for equally likely events.
\item
  Find and interpret probabilities for events that are not equally likely.
\item
  Find and interpret joint and marginal probabilities.
\item
  Find and interpret conditional probabilities.
\item
  Use the multiplication rule and independence to calculate probabilities.
\end{enumerate}

R Objectives: \emph{none}

This chapter's outcomes correspond to course outcome (3) understand the basic rules of probability.

\hypertarget{experiments-sample-spaces-and-events}{%
\section{Experiments, Sample Spaces, and Events}\label{experiments-sample-spaces-and-events}}

\textbf{Probability} is the science of uncertainty. When we run an experiment, we are unsure of what the outcome will be. Because of this uncertainty, we say an experiment is a \textbf{random process}.

The probability of an event is the proportion of times it would occur if the experiment were run infinitely many times. For a collection of \emph{equally likely events}, this looks like:
\[
  \text{probability of event} = \frac{\text{number of ways event can occur}}{\text{number of possible (unique) outcomes}}
\]

An \textbf{event} is some specified possible outcome (or collection of outcomes) we are interested in observing.

\begin{quote}
\emph{Example}: If you want to roll a 6 on a six-sided die, there are six possible outcomes \(\{1,2,3,4,5,6\}\). In general, we assume that each die face is equally likely to appear on a single roll of the die, that is, that the die is \emph{fair}. So the probability of rolling a 6 is \[\frac{\text{number of ways to roll a 6}}{\text{number of possible rolls}} = \frac{1}{6}\]
\end{quote}

\begin{quote}
\emph{Example}: We can extend this to a collection of events, say the probability of rolling a 5 or a 6: \[\frac{\text{number of ways to roll a 5 or 6}}{\text{number of possible rolls}} = \frac{2}{6}\]
\end{quote}

The collection of all possible outcomes is called a \textbf{sample space}, denoted \(S\). For the six-sided die, \(S=\{1,2,3,4,5,6\}\).

To simplify our writing, we use \textbf{probability notation}:

\begin{itemize}
\tightlist
\item
  Events are assigned capital letters.
\item
  \(P(A)\) denotes the probability of event \(A\).
\item
  Sometimes we will also shorten simple events to just a number. For example, \(P(1)\) might represent ``the probability of rolling a 1''.
\end{itemize}

We can estimate probabilities from a sample using a frequency distribution.

\begin{quote}
\emph{Example}: Consider the following frequncy distribution from section 1.6

\begin{longtable}[]{@{}cc@{}}
\toprule()
Class & Frequency \\
\midrule()
\endhead
freshman & 12 \\
sophomore & 10 \\
junior & 3 \\
senior & 5 \\
\bottomrule()
\end{longtable}

If a student is selected \emph{at random} (meaning each student is equally likely to be selected), the probability of selecting a sophomore is \[\text{probability of sophomore} = \frac{\text{number of ways to select a sophomore}}{\text{total number of students}} = \frac{10}{30} \approx 0.3333\] The probability of selecting a \emph{junior or a senior} is \[\frac{\text{number of ways to select a junior or senior}}{\text{total number of students}} = \frac{3+5}{30} = \frac{8}{30} \approx 0.2667\]

Using probability notation, we might let \(A\) be the event we selected a junior and \(B\) be the event we selected a senior. Then \[P(A \text{ or } B) = 0.2667\]
\end{quote}

\hypertarget{probability-distributions}{%
\section{Probability Distributions}\label{probability-distributions}}

Two outcomes are \textbf{disjoint} or \textbf{mutually exclusive} if they cannot both happen (at the same time). Think back to how we developed bins for histograms - the bins need to be nonoverlapping - this is the same idea!

\begin{quote}
\emph{Example}: If I roll a six-sided die one time, rolling a 5 and rolling a 6 are disjoint. I can get a 5 \emph{or} a 6, but not both on the same roll.
\end{quote}

\begin{quote}
\emph{Example}: If I select a student, they can be a freshman \emph{or} a sophomore, but that student cannot be both a freshman and a sophomore at the same time.
\end{quote}

The outcome must be one event or the other (it cannot be both at the same time).

\hypertarget{venn-diagrams}{%
\subsection{Venn Diagrams}\label{venn-diagrams}}

\textbf{Venn Diagrams} show events as circles. The circles overlap where events share common outcomes.

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-54-1.pdf}

When a Venn Diagram has \emph{no overlap} the events are mutually exclusive. This Venn Diagram shows the event ``Draw a Diamond'' and the event ``Draw a Face Card''. There are 13 diamonds and 12 face cards in a deck. In this case, the events are \emph{not} mutually exclusive: it's possible to draw both a diamond and a face card at the same time: the Jack of Diamonds, Queen of Diamonds, and King of Diamonds.

For quick reference, an image of a full 52-card deck is linked below. The ``face cards'' are the J, Q, and K. Each row represents a ``suit''. From top to bottom, the suits are clubs, spades, hearts, and diamonds. Cards can be either red (hearts and diamonds) or black (spades and clubs).

\href{https://www.cis.upenn.edu/~cis110/17fa/hw/hw08/standard52.jpg}{Click here for a graphic of a standard 52 card deck.}

\emph{On your own}: Consider events

\begin{itemize}
\tightlist
\item
  \(A\): ``Draw a spade''
\item
  \(B\): ``Draw a queen''
\item
  \(C\): ``Draw a red''
\end{itemize}

Which of these events are mutually exclusive?

\hypertarget{probability-axioms}{%
\subsection{Probability Axioms}\label{probability-axioms}}

A \textbf{probability distribution} lists all possible disjoint outcomes (think: all possible values of a variable) and their associated probabilities. This can be in the form of a table

\begin{longtable}[]{@{}ccccccc@{}}
\toprule()
Roll of a six-sided die & 1 & 2 & 3 & 4 & 5 & 6 \\
\midrule()
\endhead
Probability & 1/6 & 1/6 & 1/6 & 1/6 & 1/6 & 1/6 \\
\bottomrule()
\end{longtable}

(note that we could visualize this with a bar plot!) or an equation, which we will discuss in a later chapter.

The \textbf{probability axioms} are requirements for a valid probability distribution. They are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  All listed outcomes must be disjoint.
\item
  Each probability must be between 0 and 1.
\item
  The probabilities must sum to 1.
\end{enumerate}

Note that \#2 is true for ALL probabilities. If you ever calculate a probability and get a negative number or a number greater than 1, you know something went wrong!

\begin{quote}
\emph{Example}: Use the probability axioms to check whether the following tables are probability distributions.

\begin{enumerate}
\def\labelenumi{\Alph{enumi})}
\tightlist
\item
\end{enumerate}

\begin{longtable}[]{@{}cccc@{}}
\toprule()
X & \{1 or 2\} & \{3 or 4\} & \{5 or 6\} \\
\midrule()
\endhead
P(X) & 1/3 & 1/3 & 1/3 \\
\bottomrule()
\end{longtable}

Each axiom is satisfied, so this is a valid probability distribution.

\begin{enumerate}
\def\labelenumi{\Alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
\end{enumerate}

\begin{longtable}[]{@{}ccccc@{}}
\toprule()
Y & \{1 or 2\} & \{2 or 3\} & \{3 or 4\} & \{5 or 6\} \\
\midrule()
\endhead
P(Y) & 1/3 & 1/3 & 1/3 & -1/3 \\
\bottomrule()
\end{longtable}

In this case, the outcomes are not disjoint and one of the probabilities is negative, so this is \emph{not} a valid probability distribution.
\end{quote}

Probability distributions look a lot like relative frequency distributions. This isn't a coincidence! In fact, a relative frequency distribution is a good way to use data to approximate a probability distribution.

\hypertarget{exercises}{%
\subsection*{Exercises}\label{exercises}}
\addcontentsline{toc}{subsection}{Exercises}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Use the probability axioms to determine whether each of the following is a valid probability distribution:

  \begin{enumerate}
  \def\labelenumii{\Alph{enumii}.}
  \tightlist
  \item
  \end{enumerate}

  \begin{longtable}[]{@{}ccccc@{}}
  \toprule()
  x & 0 & 1 & 2 & 3 \\
  \midrule()
  \endhead
  P(x) & 0.1 & 0.2 & 0.1 & 0.3 \\
  \bottomrule()
  \end{longtable}

  \begin{enumerate}
  \def\labelenumii{\Alph{enumii}.}
  \setcounter{enumii}{1}
  \tightlist
  \item
  \end{enumerate}

  \begin{longtable}[]{@{}ccccc@{}}
  \toprule()
  x & 0 or 1 & 1 or 2 & 3 or 4 & 5 or 6 \\
  \midrule()
  \endhead
  P(x) & 0.1 & 0.2 & 0.4 & 0.3 \\
  \bottomrule()
  \end{longtable}
\item
  Determine whether the following events are mutually exclusive (disjoint).

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Your friend studies in the library. You study at home.
  \item
    You and your study group all earn As on an exam.
  \item
    You stay out until 3 am. You go to bed at 9 pm.
  \end{enumerate}
\item
  In a group of 24 people, 11 have cats and 13 have dogs. Four of them have both cats and dogs. Sketch a Venn Diagram for these events.
\end{enumerate}

\hypertarget{rules-of-probability}{%
\section{Rules of Probability}\label{rules-of-probability}}

Consider a six-sided die. \[P(\text{roll a 1 or 2}) = \frac{\text{2 ways}}{\text{6 outcomes}} = \frac{1}{3}.\] Notice that we get the same result by taking \[P(\text{roll a 1})+P(\text{roll a 2}) = \frac{1}{6}+\frac{1}{6} = \frac{1}{3}.\] It turns out this is widely applicable!

\hypertarget{addition-rules}{%
\subsection{Addition Rules}\label{addition-rules}}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Addition Rule for Disjoint Outcomes

If \(A_1\) and \(A_2\) are disjoint outcomes, then the probability that one of them occurs is \[P(A_1 \text{ or } A_2) = P(A_1)+P(A_2).\] This can also be extended to more than two disjoint outcomes: \[P(A_1 \text{ or } A_2 \text{ or } \dots \text{ or } A_k) = P(A_1)+P(A_2)+\dots + P(A_k)\] for \(k\) disjoint outcomes.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Now consider a deck of cards. Let \(A\) be the event that a card drawn is a diamond and let \(B\) be the event it is a face card. (Check back to 3.2 for the Venn Diagram of these events.)

\begin{itemize}
\tightlist
\item
  \(A\): \(\quad 2\diamondsuit\) \(3\diamondsuit\) \(4\diamondsuit\) \(5\diamondsuit\) \(6\diamondsuit\) \(7\diamondsuit\) \(8\diamondsuit\) \(9\diamondsuit\) \(10\diamondsuit\) \(J\diamondsuit\) \(Q\diamondsuit\) \(K\diamondsuit\) \(A\diamondsuit\)
\item
  \(B\): \(\quad J\heartsuit\) \(Q\heartsuit\) \(K\heartsuit\) \(J\clubsuit\) \(Q\clubsuit\) \(K\clubsuit\) \(J\diamondsuit\) \(Q\diamondsuit\) \(K\diamondsuit\) \(J\spadesuit\) \(Q\spadesuit\) \(K\spadesuit\)
\end{itemize}

The collection of cards that are diamonds or face cards (or both) is

\(\quad A\diamondsuit\) \(2\diamondsuit\) \(3\diamondsuit\) \(4\diamondsuit\) \(5\diamondsuit\) \(6\diamondsuit\) \(7\diamondsuit\) \(8\diamondsuit\) \(9\diamondsuit\) \(10\diamondsuit\) \(J\diamondsuit\) \(Q\diamondsuit\) \(K\diamondsuit\) \(J\clubsuit\) \(Q\clubsuit\) \(K\clubsuit\) \(J\heartsuit\) \(Q\heartsuit\) \(K\heartsuit\) \(J\spadesuit\) \(Q\spadesuit\) \(K\spadesuit\)

Looking at these cards, I can see that there are 22 of them, so \[P(A \text{ or } B) = \frac{22}{52}\]

However, if I try to apply the addition rule for disjoint outcomes, \(P(A)=\frac{13}{52}\) and \(P(B)=\frac{12}{52}\) and I would get \(\frac{13+15}{52} = \frac{25}{52}\), which isn't what we want!

What happened? When I tried to add these, I \emph{double counted} the Jack of Diamonds, Queen of Diamonds, and King of Diamonds (the cards that are in both \(A\) and \(B\)). To deal with that, I need to subtract off the double count \(\frac{13}{52}+\frac{12}{52}-\frac{3}{52}\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

General Addition Rule

For any two events \(A\) and \(B\), the probability that \emph{at least} one will occur is \[P(A \text{ or } B) = P(A)+P(B)-P(A \text{ and }B).\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Notice that when we say ``or'', we include the situations where A is true, B is true, and the situation where are both A and B are true. This is an \emph{inclusive or}. Basically, if I said ``Do you like cats or dogs?'' and you said ``Yes.'' because you like cats \emph{and} dogs, that would be a perfectly valid response. I recommend using the inclusive or with your friends any time you want to get out of making a decision.

Also notice that the general addition rule applies to \emph{any} two events, even disjoint events. This is because, for disjoint events, \(P(A \text{ and } B) = 0\); it's impossible for both to occur at the same time!

\hypertarget{complements}{%
\subsection{Complements}\label{complements}}

The \textbf{complement} of an event is all of the outcomes in the sample space that are \emph{not} in the event. For an event \(A\), we denote its complement by \(A^c\).

\begin{quote}
\emph{Example}: For a single roll of a six-sided die, the sample space is all possible rolls: 1, 2, 3, 4, 5, or 6. If the event \(A\) is rolling a 1 or a 2, then the complement of this event, denoted \(A^c\), is rolling a 3, 4, 5, or 6.

We could also write this in probability notation: \(S = \{1, 2, 3, 4, 5, 6\}\) and if \(A=\{1,2\}\), then \(A^c=\{3, 4, 5, 6\}\).
\end{quote}

\textbf{Property}: \[P(A \text{ or } A^c)=1\] Using the addition rule, \[P(A \text{ or } A^c) = P(A)+P(A^c) = 1.\] Make sure you can convince yourself that \(A\) and \(A^c\) are \emph{always} disjoint.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Complement Rule

\[P(A) = 1-P(A^c).\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{quote}
\emph{Example}: Consider rolling 2 six-sided dice and taking their sum. The event of interest is a sum less than 12. Find

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(A^c\)
\item
  \(P(A^c)\)
\item
  \(P(A)\)
\end{enumerate}

If \(A =\) (sum less than 12), then \(A^c =\) (sum greater than or equal to 12). Take a moment to notice that there is only one way to get a sum greater than or equal to 12: rolling two 6s.

The chart below shows the rolls of Die 1 as columns and the rolls for Die 2 as rows. The numbers in the middle are the sums. Note that there are 36 possible ways to roll 2 dice.

\begin{longtable}[]{@{}lcccccc@{}}
\toprule()
& 1 & 2 & 3 & 4 & 5 & 6 \\
\midrule()
\endhead
\textbf{1} & 2 & 3 & 4 & 5 & 6 & 7 \\
\textbf{2} & 3 & 4 & 5 & 6 & 7 & 8 \\
\textbf{3} & 4 & 5 & 6 & 7 & 8 & 9 \\
\textbf{4} & 5 & 6 & 7 & 8 & 9 & 10 \\
\textbf{5} & 6 & 7 & 8 & 9 & 10 & 11 \\
\textbf{6} & 7 & 8 & 9 & 10 & 11 & 12 \\
\bottomrule()
\end{longtable}

Even without the chart, by noting that there's only one way to get a sum greater than or equal to 12, we can quickly find \(P(A^C)\): \[ P(A^c) = \frac{1}{36}\]
But trying to count all of the ways to get \(A\) would take a long time! Instead, we can use \[P(A) = 1 - P(A^c) = 1-\frac{1}{36} = \frac{35}{36}\]
\end{quote}

\hypertarget{conditional-probability}{%
\section{Conditional Probability}\label{conditional-probability}}

A \textbf{contingency table} is a way to summarize \textbf{bivariate data}, or data from two variables.

\emph{Smallpox in Boston (1726)}

~

Inoculated

~

yes

no

total

Result

lived

238

{5136}

{5374}

died

6

844

850

total

{244}

5980

{6224}

{5136} is the count of people who lived AND were not inoculated.~

{6224} is the total number of observations.

{244} is the total number of people who were inoculated.

{5374} is the total number of people who lived.

This is basically a two-variable frequency distribution. And, like a frequency distribution, we can convert to proportions (relative frequencies) by dividing each count (each number) by the total number of observations:

~

Inoculated

~

yes

no

total

Result

lived

0.0382

{0.8252}

{0.8634}

died

0.0010

0.1356

0.1366

total

{0.0392}

0.9608

{1.0000}

{0.8252} is the proportion of people who lived AND were not inoculated.~

{1.000} is the proportion of total number of observations. Think of this as 100\% of the observations.

{0.0392} is the proportion of people who were inoculated.

{0.8634} is the proportion of people who lived.

The row and column totals are \textbf{marginal probabilities}. The probability of two events together (\(A\) and \(B\)) is a \textbf{joint probability}.

What can we learn about the result of smallpox if we already know something about inoculation status? For example, given that a person is inoculated, what is the probability of death? To figure this out, we restrict our attention to the 244 inoculated cases. Of these, 6 died. So the probability is 6/244.

This is called \textbf{conditional probability}, the probability of some event \(A\) if we know that event \(B\) occurred (or is true): \[P(A|B) = \frac{P(A\text{ and }B)}{P(B)}\] where the symbol \textbar{} is read as ``given''.

\begin{quote}
For death given inoculation, \[P(\text{death}|\text{inoculation}) = \frac{P(\text{death and inoculation})}{P(\text{inoculation})} = \frac{0.0010}{0.0392} = 0.0255.\]
Notice that we could also write this as \[P(\text{death}|\text{inoculation}) = \frac{P(\text{death and inoculation})}{P(\text{inoculation})} = \frac{6/6224}{244/6224} = \frac{6}{244},\] which is what we found when using the table to restrict our attention to only the inoculated cases.
\end{quote}

If knowing whether event \(B\) occurs tells us nothing about event \(A\), the events are \textbf{independent}. For example, if we know that the first flip of a (fair) coin came up heads, that doesn't tell us anything about what will happen next time we flip that coin.

We can test for independence by checking if \(P(A|B)=P(A)\).

\hypertarget{multiplication-rules}{%
\subsection{Multiplication Rules}\label{multiplication-rules}}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Multiplication Rule for Independent Processes

If \(A\) and \(B\) are independent events, then \[P(A \text{ and }B) = P(A)P(B).\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

We can extend this to more than two events: \[P(A \text{ and }B \text{ and } C \text{ and } \dots) = P(A)P(B)P(C)\dots.\]

Note that if \(P(A \text{ and }B) \ne P(A)P(B)\), then \(A\) and \(B\) are \emph{not} independent.

\begin{quote}
\emph{Example}: Find the probability of rolling a \(6\) on your first roll of a die and a \(6\) on your second roll.

Let \(A=\) (rolling a \(6\) on first roll) and \(B=\) (rolling a \(6\) on second roll). For each roll, the probabiltiy of getting a \(6\) is \(1/6\), so \(P(A) = \frac{1}{6}\) and \(P(B) = \frac{1}{6}\).

Then, because each roll is independent of any other rolls, \[P(A \text{ and }B) = P(A)P(B) = \frac{1}{6}\times\frac{1}{6} = \frac{1}{36}\]
\end{quote}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

General Multiplication Rule

If \(A\) and \(B\) are any two events, then \[P(A \text{ and }B) = P(A|B)P(B).\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Notice that this is just the conditional probability formula, rewritten in terms of \(P(A \text{ and }B)\)!

\begin{quote}
\emph{Example}: Suppose we know that 38.4\% of US households have dogs and that among those with dogs, 23.1\% have cats. Find the probability that a US household has both dogs and cats.

Let \(C=\) (household has cats) and \(D=\) (household has dogs). We know from the problem statement that \(P(D) = 0.384\).

The other piece tells us something about the probability of having cats \emph{among those with dogs}. This means that we \emph{know} that these people have dogs. That is, \emph{given} a household has dogs, the probabiltiy of cats is 23.1\%. In probability notation, \(P(C|D) = 0.231\). Then \[P(C \text{ and }D) = P(C|D)P(D) = 0.231\times 0.384 = 0.0887\] or the probability that a US household has both cats and dogs is 0.0887.
\end{quote}

\hypertarget{random-variables}{%
\chapter{Random Variables}\label{random-variables}}

\hypertarget{chapter-overview-4}{%
\section{Chapter Overview}\label{chapter-overview-4}}

In previous chapters, we introduced the idea of variables and examined their distributions. We also began our discussion on probability theory. Now, we extend these concepts into what are called random variables. We will introduce the concept of random variables in general and will discuss a specific type of distribution - the binomial distribution. Then we will discuss a continuous probability distribution, the normal distribution. The normal distribution will provide a foundation for much of the inference we will complete throughout the rest of this course.

\textbf{Chapter Learning Objectives/Outcomes}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Discuss discrete random variables using key terminology.
\item
  Express cumulative probabilities using probability notation.
\item
  Calculate the expected value and standard deviation of a discrete random variable.
\item
  Calculate binomial probabilities.
\item
  Convert normal distributions to standard normal distributions.
\item
  Calculate probabilities for a normal distribution using area under the curve.
\item
  Approximate binomial probabilities using the normal curve.
\end{enumerate}

\textbf{R Objectives}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Calculate binomial probabilities.
\item
  Find cumulative probabilities for the standard normal distribution.
\item
  Find percentiles.
\item
  Use R as a simple calculator.
\end{enumerate}

This chapter's outcomes correspond to course outcomes (4) use the binomial distribution as a model for discrete variables and (5) use the normal distribution as a model for continuous variables.

\hypertarget{discrete-random-variables}{%
\section{Discrete Random Variables}\label{discrete-random-variables}}

A \textbf{random variable} is a quantitative variable whose values are based on chance. By ``chance'', we mean that you can't \emph{know} the outcome before it occurs.

A \textbf{discrete random variable} is a random variable whose possible values can be listed.

Notation:

\begin{itemize}
\tightlist
\item
  \(x\),\(y\),\(z\) (lower case letters) denote variables.
\item
  \(X\), \(Y\), \(Z\) (upper case letters) denote \emph{random} variables.
\end{itemize}

In contrast to events, where we usually used letters toward the start of the alphabet, (random) variables are typically denoted by letters from the end of the alphabet.

\begin{itemize}
\tightlist
\item
  \(\{X=x\}\) denotes the event that the random variable \(X\) equals \(x\).
\item
  \(P(X=x)\) denotes the probability that the random variable \(X\) equals \(x\).
\end{itemize}

Recall: a probability distribution is a list of all possible values and their corresponding probabilities. (See Section 3.3 for a refresher.) A \textbf{probability histogram} is a histogram where the heights of the bars correspond to the probability of each value. (This is very similar to a relative frequency histogram!) For discrete random variables, each ``bin'' is one of the listed values.

\begin{quote}
\emph{Example}:

\begin{longtable}[]{@{}lccccc@{}}
\toprule()
Number of Siblings, \(x\) & 0 & 1 & 2 & 3 & 4 \\
\midrule()
\endhead
\textbf{Probability}, \(P(X=x)\) & 0.200 & 0.425 & 0.275 & 0.075 & 0.025 \\
\bottomrule()
\end{longtable}

(Assume for the sake of the example that no one has more than 4 siblings.)
\end{quote}

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-55-1.pdf}

Interpretation: in a large number of independent observations of a random variable \(X\), the proportion of times each possible value occurs will approximate the probability distribution of \(X\).

\hypertarget{the-mean-and-standard-deviation}{%
\subsection{The Mean and Standard Deviation}\label{the-mean-and-standard-deviation}}

Mean of a Discrete Random Variable

The mean of a discrete random variable \(X\) is denoted \(\mu_X\). If it's clear which random variable we're talking about, we can drop the subscript and write \(\mu\).
\[
  \mu_X = \Sigma xP(X=x)
\]
where \(\Sigma\) denotes ``the sum over all values of \(x\)'': \[\Sigma xP(X=x) = x_1P(X=x_1) + x_2P(X=x_2) + \dots + x_nP(X=x_n).\]

The mean of a random variable is also called the \textbf{expected value} or \textbf{expectation}. Recall that measures of center are meant to identify the most common or most likely, thus the value we can \emph{expect} to see (most often).

\begin{quote}
\emph{Example}: for the Siblings distribution, \[\mu = 0(0.200)+1(0.425)+2(0.275)+3(0.075)+4(0.025)=1.3\]
Make sure you understand how we used the formula for \(\mu\) and the probability distribution to come up with this number.
\end{quote}

Interpretation: in a large number of independent observations of a random variable \(X\), the mean of those observations will approximately equal \(\mu\).

The larger the number of observations, the closer their average tends to be to \(\mu\). This is known as the \textbf{law of large numbers}.

\begin{quote}
\emph{Example}: Suppose I took a random sample of 10 people and asked how many siblings they have. \[2,2,2,2,1,0,3,1,2,0\] In my random sample of 10, \(\bar{x}=2\), which is a reasonable estimate but not that close to the true mean \(\mu=1.3\).

\begin{itemize}
\tightlist
\item
  A random sample of 30 gave me a mean of \(\bar{x}=1.53\).
\item
  A random sample of 100 gave me a mean of \(\bar{x}=1.47\).
\item
  A random sample of 1000 gave me a mean of \(\bar{x}=1.307\).
\end{itemize}
\end{quote}

We use concepts related to the law of large numbers as a foundation for statistical inference, but note that - although very large samples are nice to have - it's not necessary to take enormous samples all the time. Often, we can come to interesting conclusions with fewer than 30 observations!

Standard Deviation of a Discrete Random Variable

The variance of a discrete random variable \(X\) is denoted \(\sigma_X^2\) (or \(\sigma^2\) if it's clear which variable we're talking about).
\[ \sigma_X^2 = \Sigma[(x-\mu_X)^2P(X=x)]\]
OR
\[ \sigma_X^2 = \Sigma[x^2P(X=x)]-\mu_X^2\]
These formulas are \emph{exactly} equivalent and you may use whichever you wish, but note that the second may be a little easier to work with.

As before, the standard deviation is the square root of the variance: \[\sigma = \sqrt{\sigma^2}\]

\begin{quote}
\emph{Example}: Calculate the standard deviation of the Siblings variable.

In general, a table is the best way to keep track of a variance calculation:

\begin{longtable}[]{@{}ccccc@{}}
\toprule()
\(x\) & \(P(X=x)\) & \(xP(X=x)\) & \(x^2\) & \(x^2P(X=x)\) \\
\midrule()
\endhead
0 & 0.200 & 0 & 0 & 0 \\
1 & 0.425 & 0.425 & 1 & 0.425 \\
2 & 0.275 & 0.550 & 4 & 1.100 \\
3 & 0.075 & 0.225 & 9 & 0.675 \\
4 & 0.025 & 0.100 & 16 & 0.400 \\
& & \(\mu\) = 1.3 & & Total = 2.6 \\
\bottomrule()
\end{longtable}

Then the variance is \[\sigma^2 = 2.6 - 1.3^2 = 0.9\] and the standard deviation is \[\sigma = \sqrt{0.9} = 0.9539.\]
\end{quote}

\hypertarget{the-binomial-distribution}{%
\section{The Binomial Distribution}\label{the-binomial-distribution}}

Think back to replication in an experiment. Each replication is what we call a \textbf{trial}. We will consider a setting where each trial has two possible outcomes.

\begin{quote}
For example, suppose you want to know if a coin is fair (both sides equally likely). You might flip the coin 100 times (thus running 100 trials). Each trial is a flip of the coin with two possible outcomes: heads or tails.
\end{quote}

The product of the first \(k\) positive integers \((1, 2, 3, \dots)\) is called \textbf{k-factorial}, denoted \(k!\): \[k! = k \times (k-1) \times\dots\times 3 \times 2 \times 1\] We define \(0!=1\).

\begin{quote}
\emph{Example}: \(5! = 5 \times 4 \times 3 \times 2 \times 1 = 120\)
\end{quote}

If \(n\) is a positive integer \((1, 2, 3, \dots)\) and \(x\) is a nonnegative integer \((0, 1, 2, \dots)\) with \(x \le n\), the \textbf{binomial coefficient} is \[\binom{n}{x} = \frac{n!}{x!(n-x)!}\]

\begin{quote}
\emph{Example}: \[\binom{5}{2} = \frac{5!}{2!(5-2)!} = \frac{5 \times 4 \times 3 \times 2 \times 1}{(2 \times 1)(3 \times 2 \times 1)}\]
\end{quote}

Sometimes, we may want to simplify a binomial coefficient \emph{before} taking all of the factorials. Why? Well, \[20! = 2432902008176640000\] Most calculators will not print this number. Instead, you'll get an error or a rounded version printed using scientific notation. Neither will help you accurately calculate the binomial coefficient.

\begin{quote}
\emph{Example}: \[\binom{20}{17} = \frac{20\times 19\times 18\times 17\times 16\times \dots \times 3\times 2\times 1}{(17\times 16\times \dots \times 3\times 2\times 1)(3\times 2\times 1)}\] but notice that I can rewrite \(20!\) as \(20\times 19\times 18\times 17!\), so \[\binom{20}{17} = \frac{20\times 19\times 18\times 17!}{17!(3\times 2\times 1)} = \frac{20\times 19\times 18}{3\times 2\times 1} = \frac{6840}{6} = 1140\]
\end{quote}

\textbf{Bernoulli trials} are repeated trials of an experiment where:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Each trial has two possible outcomes: success and failure.
\item
  Trials are independent.
\item
  The probability of success (the \textbf{success probability}) \(p\) remains the same from one trial to the next: \[P(X=\text{success})=p\]
\end{enumerate}

The \textbf{binomial distribution} is the probability distribution for the number of successes in a sequence of Bernoulli trials.

Fact: in \(n\) Bernoulli trials, the number of outcomes that contain exactly \(x\) successes equals the binomial coefficient \(\binom{n}{x}\).

Binomial Probability Formula

Let \(x\) denote the total number of successes in \(n\) Bernoulli trials with success probability \(p\). The probability distribution of the random variable \(X\) is given by \[P(X=x) = \binom{n}{x}p^x(1-p)^{n-x} \quad\quad x = 0,1,2,\dots,n\] The random variable \(X\) is called a \textbf{binomial random variable} and is said to have the \textbf{binomial distribution}. Because \(n\) and \(p\) fully define this distribution, they are called the distribution's \textbf{parameters}.

To find a binomial probability formula:

Check assumptions.

Exactly \(n\) trials to be performed.

Two possible outcomes for each trial.

Trials are independent (each trial does not impact the result of the next)

Success probability \(p\) remains the same from trial to trial.

Identify a ``success''. Generally, this is whichever of the two possible outcomes we are most interested in.

Determine the success probability \(p\).

Determine \(n\), the number of trials.

Plug \(n\) and \(p\) into the binomial distribution formula.

We can also use the binomial probability formula to calculate probabilities like \(P(X\le x)\). Notice that we can rewrite this using concepts from the previous chapter \[P(X \le k) = P(X=k \text{ or } X=k-1 \text{ or } \dots  \text{ or } X=2 \text{ or } X=1  \text{ or } X=0)\] Since \(X\) is a discrete random variable, each possible value is \emph{disjoint}. We can use this! \[P(X \le k) = P(X=k) + P(X=k-1) + \dots + P(X=2) + P(X=1) + P(X=0)\]

\begin{quote}
\emph{Example}: \(P(X \le 3) = P(X=3)+P(X=2)+P(X=1)+P(X=0)\)
\end{quote}

We can also extend this concept to work with probabilities like \(P(a < X \le b)\).

\begin{quote}
\emph{Example}: \(P(2 < X \le 5)\)

First, notice that if \(2 < X \le 5\), then \(X\) can be 3, 4, or 5: \[P(2 < X \le 5) = P(X=3)+P(X=4)+P(X=5)\]
\end{quote}

\begin{quote}
Note: if going from \(2 < X \le 5\) to ``\(X\) can be 3, 4, or 5'' doesn't make sense to you, start by writing out the sample space. Suppose \(n=10\). Then the sample space for the binomial distribution is \[S = \{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\}\] Then I can check any number in this sample space by plugging it in for \(X\). So for 1, I can check \(2 < 1 \le 5\). Obviously this is not true, so we won't include 1. Checking the number 2, I get \(2 < 2 \le 5\). Since 2 \textless{} 2 is NOT true, we don't include 2. Etc.
\end{quote}

\hypertarget{mean-and-variance}{%
\subsection{Mean and Variance}\label{mean-and-variance}}

The shape of a binomial distribution is determined by the success probability:

\begin{itemize}
\tightlist
\item
  If \(p \approx 0.5\), the distribution is approximately symmetric.
\item
  If \(p < 0.5\), the distribution is right-skewed.
\item
  If \(p > 0.5\), the distribution is left-skewed.
\end{itemize}

The mean of a binomial distribution is \(\mu = np\). The variance is \(\sigma^2 = np(1-p)\).

\hypertarget{binomial-probabilities-in-r}{%
\subsection*{Binomial Probabilities in R}\label{binomial-probabilities-in-r}}
\addcontentsline{toc}{subsection}{Binomial Probabilities in R}

When it comes to calculating binomial probabilities, hand calculations can be cumbersome. Fortunately, this is another thing we can do in R!

Approximately 66\% of US adults take prescription medications. Find the probability that, in a sample of 100 adults, exactly 65 take prescription drugs.

We want to find \(P(X = 65)\) where \(X\) has a binomial distribution with \(n=100\) and \(p=0.66\). (Take a moment on your own to make sure you can convince yourself that this satisfies the conditions for a binomial setting and that you understand how we got here from the prompt above.)

Instead of doing this by hand (the larger \(n\) is, the more difficult this tends to get!), we will use the \texttt{dbinom} command in R. The \texttt{dbinom} command takes in the following information:

\begin{itemize}
\tightlist
\item
  \texttt{x} the value \(x\) takes on in the expression \(P(X=x)\)
\item
  \texttt{size} the value of \(n\)
\item
  \texttt{prob} the probability \(p\)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dbinom}\NormalTok{(}\AttributeTok{x =} \DecValTok{65}\NormalTok{, }\AttributeTok{size =} \DecValTok{100}\NormalTok{, }\AttributeTok{prob =} \FloatTok{0.66}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.0815753
\end{verbatim}

So without doing any hand calculations, I find that \(P(X=65) = 0.082\); the probability that exactly 65 of 100 randomly selected US adults take prescription medication is 0.082.

Suppose now we want to find \(P(63 < X < 68)\). How can we manage that? We can figure out that this probability includes numbers between 63 and 68, but does not include 63 or 68. In fact, it is the numbers 64 through 67. SO we can break this up as \[ P(63 < X < 68) = P(X=64) + P(X=65)+ P(X=66) + P(X=67) \]

In R, we can get a sequence of whole numbers using the format \texttt{a:b}. For example

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{64}\SpecialCharTok{:}\DecValTok{67}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 64 65 66 67
\end{verbatim}

gives all whole numbers from 64 through 67.

I can then put this directly into the \texttt{dbinom} command!

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dbinom}\NormalTok{(}\AttributeTok{x =} \DecValTok{64}\SpecialCharTok{:}\DecValTok{67}\NormalTok{, }\AttributeTok{size =} \DecValTok{100}\NormalTok{, }\AttributeTok{prob =} \FloatTok{0.66}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.07587601 0.08157530 0.08397457 0.08272122
\end{verbatim}

This produces each individual probability \(P(X=64)\), \(P(X=65)\), \(P(X=66)\), and \(P(X=67)\). To quickly add these up, I am going to use the \texttt{sum} command. Notice that I put the entire \texttt{dibnom} command \emph{in the parentheses} of the \texttt{sum()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sum}\NormalTok{(}
  \FunctionTok{dbinom}\NormalTok{(}\AttributeTok{x =} \DecValTok{64}\SpecialCharTok{:}\DecValTok{67}\NormalTok{, }\AttributeTok{size =} \DecValTok{100}\NormalTok{, }\AttributeTok{prob =} \FloatTok{0.66}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.3241471
\end{verbatim}

And so \(P(63 < X < 68) = 0.324\); the probability that between 64 and 67 (inclusive) US adults in a sample of 100 take prescription medication is 0.324.

\hypertarget{the-normal-distribution}{%
\section{The Normal Distribution}\label{the-normal-distribution}}

If we can represent a discrete variable with a probability histogram, what can we do with a continuous variable?

We represent the shape of a continuous variable using a \textbf{density curve}. This is like a histogram, but with a smooth curve:
\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-60-1.pdf}

Properties:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The curve is always above the horizontal axis (because probabilities are always nonnegative).
\item
  The total area under the curve equals 1.
\end{enumerate}

For a variable with a density curve, the proportion of all possible observations that lie within a specified range equals the corresponding area under the density curve.

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-61-1.pdf}

A \textbf{normal curve} is a special type of density curve that has a ``bell-shaped'' distribution. In fact, all of the density curves I've shown in this section have been normal curves! We say that a variable is \textbf{normally distributed} or has a \textbf{normal distribution} if its distribution has the shape of a normal curve.

Why ``normal''? Because it appears so often in practice! Lots of things are more common around the average and less common as you get farther from the average: height, amount of sleep people get each night, standardized test scores, etc. (In practice, these things aren't \emph{exactly} normally distributed\ldots{} instead, they're \textbf{approximately normally distributed} - and that's ok.)

Normal distributions\ldots{}

\begin{itemize}
\tightlist
\item
  are fully determined by parameters mean \(\mu\) and standard deviation \(\sigma\).
\item
  are symmetric and centered at \(\mu\).
\item
  have spreads that depend on \(\sigma\).
\end{itemize}

Pay close attention to the horizontal axis and how spread out the densities are in each of the following plots:

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-62-1.pdf}

Notice that the bottom left plot comes to a sharper peak, while the bottom right has a gentler slope. This is what we mean by ``spread'': the density on the bottom right is the most spread out.

\hypertarget{z-scores}{%
\subsection{Z-Scores}\label{z-scores}}

We \textbf{standardize} a variable using \[z = \frac{x-\mu}{\sigma}.\] This is also called a \textbf{z-score}. Standardizing using this formula will \emph{always} result in a variable with mean 0 and standard deviation 1 (even if it's not normal!). If \(X\) is approximately normal, then the standardized variable \(Z\) will have a standard normal distribution.

Note: when we z-score a variable, we preserve the area under the curve properties! If \(X\) is Normal\((\mu,\sigma)\), then \[P(X < c) = P\left(Z < \frac{c - \mu}{\sigma}\right) = P(Z < z).\]

Because z-scores always result in variables with mean 0 and standard deviation 1, they are also very useful for comparing values which are originally on very different scales.

Note that a z-score tells us how many standard deviations an observation is from the mean. A positive z-score \(z>0\) is \emph{above} the mean; a negative z-score \(z<0\) is \emph{below} the mean.

\begin{quote}
\emph{Example}: \(z=-0.23\) is 0.23 standard deviations below the mean.
\end{quote}

\hypertarget{empirical-rule-for-variables}{%
\subsection{Empirical Rule for Variables}\label{empirical-rule-for-variables}}

For any (approximately) normally distributed variable,

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Approximately 68\% of all possible observations lie within one standard deviation of the mean: \(\mu \pm \sigma.\)
\item
  Approximately 95\% of all possible observations lie within two standard deviations of the mean: \(\mu \pm 2\sigma.\)
\item
  Approximately 99.7\% of all possible observations lie within three standard deviations of the mean: \(\mu \pm 3\sigma.\)
\end{enumerate}

Given some data, you can check if approximately 68\% of the data falls within \(\bar{x}\pm s\), 95\% within \(\bar{x}\pm 2s\), and 99.7\% within \(\bar{x}\pm 3s\) to examine whether the data follow the empirical rule.

To check whether a variable is (approximately) normally distributed, we can check the histogram to see if it is symmetric and bell-shaped\ldots{} or we can check to see if the variable conforms (approximately) to the empirical rule! If we decide it is approximately normal, we can estimate the parameters: \(\mu\) using \(\bar{x}\) and \(\sigma\) using \(s\).

\hypertarget{area-under-the-standard-normal-curve}{%
\section{Area Under the Standard Normal Curve}\label{area-under-the-standard-normal-curve}}

In order to make normal distributions easier to work with, we will standardize them. A \textbf{standard normal distribution} is a normal distribution with mean \(\mu=0\) and standard deviation \(\sigma=1\).

Properties:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Total area under the curve is 1.
\item
  The curve extends infinitely in both directions, never touching the horizontal axis.
\item
  Symmetric about 0.
\item
  Almost all of the area under the curve is between -3 and 3.
\end{enumerate}

We will think about area under the standard normal curve in terms of \textbf{cumulative probabilities} or probabilities of the form \(P(Z < z)\).

We will use the fact that the total area under the curve is 1 to find probabilities like \(P(Z > c)\):

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-63-1.pdf}

Using the graphic to help visualize, we can see that \[1 = P(Z < c) + P(Z > c)\] which we can then rewrite as
\[P(Z > c) = 1-P(Z<c).\]

We can also use this concept to find \(P(a < Z < b)\).

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-64-1.pdf}

Notice that \[1 = P(Z < a) + P(a < Z < b) + P(Z > b),\] which we can rewrite as \[P(a < Z < b) = 1 - P(Z > b) - P(Z < a)\] and since we just found that \(P(Z > b) = 1 - P(Z < b)\), we can replace \(1 - P(Z > b)\) with \(P(Z < b)\), and get \[P(a < Z < b) = P(Z < b) - P(Z < a).\]

Key Cumulative Probability Concepts

\begin{itemize}
\tightlist
\item
  \(P(Z > c) = 1 - P(Z < c)\)
\item
  \(P(a < Z < b) = P(Z < b) - P(Z < a)\)
\end{itemize}

A final note, because the normal distribution is symmetric, \(P(X < \mu) = P(X > \mu) = 0.5\). Notice this also implies that, when a distribution is symmetric (and unimodal), the mean and median are the same!

Now that we can get all of our probabilities written as \emph{cumulative} probabilities, we're ready to use software to find the area under the curve!

Finding Area Under the Curve: Applets

One option for finding probabilities and z-scores associated with the normal curve is to use an online applet. The Rossman and Chance Normal Probability Calculator is my preferred applet. It's relatively straightforward to use and would be difficult to demonstrate in these course notes! We will demonstrate this applet in class. I recommend you bookmark any websites you use to find probabilities!

You can also find the area under a normal distribution using a Normal Distribution Table. These are outdated and not used anywhere but the statistics classroom. As a result, I do not teach them.

\hypertarget{r-normal-distribution-probabilities}{%
\subsection*{R: Normal Distribution Probabilities}\label{r-normal-distribution-probabilities}}
\addcontentsline{toc}{subsection}{R: Normal Distribution Probabilities}

Standard normal probabilities of the form \(P(Z < z)\) are found using the command `pnorm(z)'. To find \(P(Z<1)\), I would type \texttt{pnorm(1)}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pnorm}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.8413447
\end{verbatim}

so \(P(Z < 1) = 0.841\).

A quick note about R: R will print very large numbers and numbers close to 0 using \emph{scientific notation}. However, R's scientific notation may not look the way you're used to! Check out the R output for \(P(Z < -5)\):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pnorm}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2.866516e-07
\end{verbatim}

When you see \texttt{e-07}, that means \(\times10^{-7}\)\ldots{} so \(P(Z < -5) = 2.8665 \times 10^{-7} \approx 0.00000029\).

\hypertarget{working-with-normally-distributed-variables}{%
\section{Working with Normally Distributed Variables}\label{working-with-normally-distributed-variables}}

\hypertarget{normal-distribution-probabilities}{%
\subsection{Normal Distribution Probabilities}\label{normal-distribution-probabilities}}

Using z-scores and area under the standard normal curve, we can find probabilities for any normal distribution problem!

Determining Normal Distribution Probabilities

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Sketch the normal curve for the variable.
\item
  Shade the region of interest and mark its delimiting x-value(s).
\item
  Find the z-score(s) for the value(s).
\item
  Use an applet (or the \texttt{pnorm} command in R) to find the associated area.
\end{enumerate}

\begin{quote}
\emph{Example}: Find the proportion of SAT-takers who score between 1150 and 1300. Assume that SAT scores are approximately normally distributed with mean \(\mu=1100\) and standard deviation \(\sigma = 200\).

First, let's figure out what we want to calculate. Using area under the curve concepts, the proportion of test-takers who score \emph{between} 1150 and 1300 will be \(P(1150 < X < 1300)\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Sketch:
\end{enumerate}
\end{quote}

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-67-1.pdf}

\begin{quote}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Shade and label:
\end{enumerate}
\end{quote}

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-68-1.pdf}

\begin{quote}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Calculate z-scores: \[x = 1150 \rightarrow z = \frac{1150-1100}{200} = 0.25\] and \[x=1300 \rightarrow z = \frac{1300-1100}{200} = 1.\]
\item
  Use an applet to find \(P(Z < 0.25) = 0.599\) and \(P(Z < 1) = 0.841\) \emph{or} use the \texttt{pnorm} command in R:
\end{enumerate}
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pnorm}\NormalTok{(}\FloatTok{0.25}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5987063
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pnorm}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.8413447
\end{verbatim}

\begin{quote}
Note that \[P(1150 < X < 1300) = P\left(\frac{1150-1100}{200} < Z < \frac{1300-1100}{200}\right) = P(0.25 < Z < 1)\] and, using cumulative probability concepts, \[P(0.25 < Z < 1) = P(Z < 1) - P(Z < 0.25).\] We found \(P(Z < 0.25) \approx 0.5987\) and \(P(Z < 1) \approx 0.8413\), so \[P(Z < 1) - P(Z < 0.25) \approx 0.8413 - 0.5987 = 0.2426.\] That is, approximately 26.26\% of test-takers score between 1150 and 1300 on the SAT.
\end{quote}

\hypertarget{percentiles}{%
\subsection{Percentiles}\label{percentiles}}

We can also find the \emph{observation} associated with a percentage/proportion.

The \(w\)th \textbf{percentile} \(p_w\) is the observation that is higher than w\% of all observations \[P(X < p_w) = w\]

Finding a Percentile

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Sketch the normal curve for the variable.
\item
  Shade the region of interest and label the area.
\item
  Use the applet (or R - see below) to determine the z-score for the area.
\item
  Find the x-value using \(z\), \(\mu\), and \(\sigma\).
\end{enumerate}

Note that if \(z = \frac{x-\mu}{\sigma}\), then \(x = \mu + z\sigma\).

\begin{quote}
\emph{Example}: Find the 90th percentile for SAT scores.

From the previous example, we know that SAT scores are approximately Normal(\(\mu=1100\), \(\sigma=200\)).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Sketch the normal curve.
\end{enumerate}
\end{quote}

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-70-1.pdf}

\begin{quote}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Shade the region of interest and label the area.
\end{enumerate}
\end{quote}

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-71-1.pdf}

\begin{quote}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Use the applet to determine the z-score for the area. This results in \(z = 1.28\).
\item
  Find the x-value using \(z\approx 1.28\), \(\mu=1100\), and \(\sigma=200\): \[x = 1100 + 1.28(200) = 1356\] so 90\% of SAT test-takers score below 1356.
\end{enumerate}
\end{quote}

\hypertarget{r-normal-distribution-percentiles}{%
\subsubsection*{R: Normal Distribution Percentiles}\label{r-normal-distribution-percentiles}}
\addcontentsline{toc}{subsubsection}{R: Normal Distribution Percentiles}

Instead of using an applet, we can use the \texttt{qnorm} command in R to find the z-score corresponding to a percentile. In this case, we simply enter the percentile of interest \emph{expressed as a proportion} in the \texttt{qnorm} command. That is, to find the z score for the 90th percentile, we would enter

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qnorm}\NormalTok{(}\FloatTok{0.9}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.281552
\end{verbatim}

which gives the same result as the applet in the example above. Then, we can use R as a calculator to find the value of \(x\) (recall \(\mu=1100\) and \(\sigma=200\))

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1100} \SpecialCharTok{+} \FloatTok{1.281552}\SpecialCharTok{*}\DecValTok{200}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1356.31
\end{verbatim}

This gives us the same result as before, that 90\% of SAT test-takers score below 1356.

\hypertarget{introduction-to-confidence-intervals}{%
\chapter{Introduction to Confidence Intervals}\label{introduction-to-confidence-intervals}}

\hypertarget{chapter-overview-5}{%
\section{Chapter Overview}\label{chapter-overview-5}}

This chapter will bridge the gap between our discussion on the normal distribution and our first forays into statistical inference. As it turns out, much of the statistical inference we will use relies on the normal distribution and the t-distribution, which we will introduce in this chapter. We begin our study of statistical inference by learning about confidence intervals.

\textbf{Chapter Learning Objectives/Outcomes}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Find the distribution of a sample mean.
\item
  Estimate probabilities for a sample mean.
\item
  Calculate and interpret confidence intervals for a population mean.
\item
  Use the standard normal and t-distributions to find critical values.
\end{enumerate}

\textbf{R Objectives}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Find z and t critical values.
\item
  Generate complete confidence intervals for a population mean.
\end{enumerate}

This chapter's outcomes correspond to course outcome (6) apply statistical inference techniques of parameter estimation such as point estimation and confidence interval estimation and (7) apply techniques of testing various statistical hypotheses concerning population parameters.

\hypertarget{sampling-distributions}{%
\section{Sampling Distributions}\label{sampling-distributions}}

\hypertarget{sampling-error}{%
\subsection{Sampling Error}\label{sampling-error}}

We want to use a sample to learn something about a population, but no sample is perfect! \textbf{Sampling error} is the error resulting from using a sample to estimate a population characteristic.

If we use a sample mean \(\bar{x}\) to estimate \(\mu\), chances are that \(\bar{x}\ne\mu\) (they might be close but\ldots{} they might not be!). We will consider

\begin{itemize}
\tightlist
\item
  How close \emph{is} \(\bar{x}\) to \(\mu\)?
\item
  What if we took many samples and calculated \(\bar{x}\) many times?

  \begin{itemize}
  \tightlist
  \item
    How would that relate to \(\mu\)?
  \item
    What would be the distribution of these values?
  \end{itemize}
\end{itemize}

The distribution of a statistic (across all possible samples of size \(n\)) is called the \textbf{sampling distribution}. We will focus primarily on the distribution of the sample mean.

For a variable \(x\) and given a sample size \(n\), the distribution of \(\bar{x}\) is called the \textbf{sampling distribution of the sample mean} or the \textbf{distribution of \(\boldsymbol{\bar{x}}\)}.

\begin{quote}
\emph{Example}: Suppose our population is the five starting players on a particular basketball team. We are interested in their heights (measures in inches). The full population data is

\begin{longtable}[]{@{}lccccc@{}}
\toprule()
Player & A & B & C & D & E \\
\midrule()
\endhead
Height & 76 & 78 & 79 & 81 & 86 \\
\bottomrule()
\end{longtable}

The population mean is \(\mu=80\). Consider all possible samples of size \(n=2\):

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0625}}
  >{\centering\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0938}}
  >{\centering\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0938}}
  >{\centering\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0938}}
  >{\centering\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0938}}
  >{\centering\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0938}}
  >{\centering\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0938}}
  >{\centering\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0938}}
  >{\centering\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0938}}
  >{\centering\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0938}}
  >{\centering\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0938}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Sample
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
A,B
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
A,C
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
A,D
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
A,E
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
B,C
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
B,D
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
B,E
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
C,D
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
C,E
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
D,E
\end{minipage} \\
\midrule()
\endhead
\(\bar{x}\) & 77 & 77.5 & 78.5 & 81.0 & 78.5 & 79.5 & 82.0 & 80.0 & 82.5 & 83.5 \\
\bottomrule()
\end{longtable}

There are 10 possible samples of size 2. Of these samples, 10\% have means exactly equal to \(\mu\) (for a \emph{random} sample of size 2, you'd have a 10\% chance to find \(\bar{x}=\mu\)\ldots{} and a 90\% chance not to!).
\end{quote}

In general, the larger the sample size, the smaller the sampling error tends to be in estimating \(\mu\) using \(\bar{x}\).

In practice, we have one sample and \(\mu\) is unknown. We also have limited resources to collect data, so it may not be feasible to collect a very large sample.

The mean of the distribution of \(\bar{x}\) is \(\mu_{\bar{X}}=\mu\) and the standard deviation is \(\sigma_{\bar{X}}=\sigma/\sqrt{n}\). We refer to the standard deviation of a sampling distribution as \textbf{standard error}. (Note that this standard error formula is built for very large populations, so it will not work well for our basketball players. This is okay! We usually work with populations so large that we treat them as ``infinite''.)

\begin{quote}
\emph{Example}: The mean living space for a detatched single family home in the United States is 1742 ft\(^2\) with a standard deviation of 568 square feet. (Does that mean seem huge to anyone else??) For samples of 25 homes, determine the mean and standard error of \(\bar{x}\).

Using our formulae: \[\mu_{\bar{X}} = \mu = 1742\] and \[\sigma_{\bar{X}} = \frac{\sigma}{\sqrt{n}} = \frac{568}{\sqrt{25}} = 113.6.\]
\end{quote}

\hypertarget{the-central-limit-theorem}{%
\subsection{The Central Limit Theorem}\label{the-central-limit-theorem}}

Consider the setting where \(X\) is Normal(\(\mu\), \(\sigma\)). The plots below show (A) a random sample of 1000 from a Normal(100, 25) distribution and (B) the approximate sampling distribution of \(\bar{X}\) when X is Normal(100, 25).

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-74-1.pdf}

Notice how the x-axis changes from one plot to the next.

In fact, if \(X\) is Normal(\(\mu\), \(\sigma\)), then \(\bar{X}\) is Normal(\(\mu_{\bar{X}}=\mu\), \(\sigma_{\bar{X}}=\sigma/\sqrt{n}\)).

Surprisingly, we see a similar result for \(\bar{X}\) even when \(X\) is not normally distributed!

Central Limit Theorem

For relatively large sample sizes, the random variable \(\bar{X}\) is approximately normally distributed \emph{regardless of the distribution of} \(X\): \[\bar{X}\text{ is Normal}(\mu_{\bar{X}}=\mu, \sigma_{\bar{X}}=\sigma/\sqrt{n}).\]

Notes

\begin{itemize}
\tightlist
\item
  This approximation improves with increasing sample size.
\item
  In general, ``relatively large'' means sample sizes \(n \ge 30\).
\end{itemize}

\hypertarget{developing-confidence-intervals}{%
\section{Developing Confidence Intervals}\label{developing-confidence-intervals}}

Recall: A \textbf{point estimate} is a single-value estimate of a population parameter. We say that a statistic is an \textbf{unbiased estimator} if the mean of its distribution is equal to the population parameter. Otherwise, it is a \textbf{biased estimator}.

\emph{Comment}: Remember how our formula for sample variance, the ``mean squared deviance'' divides by \(n-1\) instead of \(n\)? We do this so that \(s\) is an \emph{unbiased} estimate of \(\sigma\).

Ideally, we want estimates that are unbiased with small standard error. For example, a sample mean (unbiased) with a large sample size (results in smaller standard error).

Point estimates are useful, but they only give us so much information. The variability of an estimate is also important!

\begin{quote}
\emph{Example} Think about estimating what tomorrow's weather will be like. If it's May in Sacramento, the average high temperature is 82 degrees Fahrenheit, but it's not uncommon to have highs anywhere from 75 to 90! Since the highs are so \emph{variable}, it's hard to be confident using 82 to predict tomorrow's weather.

On the flip side, think about July in Phoenix. The average high is 106 degrees Fahrenheit. In Phoenix, it's uncommon to have a July day with a high below 100. Since the highs are \emph{not variable}, you could feel pretty confident using 106 to predict tomorrow's weather.
\end{quote}

Take a look at these two boxplots:

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-75-1.pdf}

Both samples are size \(n=100\) and have \(\bar{x}=0\), which would be our point estimate for \(\mu\)\ldots{} but Variable 1 has a standard deviation of \(\sigma=0.5\) and Variable 2 has standard deviation \(\sigma=5\). As a result, we can be more confident in our estimate of the population mean for Variable 1 than for Variable 2.

We want to formalize this idea of confidence in our estimates. A \textbf{confidence interval} is an interval of numbers based on the point estimate of the parameter. Say we want to be 95\% confident about a statement. In Statistics, this means that we have arrived at our statement using a method that will give us a correct statement 95\% of the time.

Our best point estimate for \(\mu\) (based on a random sample) is \(\bar{x}\), so that value will make up the center (or midpoint) of the interval. To create an interval around \(\bar{x}\), we will construct what is called the \textbf{margin of error}. We will use the variability of the data along with some normal distribution properties. This will look like \[z\times\frac{\sigma}{\sqrt{n}}\] The value \(z\) will come from the normal distribution and will be based on how confident we want to be, e.g., 95\% confident.

Putting everything together, the 95\% confidence interval is \[\left(\bar{x} - z_*\frac{\sigma}{\sqrt{n}}, \bar{x} + z_*\frac{\sigma}{\sqrt{n}}\right)\] where \(z_* = 1.96\). The value \(1.96\) is chosen because \((-1.96 < Z < 1.96) = 0.95\) (this is what makes it a 95\% confidence interval!).

\emph{Note}: A more detailed mathematical explanation of how we get this interval is available in Appendix C.

\hypertarget{interpreting-a-confidence-interval}{%
\subsection{Interpreting a Confidence Interval}\label{interpreting-a-confidence-interval}}

To interpret a confidence interval, we need to think back to our definition of probability as ``the proportion of times an event would occur if the experiment were run infinitely many times''. In the confidence interval case, if an experiment is run infinitely many times, the true value of \(\mu\) will be contained in 95\% of the intervals.

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-76-1.pdf}

The graphic above shows 95\% confidence intervals for 100 samples of size \(n=60\) drawn from a population with mean \(\mu=80\) and standard deviation \(\sigma=25\). Each sample's confidence interval is represented by a horizontal line. The dot in the middle of each is the sample mean. When a confidence interval does \emph{not} capture the population mean \(\mu\), the line is printed in red. Based on this concept of repeated sampling, we would expect about 95\% of these intervals to capture \(\mu\). In fact, 96 of the 100 intervals capture \(\mu\).

Finally, when you interpret a confidence interval, it is important to do so in the context of the problem.

\begin{quote}
\emph{Example} The preferred keyboard height for typists is approximately normally distributed with \(\sigma=2.0\). A sample of size \(n=31\), resulted in a mean prefered keyboard height of \(80 cm\). Find and interpret a 95\% confidence interval for keyboard height.

The interval is \[\bar{x} \pm z_{\alpha/2}\frac{\sigma}{\sqrt{n}} = 80.0 \pm 1.96\times\frac{2.0}{\sqrt{31}} = 80.0 \pm 0.70 = (79.3, 80.7).\] Interpretation:We can be 95\% confident that the mean preferred keyboard height for typists is between 79.3cm and 80.7cm.
\end{quote}

Notice that I kept the interpretation simple! That's okay - just be sure you are \emph{also} able to explain what it means to be 95\% confident (using the concept of repeated sampling).

Common mistakes:

\begin{itemize}
\tightlist
\item
  It is NOT accurate to say that ``the probability that \(\mu\) is in the confidence interval is 0.95''. The parameter \(\mu\) is some fixed quantity and it's either in the interval or it isn't.
\item
  We are NOT ``95\% confident that \(\bar{x}\) is in the interval''. The value \(\bar{x}\) is some known quantity and it's always in the interval.
\end{itemize}

\hypertarget{exercises-1}{%
\subsection{Exercises}\label{exercises-1}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Suppose I took a random sample of 50 Sac State students and asked about their SAT scores and found a mean score of 1112. Prior experience with SAT scores in the CSU system suggests that SAT scores are well-approximated by a normal distribution with standard deviation known to be 50.

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Find a 95\% confidence interval for Sac State SAT scores.
  \item
    Interpret your interval in the context of the problem.
  \item
    What is the width of your interval? If you want a narrower interval, what could you do?
  \end{enumerate}
\end{enumerate}

\hypertarget{other-levels-of-confidence}{%
\section{Other Levels of Confidence}\label{other-levels-of-confidence}}

While the 95\% confidence interval is common in research, there's nothing inherently special about it. You could calculate a 90\%, a 99\%, or - if you're feeling spicy - something like a 43.8\% confidence interval. These numbers are called the \textbf{confidence level} and they represent the proportion of times that the parameter will fall in the interval (if we took many samples).

The 100(1-\(\alpha\))\% confidence interval for \(\mu\) is given by \[\bar{x}\pm z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\] where \(z_{\alpha/2}\) is the z-score associated with the \([1-(\alpha/2)]\)th percentile of the standard normal distribution. The value \(z_{\alpha/2}\) is called the \textbf{critical value} (``c.v.'' on the plot, below).

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-77-1.pdf}

Common Critical Values

\begin{longtable}[]{@{}ccc@{}}
\toprule()
Confidence Level & \(\alpha\) & Critical Value, \(z_{\alpha/2}\) \\
\midrule()
\endhead
90\% & 0.10 & 1.645 \\
95\% & 0.05 & 1.96 \\
98\% & 0.02 & 2.326 \\
99\% & 0.01 & 2.575 \\
\bottomrule()
\end{longtable}

\hypertarget{r-finding-critical-values}{%
\subsection*{R: Finding Critical Values}\label{r-finding-critical-values}}
\addcontentsline{toc}{subsection}{R: Finding Critical Values}

We can find critical values in R using the same command we used to find percentiles: \texttt{qnorm}. Recall that \(z_{\alpha/2}\) is the z-score associated with the \([1-(\alpha/2)]\)th percentile of the standard normal distribution. So for a \((1-\alpha)100\%\) confidence interval, we need to find the value of \(1-\alpha/2\) to input into the \texttt{qnorm} command.

For example, to find \(z_{\alpha/2}\) for a 93\% confidence interval, we would use \[(1-\alpha)100\% = 93\%\] to solve for \(\alpha\) and get \(\alpha = 0.07\). Then we need the \([1-(\alpha/2)]\)th percentile: \[1-\frac{\alpha}{2} = 1 - \frac{0.07}{2} = 0.965\] Finally, we enter this value into the \texttt{qnorm} command:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qnorm}\NormalTok{(}\FloatTok{0.965}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.811911
\end{verbatim}

and the critical value for a 93\% confidence interval is \(z_{\alpha/2}=1.812\).

\hypertarget{breaking-down-a-confidence-interval}{%
\subsection{Breaking Down a Confidence Interval}\label{breaking-down-a-confidence-interval}}

Consider \[\left(\bar{x}- z_{\alpha/2}\frac{\sigma}{\sqrt{n}}, \quad \bar{x} + z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\right)\] The key values are

\begin{itemize}
\tightlist
\item
  \(\bar{x}\), the sample mean
\item
  \(\sigma\), the population standard deviation
\item
  \(n\), the sample size
\item
  \(z_{\alpha/2}\), the critical value \[P(Z > z_{\alpha/2}) = \frac{\alpha}{2}\]
\end{itemize}

The value of interest is \(\mu\), the (unknown) population mean; the confidence interval gives us a reasonable range of values for \(\mu\).

In addition, the formula includes

\begin{itemize}
\tightlist
\item
  The standard error, \(\frac{\sigma}{\sqrt{n}}\)
\item
  The margin of error, \(z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\)
\end{itemize}

\hypertarget{confidence-level-precision-and-sample-size}{%
\subsection{Confidence Level, Precision, and Sample Size}\label{confidence-level-precision-and-sample-size}}

If we can be 99\% confident (or even higher), why do we tend to ``settle'' for 95\%?? Take a look at the common critical values (above) and the confidence interval formula \[\bar{x} \pm z_{\alpha/2}\frac{\sigma}{\sqrt{n}}.\] What will higher levels of confidence do to this interval? Think back to the intuitive interval width explanation with the weather. Mathematically, the same thing will happen: the interval will get wider! And remember, a narrow interval is a more informative interval. There is a trade off here between interval width and confidence. In general, the scientific community has settled on 95\% as a compromise between the two, but different fields may use different levels of confidence.

There is one other thing we can control in the confidence interval: the sample size \(n\). One strategy is to specify the confidence level and the maximum acceptable interval width and use these to determine sample size. We know that \[\text{interval width} \ge 2z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\] (Note: I use \(\ge\) because \(2z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\) is the \emph{maximum} interval width - we would still be happy if this value turned out to be smaller!) Letting interval width equal \(w\), we can solve for \(n\): \[ n \ge \left(2z_{\alpha/2}\frac{\sigma}{w}\right)^2\] Alternately, we may specify a maximum margin of error \(m\) instead: \[ n \ge \left(z_{\alpha/2}\frac{\sigma}{m}\right)^2\] Once we've done this calculation, we need a whole number for \(n\). Since \(n \ge\) something, we will \emph{always round up}.

\begin{quote}
\emph{Example} Suppose we want a 95\% confidence interval for the mean of a normally distributed population with standard deviation \(\sigma=10\). It is important for our margin of error to be no more than 2. What sample size do we need?

Using the formula for sample size with a desired margin of error, I can plug in \(z_{0.05/2}=1.96\), \(m=2\) and \(\sigma=10\): \[n = \left(1.96\times\frac{10}{2}\right)^2 = 96.04\]. So (rounding up!) I need a sample size of \emph{at least 97}.
\end{quote}

A few comments:

\begin{itemize}
\tightlist
\item
  As desired width/margin of error decreases, \(n\) will increase.
\item
  As \(\sigma\) increases, \(n\) will also increase. (More population variability will necessitate a larger sample size.)
\item
  As confidence level increases, \(n\) will also increase.
\end{itemize}

\hypertarget{exercises-2}{%
\subsection{Exercises}\label{exercises-2}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In the previous section, you worked with a random sample of 50 Sac State students with mean SAT score 1112. Prior experience with SAT scores in the CSU system suggests that SAT scores are well-approximated by a normal distribution with standard deviation known to be 50. Calculate a

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    98\% confidence interval.
  \item
    90\% confidence interval.
  \item
    Interpret each interval in the context of the problem. Comment on how the intervals change as you change the confidence level.
  \item
    Find the sample size required for a 98\% confidence interval with maximum margin of error 10.
  \end{enumerate}
\end{enumerate}

\hypertarget{confidence-intervals-for-a-mean}{%
\section{Confidence Intervals for a Mean}\label{confidence-intervals-for-a-mean}}

In practice, the value of \(\sigma\) is almost never known\ldots{} but we know that we can estimate \(\sigma\) using \(s\). Can we plug in \(s\) for \(\sigma\)? Sometimes!

Remember the Central Limit Theorem (Section 5.1)? For samples of size \(n \ge 30\), \(\bar{X}\) will be approximately normal even if \(X\) isn't. In this case, we can plug in \(s\) for \(\sigma\): \[\bar{x} \pm z_{\alpha/2}\frac{s}{\sqrt{n}}.\]

That setting is pretty straightforward! Now we need to consider the setting where \(n < 30\), which will require a bit of additional work.

\hypertarget{the-t-distribution}{%
\subsection{The T-Distribution}\label{the-t-distribution}}

Enter: the t-distribution. If \[Z = \frac{\bar{X}-\mu}{\sigma/\sqrt{n}}\] has a standard normal distribution (for \(X\) normal or \(n\ge30\)), the slightly modified \[T = \frac{\bar{X}-\mu}{s/\sqrt{n}}\] has what we call the \textbf{t-distribution} with \(n-1\) \textbf{degrees of freedom} (even when \(n < 30\)!). The only thing we need to know about degrees of freedom is that \(\text{df}=n-1\) is the t-distribution's only parameter.

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-79-1.pdf}

The t-distribution is symmetric and always centered at 0. When \(n\ge30\), the t-distribution is approximately equivalent to the standard normal distribution. For smaller sample sizes, the t-distribution has more area in the tails (and therefore less area in the center of the distribution). Therefore, we can always use t confidence intervals (even if \(n\) is large) because the critical values are essentially equivalent between the t and standard normal distributions for large values of \(n\). This is usually what happens in practice.

In practice, we plug in \(s\) for \(\sigma\) and almost always use a t critical value (instead of a z critical value): \(t_{\text{df}, \, \alpha/2}\). The t critical value is the \([1- \alpha/2]\)th percentile of the t-distribution with \(n-1\) degrees of freedom. The resulting 95\% confidence interval is \[\bar{x} \pm t_{\text{df}, \, \alpha/2}\frac{s}{\sqrt{n}}.\]

You may use the applet, Rossman and Chance t Probability Calculator to find t critical values. For this applet, enter the degrees of freedom \(n-1\) next to ``df''. Then check the top box under ``t-value probability'' and make sure the inequality is clicked to ``\textgreater{}'' . Enter the value of \(\alpha/2\) for the probability. Click anywhere else on the page and the applet will automatically fill in the box under ``t-value''. This is your t critical value.

\hypertarget{r-t-critical-values}{%
\subsection*{R: T Critical Values}\label{r-t-critical-values}}
\addcontentsline{toc}{subsection}{R: T Critical Values}

To find a t critical value, we will again use R, now with the command \texttt{qt}. (Notice that this is similar to the command for the standard normal distribution, but instead of ``norm'' for normal it has ``t'' for the t-distribution.) The \texttt{qt} command takes in the following:

\begin{itemize}
\tightlist
\item
  \texttt{p}, the probability
\item
  \texttt{df}, the degrees of freedom
\end{itemize}

For example, for a 98\% interval with a sample size of 15, \[100(1-\alpha) = 98 \implies \alpha=0.02\] Then \(1-\alpha/2 = 0.99\) and \(\text{df}=15-1=14\).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qt}\NormalTok{(}\AttributeTok{p =} \FloatTok{0.99}\NormalTok{, }\AttributeTok{df =} \DecValTok{14}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2.624494
\end{verbatim}

which gives the t critical value \(t_{14,\alpha/2} = 2.625\).

\hypertarget{r-confidence-intevals-for-a-mean}{%
\section*{R: Confidence Intevals for a Mean}\label{r-confidence-intevals-for-a-mean}}
\addcontentsline{toc}{section}{R: Confidence Intevals for a Mean}

To generate complete confidence intervals for a mean in R, we will use the command \texttt{t.test}. The two arguments we will use for confidence interval generation are

\begin{itemize}
\tightlist
\item
  \texttt{x}: the variable that contains the data we want to use to construct a confidence interval.
\item
  \texttt{conf.level}: the desired confidence level
\end{itemize}

We will continue to use the \texttt{Loblolly} pine tree data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{attach}\NormalTok{(Loblolly)}
\end{Highlighting}
\end{Shaded}

In this case, the variable of interest is \texttt{x\ =\ height} and the desired confidence level is \texttt{conf.level\ =\ 0.95}. We our confidence interval using the following command:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(}\AttributeTok{x =}\NormalTok{ height, }\AttributeTok{conf.level =} \FloatTok{0.95}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  One Sample t-test
## 
## data:  height
## t = 14.348, df = 83, p-value < 2.2e-16
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  27.87796 36.85085
## sample estimates:
## mean of x 
##   32.3644
\end{verbatim}

R printed more information than we know what to do with right now. That's ok! We will get to it later. Right now, focus your attention to where it says ``95 percent confidence interval:'' and the line below that, which gives the interval (27.88, 36.85). At the bottom, it also provides the sample mean height of 32.36 feet. Based on the R output, we can say that we are 95\% confident that the true mean height of the loblolly pines is between 27.88 and 36.85 feet (assuming the researchers took a random sample of trees).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{detach}\NormalTok{(Loblolly)}
\end{Highlighting}
\end{Shaded}

\hypertarget{introduction-to-hypothesis-testing}{%
\chapter{Introduction to Hypothesis Testing}\label{introduction-to-hypothesis-testing}}

\hypertarget{chapter-overview-6}{%
\section{Chapter Overview}\label{chapter-overview-6}}

In this chapter, we will continue our discussion on statistical inference with a discussion on hypothesis testing. In hypothesis testing, we take a more active approach to our data by asking questions about population parameters and developing a framework to answer those questions. We will root this discussion in confidence intervals before learning about several other approaches to hypothesis testing.

\textbf{Chapter Learning Outcomes/Objectives}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Test one sample means using

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    confidence intervals.
  \item
    the critical value approach.
  \item
    the p-value approach.
  \end{enumerate}
\end{enumerate}

\textbf{R Objectives}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Generate hypothesis tests for a mean.
\item
  Interpret R output for tests of a mean.
\end{enumerate}

This chapter's outcomes correspond to course outcomes (6) apply statistical inference techniques of parameter estimation such as point estimation and confidence interval estimation and (7) apply techniques of testing various statistical hypotheses concerning population parameters.

\hypertarget{logic-of-hypothesis-testing}{%
\section{Logic of Hypothesis Testing}\label{logic-of-hypothesis-testing}}

This section is framed in terms of questions about a population mean \(\mu\), but the same logic applies to \(p\) (and other population parameters).

One of our goals with statistical inference is to make decisions or judgements about the value of a parameter. A confidence interval is a good starting point, but we might also want to ask questions like

\begin{itemize}
\tightlist
\item
  Do cans of soda actually contain 12 oz?
\item
  Is Medicine A better than Medicine B?
\end{itemize}

A \textbf{hypothesis} is a statement that something is true. A hypothesis test involves two (competing) hypotheses:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The \textbf{null hypothesis}, denoted \(H_0\), is the hypothesis to be tested. This is the ``default'' assumption.
\item
  The \textbf{alternative hypothesis}, denoted \(H_A\) is the alternative to the null.
\end{enumerate}

Note that the subscript 0 is ``nought'' (pronounced ``not''). A \textbf{hypothesis test} helps us decide whether the null hypothesis should be rejected in favor of the alternative.

\begin{quote}
\emph{Example}: Cans of soda are labeled with ``12 FL OZ''. Is this accurate?

The default, or uninteresting, assumption is that cans of soda contain 12 oz.

\begin{itemize}
\tightlist
\item
  \(H_0\): the mean volume of soda in a can is 12 oz.
\item
  \(H_A\): the mean volume of soda in a can is NOT 12 oz.
\end{itemize}
\end{quote}

We can write these hypotheses in words (as above) or in statistical notation. The null specifies a single value of \(\mu\)

\begin{itemize}
\tightlist
\item
  \(H_0\): \(\mu=\mu_0\)
\end{itemize}

We call \(\mu_0\) the \textbf{null value}. When we run a hypothesis test, \(\mu_0\) will be replaced by some number. For the soda can example, the null value is 12. We would write \(H_0: \mu = 12\).

The alternative specifies a \emph{range} of possible values for \(\mu\):

\begin{itemize}
\tightlist
\item
  \(H_A\): \(\mu\ne\mu_0\). ``The true mean is different from the null value.''
\end{itemize}

\textbf{The Logic of Hypothesis Testing}

Take a random sample from the population. If the data area consistent with the null hypothesis, do not reject the null hypothesis. If the data are inconsistent with the null hypothesis \emph{and} supportive of the alternative hypothesis, reject the null in favor of the alternative.

\begin{quote}
\emph{Example}: One way to think about the logic of hypothesis testing is by comparing it to the U.S. court system. In a jury trial, jurors are told to assume the defendant is ``innocent until proven guilty''. Innocence is the default assumption, so

\begin{itemize}
\tightlist
\item
  \(H_0\): the defendant is innocent.
\item
  \(H_A\): the defendant is guilty.
\end{itemize}

Like in hypothesis testing, it is not the jury's job to decide if the defendant is innocent. That should be their default assumption. They are only there to decide if the defendant is guilty or if there is not enough evidence to override that default assumption. The \emph{burden of proof} lies on the alternative hypothesis.
\end{quote}

Notice the careful language in the logic of hypothesis testing: we either reject, or fail to reject, the null hypothesis. We never ``accept'' a null hypothesis.

\hypertarget{decision-errors}{%
\subsection{Decision Errors}\label{decision-errors}}

\begin{itemize}
\tightlist
\item
  A \textbf{Type I Error} is rejecting the null when it is true. (Null is true, but we conclude null is false.)
\item
  A \textbf{Type II Error} is not rejecting the null when it is false. (Null is false, but we do not conclude it is false.)
\end{itemize}

\(H_0\) is

True

False

Decision

Do not reject \(H_0\)

Correct decision

Type II Error

Reject \(H_0\)

Type I Error

Correct decision

\begin{quote}
\emph{Example}: In our jury trial,

\begin{itemize}
\tightlist
\item
  \(H_0\): the defendant is innocent.
\item
  \(H_A\): the defendant is guilty.
\end{itemize}

A Type I error is concluding guilt when the defendant is innocent. A Type II error is failing to convict when the person is guilty.
\end{quote}

How likely are we to make errors? Well, \(P(\)Type I Error\()=\alpha\), the \textbf{significance level}. (Yes, this is the same \(\alpha\) we saw in confidence intervals!) For Type II error, \(P(\)Type II Error\()=\beta\). This is related to the sample size calculation from the previous chapter, but is otherwise something we don't have time to cover.

We would like both \(\alpha\) and \(\beta\) to be small but, like many other things in statistics, there's a trade off! For a fixed sample size,

\begin{itemize}
\tightlist
\item
  If we decrease \(\alpha\), then \(\beta\) will increase.
\item
  If we increase \(\alpha\), then \(\beta\) will decrease.
\end{itemize}

In practice, we set \(\alpha\) (as we did in confidence intervals). We can improve \(\beta\) by increasing sample size. Since resources are finite (we can't get enormous sample sizes all the time), we will need to consider the consequences of each type of error.

\begin{quote}
\emph{Example} We could think about assessing consequences through the jury trial example. Consider two possible charges:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Defendant is accused of stealing a loaf of bread. If found guilty, they may face some jail time and will have a criminal record.
\item
  Defendant is accused of murder. If found guilty, they will have a felony and may spend decades in prison.
\end{enumerate}

Since these are moral questions, I will let you consider the consequences of each type of error. However, keep in mind that we do make scientific decisions that have lasting impacts on people's lives.
\end{quote}

\textbf{Hypothesis Test Conclusions}

\begin{itemize}
\tightlist
\item
  If the null hypothesis is rejected, we say the result is \textbf{statistically significant}. We can interpret this result with:

  \begin{itemize}
  \tightlist
  \item
    At the \(\alpha\) level of significance, the data provide sufficient evidence to support the alternative hypothesis.
  \end{itemize}
\item
  If the null hypothesis is \emph{not} rejected, we say the result is \textbf{not statistically significant}. We can interpret this result with:

  \begin{itemize}
  \tightlist
  \item
    At the \(\alpha\) level of significance, the data do \emph{not} provide sufficient evidence to support the alternative hypothesis.
  \end{itemize}
\end{itemize}

Notice that these conclusions are framed in terms of the alternative hypothesis, which is either supported or not supported. We will \emph{never} conclude the null hypothesis. Finally, when we write these types of conclusions, we will write them in the context of the problem.

\hypertarget{confidence-interval-approach-to-hypothesis-testing}{%
\section{Confidence Interval Approach to Hypothesis Testing}\label{confidence-interval-approach-to-hypothesis-testing}}

We can use a confidence interval to help us weigh the evidence against the null hypothesis. A confidence interval gives us a range of \emph{plausible} values for \(\mu\). If the null value is in the interval, then \(\mu_0\) is a plausible value for \(\mu\). If the null value is \emph{not} in the interval, then \(\mu_0\) is \emph{not} a plausible value for \(\mu\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State null and alternative hypotheses.
\item
  Decide on significance level \(\alpha\). Check assumptions (decide which confidence interval setting to use).
\item
  Find the critical value.
\item
  Compute confidence interval.
\item
  If the null value is \emph{not} in the confidence interval, reject the null hypothesis. Otherwise, do not reject.
\item
  Interpret results in the context of the problem.
\end{enumerate}

\begin{quote}
\emph{Example}: Is the average mercury level in dolphin muslces different from \(2.5\mu g/g\)? Test at the 0.05 level of significance. A random sample of \(19\) dolphins resulted in a mean of \(4.4 \mu g/g\) and a standard deviation of \(2.3 \mu g/g\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(H_0: \mu = 2.5\) and \(H_A: \mu \ne 2.5\).
\item
  Significance level is \(\alpha=0.05\). The value of \(\sigma\) is unknown and \(n = 19 < 30\), so we are in setting 3.
\item
  For setting 3, the critical value is \(t_{df, \alpha/2}\). Here, \(df=n-1=18\) and \(\alpha/2 = 0.025\):
\end{enumerate}
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qt}\NormalTok{(}\FloatTok{0.025}\NormalTok{, }\DecValTok{18}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -2.100922
\end{verbatim}

\begin{quote}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  The confidence interval is \begin{align} \bar{x} &\pm t_{df, \alpha/2}\frac{s}{\sqrt{n}} \\ 4.4 &\pm 2.101 \frac{2.3}{\sqrt{19}} \\ 4.4 &\pm 1.109 \end{align} or \((3.29, 5.51)\).
\item
  Since the null value, \(2.5\), is not in the interval, it is \emph{not} a plausible value for \(\mu\) (at the 95\% level of confidence). Therefore we reject the null hypothesis.
\item
  At the 0.05 level of significance, the data provide sufficient evidence to conclude that the true mean mercury level in dolphin muscles is \emph{greater than} \(2.5\mu g/g\).
\end{enumerate}

Note: The alternative hypothesis is ``not equal to'', but we conclude ``greater than'' because all of the plausible values in the confidence interval are greater than the null value.
\end{quote}

\hypertarget{critical-value-approach-to-hypothesis-testing}{%
\section{Critical Value Approach to Hypothesis Testing}\label{critical-value-approach-to-hypothesis-testing}}

We learned about critical values when we discussed confidence intervals. Now, we want to use these values directly in a hypothesis test. We will compare these values to a value based on the data, called a \textbf{test statistic}.

Idea: the null is our ``default assumption''. If the null is true, how likely are we to observe a sample that looks like the one we have? If our sample is very inconsistent with the null hypothesis, we want to reject the null hypothesis.

\hypertarget{test-statistics}{%
\subsection{Test statistics}\label{test-statistics}}

Test statistics are similar to z- and t-scores: \[\text{test statistic} = \frac{\text{point estimate}-\text{null value}}{\text{standard error}}.\] In fact, they serve a similar function in converting a variable \(\bar{X}\) into a distribution we can work with easily.

\begin{itemize}
\tightlist
\item
  \textbf{Large Sample Setting}: \(\mu\) is target parameter, \(n \ge 30\)
\end{itemize}

\[z = \frac{\bar{x}-\mu_0}{s/\sqrt{n}}\]

\begin{itemize}
\tightlist
\item
  \textbf{Small Sample Setting}: \(\mu\) is target parameter, \(n < 30\)
\end{itemize}

\[t = \frac{\bar{x}-\mu_0}{s/\sqrt{n}}\]

The set of values for the test statistic that cause us to reject \(H_0\) is the \textbf{rejection region}. The remaining values are the \textbf{nonrejection region}. The value that separates these is the critical value!

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-85-1.pdf}

Steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State the null and alternative hypotheses.
\item
  Determine the significance level \(\alpha\). Check assumptions (decide which setting to use).
\item
  Compute the value of the test statistic.
\item
  Determine the critical values.
\item
  If the test statistic is in the rejection region, reject the null hypothesis. Otherwise, do not reject.
\item
  Interpret results.
\end{enumerate}

\begin{quote}
\emph{Example}: Is the average mercury level in dolphin muslces different from \(2.5\mu g/g\)? Test at the 0.05 level of significance. A random sample of \(19\) dolphins resulted in a mean of \(4.4 \mu g/g\) and a standard deviation of \(2.3 \mu g/g\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(H_0: \mu = 2.5\) and \(H_A: \mu \ne 2.5\).
\item
  Significance level is \(\alpha=0.05\). The value of \(\sigma\) is unknown and \(n = 19 < 30\), so we are in setting 3.
\item
  The test statistic is \begin{align} t &= \frac{\bar{x}-\mu_0}{s/\sqrt{n}} \\ &= \frac{4.4-2.5}{2.3/\sqrt{19}} \\ &= 3.601 \end{align}
\item
  The critical value is \(t_{df, \alpha/2}\). Here, \(df=n-1=18\) and \(\alpha/2 = 0.025\):
\end{enumerate}
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qt}\NormalTok{(}\FloatTok{0.025}\NormalTok{, }\DecValTok{18}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -2.100922
\end{verbatim}

\begin{quote}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  The test statistic is in the rejection region, so we will reject the null hypothesis:
\end{enumerate}
\end{quote}

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-87-1.pdf}

\begin{quote}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  At the 0.05 level of significance, the data provide sufficient evidence to conclude that the true mean mercury level in dolphin muscles is greater than \(2.5\mu g/g\).
\end{enumerate}
\end{quote}

Notice that this is the same conclusion we came to when we used the confidence interval approach. These approaches are exactly equivalent!

\hypertarget{p-value-approach-to-hypothesis-testing}{%
\section{P-Value Approach to Hypothesis Testing}\label{p-value-approach-to-hypothesis-testing}}

If the null hypothesis is true, what is the probability of getting a random sample that is as inconsistent with the null hypothesis as the random sample we got? This probability is called the \textbf{p-value}.

\begin{quote}
\emph{Example}: Is the average mercury level in dolphin muscles different from \(2.5\mu g/g\)? Test at the 0.05 level of significance. A random sample of \(19\) dolphins resulted in a mean of \(4.4 \mu g/g\) and a standard deviation of \(2.3 \mu g/g\).

Probability of a sample \emph{as inconsistent} as our sample is \(P(t_{df} \text{ is as extreme as the test statistic})\). Consider \[P(t_{18} > 3.6) = 0.001\] but we want to think about the probability of being ``as extreme'' in \emph{either direction} (either tail), so \[\text{p-value} = 2P(t_{18}>3.6) = 0.002\]
\end{quote}

If \(\text{p-value} < \alpha\), reject the null hypothesis. Otherwise, do not reject.

\hypertarget{p-values}{%
\subsection{P-Values}\label{p-values}}

\begin{itemize}
\item
  \textbf{Large Sample Setting}: \(\mu\) is target parameter, \(n \ge 30\), \[2P(Z > |z|)\] where \(z\) is the test statistic.
\item
  \textbf{Small Sample Setting}: \(\mu\) is target parameter, \(n < 30\), \[2P(t_{df} > |t|)\] where \(t\) is the test statistic.
\end{itemize}

Note: \(|a|\) is the ``absolute value'' of \(a\). The absolute value takes a number and throws away the sign, so \(|2|=2\) and \(|-3|=3\).

Steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State the null and alternative hypotheses.
\item
  Determine the significance level \(\alpha\). Check assumptions (decide which setting to use).
\item
  Compute the value of the test statistic.
\item
  Determine the p-value.
\item
  If \(\text{p-value} < \alpha\), reject the null hypothesis. Otherwise, do not reject.
\item
  Interpret results.
\end{enumerate}

We often use p-values instead of the critical value approach because they are meaningful on their own (they have a direct interpretation).

\begin{quote}
\emph{Example}: For the dolphins,

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(H_0: \mu = 2.5\) and \(H_A: \mu \ne 2.5\).
\item
  Significance level is \(\alpha=0.05\). The value of \(\sigma\) is unknown and \(n = 19 < 30\), so we are in setting 3.
\item
  The test statistic is \begin{align} t &= \frac{\bar{x}-\mu_0}{s/\sqrt{n}} \\ &= \frac{4.4-2.5}{2.3/\sqrt{19}} \\ &= 3.601 \end{align}
\item
  The p-value is \[2P(t_{df} > |t|) - 2P(t_{18} > 3.601) = 0.002\]
\item
  Since \(\text{p-value}=0.002 < \alpha=0.05\), reject the null hypothesis.
\item
  At the 0.05 level of significance, the data provide sufficient evidence to conclude that the true mean mercury level in dolphin muscles is greater than \(2.5\mu g/g\).
\end{enumerate}
\end{quote}

As before, this is the same conclusion we came to when we used the confidence interval and critical value approaches. All of these approaches are exactly equivalent.

\hypertarget{r-hypothesis-tests-for-a-mean}{%
\section*{R: Hypothesis Tests for a Mean}\label{r-hypothesis-tests-for-a-mean}}
\addcontentsline{toc}{section}{R: Hypothesis Tests for a Mean}

To conduct hypothesis tests for a mean in R, we will again use the \texttt{t.test} command. The arguments we will use for hypothesis testing are

\begin{itemize}
\tightlist
\item
  \texttt{x}: the variable that contains the data we want to use to construct a confidence interval.
\item
  \texttt{mu}: the null value, \(\mu_0\).
\item
  \texttt{conf.level}: the desired confidence level (\(1-\alpha\)).
\end{itemize}

We will again to use the \texttt{Loblolly} pine tree data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{attach}\NormalTok{(Loblolly)}
\end{Highlighting}
\end{Shaded}

Let's test if the average height of Loblolly pines differs from \(40\) feet. We will test at a 0.01 level of significance (\(\alpha = 0.01\)). So \(H_0: \mu = 40\) and \(H_A: \mu \ne 40\) and the R command will look like

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(}\AttributeTok{x =}\NormalTok{ height, }\AttributeTok{mu =} \DecValTok{40}\NormalTok{, }\AttributeTok{conf.level =} \FloatTok{0.99}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  One Sample t-test
## 
## data:  height
## t = -3.3851, df = 83, p-value = 0.001089
## alternative hypothesis: true mean is not equal to 40
## 99 percent confidence interval:
##  26.41761 38.31120
## sample estimates:
## mean of x 
##   32.3644
\end{verbatim}

Last time we used this command, we noted that R printed more information than we knew how to handle. That information was about hypothesis tests! The output from this test shows the following (top to bottom):

\begin{itemize}
\tightlist
\item
  the data used in the hypothesis test.
\item
  the value of the test statistic (\(t = -3.3851\)), the degrees of freedom (\(83\)), and the p-value (\(0.001\)).
\item
  the alternative hypothesis.
\item
  the confidence interval.
\item
  the sample mean.
\end{itemize}

Based on this output, we have everything we need to conduct a hypothesis test using (A) the confidence interval approach, (B) the critical value approach, or (C) the p-value approach! In practice, we might include results from multiple approaches: At the 0.01 level of significance, there is sufficient evidence to reject the null hypothesis and conclude that the true mean height of Loblolly pines is less than 40 feet (\(t = -3.385\), p-value\(=0.001\)).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{detach}\NormalTok{(Loblolly)}
\end{Highlighting}
\end{Shaded}

\hypertarget{inference-for-a-proportion}{%
\chapter{Inference for a Proportion}\label{inference-for-a-proportion}}

\hypertarget{chapter-overview-7}{%
\section{Chapter Overview}\label{chapter-overview-7}}

In this chapter, we will continue our discussion on statistical inference with a discussion on hypothesis testing. In hypothesis testing, we take a more active approach to our data by asking questions about population parameters and developing a framework to answer those questions. We will root this discussion in confidence intervals before learning about several other approaches to hypothesis testing.

\textbf{Chapter Learning Outcomes/Objectives}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Perform and interpret inference for a population proportion.
\end{enumerate}

\textbf{R Objectives}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Generate hypothesis tests for a proportion.
\item
  Interpret R output for tests of a proportion.
\end{enumerate}

This chapter's outcomes correspond to course outcomes (6) apply statistical inference techniques of parameter estimation such as point estimation and confidence interval estimation and (7) apply techniques of testing various statistical hypotheses concerning population parameters.

\hypertarget{confidence-intervals-for-a-proportion}{%
\section{Confidence Intervals for a Proportion}\label{confidence-intervals-for-a-proportion}}

Inference for a proportion is really similar to inference for a mean! It turns out we can apply the Central Limit Theorem to the sampling distribution for a proportion. But wait - isn't our Central Limit Theorem only for means?

Think back to the binomial distribution (Section 4.3). A binomial experiment is made up of a series of Bernoulli trials, which result in 0s and 1s. If we add up these values, we get the number of successes \(x\). If we take the mean of these successes, we get the \emph{proportion} of successes. In short, \(\bar{x} = \hat{p}\) and we can work with the sampling distribution for a sample mean!

The mean of a Bernoulli random variable is \(\mu = p\) and the standard deviation is \(\sigma = \sqrt{p(1-p)}\). So if we apply the Central Limit Theorem, \(\hat{p}\) is approximately normally distributed with mean \[\mu_{\hat{p}} = p\] and standard error \[\sigma_{\hat{p}} = \frac{\sqrt{p(1-p)}}{\sqrt{n}} = \sqrt{\frac{p(1-p)}{n}}\]

Each of the confidence intervals for a mean uses the same logic: \[\text{estimate }\pm\text{ critical value }\times\text{ standard error }\] Confidence intervals for a proportion will do the same. We do not know the true value of \(p\) for the standard error, so we will plug in \(\hat{p}\).

A \(100(1-\alpha)\%\) confidence interval for \(p\).

\[\hat{p}\pm z_{\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}\]

To use this formula, we need to check that \(n\hat{p} > 10\) and \(n(1-\hat{p})>10\). (Note that \(n\hat{p}\) is the number of successes and \(n(1-\hat{p})\) is the number of failures, so this is another way to check this condition!)

Why? This relies on a normal approximation that does not work well if either of those quantities is less than or equal to 10. (This a topic which we have skipped, but the theory behind it is similar to the theory presented here for why we can use the Central Limit Theorem with proportions.)

\begin{quote}
\textbf{Example:} Suppose we take a random sample of 27 US households and find that 15 of them have dogs. Find a 95\% confidence interval for the proportion of US households with dogs.

\textbf{Solution:} From the problem statement, \(\alpha = 0.05\). Also, \(\hat{p} = 15/27 = 0.56\). The number of successes (households with dogs) in the sample is 15 and the number of failures is 12, both greater than 10, so our assumptions are satisfied.

The critical value is \(z_{\alpha/2}\). Using the normal distribution applet with \(\alpha = 0.05\), this yields a value of 1.96. Plugging everything in, \[\hat{p}\pm z_{\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}} = 0.56 \pm 1.96\sqrt{\frac{0.56\times0.44}{27}} = 0.37 \pm 0.19\] or a 95\% confidence interval of (0.37, 0.75).

Based on our sample, we can be 95\% confident that the proportion of US households with dogs is between 0.37 and 0.75.
\end{quote}

\hypertarget{hypothesis-tests-for-a-proportion}{%
\section{Hypothesis Tests for a Proportion}\label{hypothesis-tests-for-a-proportion}}

For a single proportion, the null and alternative hypotheses are

\begin{itemize}
\tightlist
\item
  \(H_0: p = p_0\)
\item
  \(H_A: p \ne p_0\)
\end{itemize}

We can perform a hypothesis test for \(p\) using the confidence interval, critical value, or p-value approach described in Chapter 6.

\textbf{Setting and Assumptions}: \(p\) is target parameter, \(np_0 > 10\), \(n(1-p_0)>10\).

\hypertarget{confidence-interval-approach}{%
\subsection{Confidence Interval Approach}\label{confidence-interval-approach}}

The \(100(1-\alpha)\%\) confidence interval for \(p\) is \[\hat{p}\pm z_{\alpha/2}\sqrt{\frac{p_0(1-p_0)}{n}}\] Notice that we use \(p_0\) in the standard error and \emph{not} the sample proportion. This is different from how we dealt with the standard error when calculating confidence intervals outside of a hypothesis testing context. We do this because the standard error is calculated based on the distribution based on the null hypothesis, which says that \(p=p_0\).

Steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State null and alternative hypotheses.
\item
  Decide on significance level \(\alpha\). Check assumptions.
\item
  Find the critical value.
\item
  Compute confidence interval.
\item
  If the null value is \emph{not} in the confidence interval, reject the null hypothesis. Otherwise, do not reject.
\item
  Interpret results in the context of the problem.
\end{enumerate}

\begin{quote}
\textbf{Example:} A quick internet search suggests that 38.4\% of US households have dogs. Based on the sample described previously, is it reasonable to assume that the internet search is correct? Test at the 0.05 level of significance using a confidence interval approach.

\textbf{Solution:} We know from the previous example that \(\hat{p} = 0.56\) and \(n=27\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  We want to see if the internet search is correct, so the null and alternative hypotheses are \[H_0: p = 0.384\] \[H_A: p \ne 0.384\]
\item
  From the problem statement, \(\alpha = 0.05\). Also, \(np_0 = 27(0.384)=10.4\) and \(n(1-p_0)=27(0.616)=16.6\), both greater than 10, so our assumptions are satisfied.
\item
  The critical value is \(z_{\alpha/2}\). Using the normal distribution applet with \(\alpha = 0.05\), this yields a value of 1.96.
\item
  Plugging everything in, \[\hat{p}\pm z_{\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}} = 0.56 \pm 1.96\sqrt{\frac{0.37\times0.63}{27}} = 0.56 \pm 0.18\] or a 95\% confidence interval of (0.38, 0.74).
\item
  The null value is in the interval, so we fail to reject \(H_0\).
\item
  At the 0.05 level of significance, the data provide \emph{insufficient evidence} to conclude that the proportion of US Households with dogs differs from 0.384.
\end{enumerate}
\end{quote}

\hypertarget{critical-value-approach}{%
\subsection{Critical Value Approach}\label{critical-value-approach}}

The critical value is \(z_{\alpha/2}\) and the test statistic is \[z = \frac{\hat{p} - p_0}{\sqrt{\frac{p_0(1-p_0)}{n}}}\] Notice that we again plug in \(p_0\) for the standard error!

Steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State the null and alternative hypotheses.
\item
  Determine the significance level \(\alpha\). Check assumptions.
\item
  Compute the value of the test statistic.
\item
  Determine the critical values.
\item
  If the test statistic is in the rejection region, reject the null hypothesis. Otherwise, do not reject.
\item
  Interpret results.
\end{enumerate}

\begin{quote}
\textbf{Example:} A quick internet search suggests that 38.4\% of US households have dogs. Based on the sample described previously, is it reasonable to assume that the internet search is correct? Test at the 0.05 level of significance using a critical value approach.

\textbf{Solution:} We know from a previous example that \(\hat{p} = 0.56\) and \(n=27\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  We want to see if the internet search is correct, so the null and alternative hypotheses are \[H_0: p = 0.384\] \[H_A: p \ne 0.384\]
\item
  From the problem statement, \(\alpha = 0.05\). Also, \(np_0 = 27(0.384)=10.4\) and \(n(1-p_0)=27(0.616)=16.6\), both greater than 10, so our assumptions are satisfied.
\item
  The test statisic is \[z = \frac{\hat{p} - p_0}{\sqrt{\frac{p_0(1-p_0)}{n}}} = \frac{0.56 - 0.384}{\sqrt{\frac{0.384(0.616)}{27}}} = 1.41\]
\item
  The critical value is \(z_{\alpha/2}\). Using the normal distribution applet with \(\alpha = 0.05\), this yields a value of 1.96.
\item
  The test statistics is \emph{not} in the rejection region, so we fail to reject \(H_0\).
\item
  At the 0.05 level of significance, the data provide \emph{insufficient evidence} to conclude that the proportion of US Households with dogs differs from 0.384.
\end{enumerate}
\end{quote}

\hypertarget{p-value-approach}{%
\subsection{P-Value Approach}\label{p-value-approach}}

The p-value is \[2P(Z > |z|)\] where \(z\) is the test statistic described above.

Steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State the null and alternative hypotheses.
\item
  Determine the significance level \(\alpha\). Check assumptions.
\item
  Compute the value of the test statistic.
\item
  Determine the p-value.
\item
  If \(\text{p-value} < \alpha\), reject the null hypothesis. Otherwise, do not reject.
\item
  Interpret results.
\end{enumerate}

\begin{quote}
\textbf{Example:} A quick internet search suggests that 38.4\% of US households have dogs. Based on the sample described previously, is it reasonable to assume that the internet search is correct? Test at the 0.05 level of significance using a p-value approach.

\textbf{Solution:} We know from a previous example that \(\hat{p} = 0.56\) and \(n=27\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  We want to see if the internet search is correct, so the null and alternative hypotheses are \[H_0: p = 0.384\] \[H_A: p \ne 0.384\]
\item
  From the problem statement, \(\alpha = 0.05\). Also, \(np_0 = 27(0.384)=10.4\) and \(n(1-p_0)=27(0.616)=16.6\), both greater than 10, so our assumptions are satisfied.
\item
  The test statisic is \[z = \frac{\hat{p} - p_0}{\sqrt{\frac{p_0(1-p_0)}{n}}} = \frac{0.56 - 0.384}{\sqrt{\frac{0.384(0.616)}{27}}} = 1.41\]
\item
  The p-value is \[2P(Z > |z|) = 2P(Z > 1.41)\] Using the normal distribution applet, we find this probability to be \(2(0.079) = 0.159\).
\item
  The p-value \(= 0.159 > \alpha = 0.05\), so we fail to reject \(H_0\).
\item
  At the 0.05 level of significance, the data provide \emph{insufficient evidence} to conclude that the proportion of US Households with dogs differs from 0.384.
\end{enumerate}
\end{quote}

\hypertarget{r-hypothesis-tests-for-a-proportion}{%
\section*{R: Hypothesis Tests for a Proportion}\label{r-hypothesis-tests-for-a-proportion}}
\addcontentsline{toc}{section}{R: Hypothesis Tests for a Proportion}

To generate confidence intervals and hypothesis tests for a proportion, we will use the command \texttt{binom.test}. This will give us slightly different results than the z-test we used throughout this chapter, but it is actually going to be more exact! This approach also does not have any limitations on the values \(n\hat{p}\) or \(np_0\). We use the z-test when working by hand because the exact binomial test is difficult to do on paper. The arguments we need are:

\begin{itemize}
\tightlist
\item
  \texttt{x}: the number of successes.
\item
  \texttt{n}: the number of trials.
\item
  \texttt{p}: the null value \(p_0\).
\item
  \texttt{conf.level}: the desired confidence level (\(1-\alpha\)).
\end{itemize}

Let's continue to use the example seen throughout this chapter. We have a random sample of 27 US households and 15 of them have dogs. We also have the claim that, in fact, 38.4\% of US households have dogs. We will use a significance level of \(\alpha=0.05\).

Based on the prompt, there are \texttt{x\ =\ 15} successes; \texttt{n=27} trials; and \(p_0=\)\texttt{p=0.384}. So the R command will look like

\begin{verbatim}
## 
##  Exact binomial test
## 
## data:  15 and 27
## number of successes = 15, number of trials = 27, p-value = 0.07591
## alternative hypothesis: true probability of success is not equal to 0.384
## 95 percent confidence interval:
##  0.3532642 0.7452012
## sample estimates:
## probability of success 
##              0.5555556
\end{verbatim}

The output shows (top to bottom):

\begin{itemize}
\tightlist
\item
  a summary of the data we entered, along with the null value \(p_0\).
\item
  the test statistic and associated degrees of freedom (we will ignore this part) and the p-value.
\item
  the alternative hypothesis.
\item
  a 95\% confidence interval for \(p\).
\item
  the sample proportion \(\hat{p}\).
\end{itemize}

Since this is slightly different from the test used when we discussed doing these calculations by hand, when we do hypothesis tests for a proportion using R, we will \emph{not} use the critical value approach. Based on the confidence interval and p-value, at the 0.05 level of significance, the data provide insufficient evidence to conclude that the proportion of US Households with dogs differs from 0.384. (In general, we will come to the same conclusion whether we do these tests by hand or using R.)

\hypertarget{inference-comparing-parameters}{%
\chapter{Inference: Comparing Parameters}\label{inference-comparing-parameters}}

\hypertarget{chapter-overview-8}{%
\section{Chapter Overview}\label{chapter-overview-8}}

In this chapter, we extend the concepts from Chapter 6 to answer questions like ``is there a difference between these means?'' We will also consider hypothesis tests for whether a sample represents the population or closely matches a particular distribution.

\textbf{Chapter Learning Outcomes/Objectives}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Perform and interpret inference for

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    the difference of two proportions.
  \item
    paired data and two sample means.
  \end{enumerate}
\end{enumerate}

\textbf{R Objectives}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Generate hypothesis tests for the difference of two proportions.
\item
  Generate hypothesis tests for the difference of two means.
\item
  Interpret R output for tests of two proportions and two means.
\end{enumerate}

This chapter's outcomes correspond to course outcomes (6) apply statistical inference techniques of parameter estimation such as point estimation and confidence interval estimation and (7) apply techniques of testing various statistical hypotheses concerning population parameters.

\hypertarget{hypothesis-tests-for-two-proportions}{%
\section{Hypothesis Tests for Two Proportions}\label{hypothesis-tests-for-two-proportions}}

Sometimes, we might like to \emph{compare} two proportions. We do this by looking at their \emph{difference}: \(p_1 - p_2\). This is going to be fairly similar to the tests we used for a single proportion. Let \(n_1\) be the sample size for the first group and \(p_1\) the proportion for the first group. Similarly, let \(n_2\) be the sample size for the second group and \(p_2\) the proportion for the second group.

Conditions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Independence within and between groups (generally satisfied if the data are from random samples or a randomized experiment).
\item
  We need \(n_1p_1 > 10\) and \(n_1(1-p_1)>10\) \textbf{and} \(n_2p_2 > 10\) and \(n_2(1-p_2)>10\)
\end{enumerate}

If these conditions are satisfied, the standard error is \[\sqrt{\frac{p_1(1-p_1)}{n_1} + \frac{p_2(1-p_2)}{n_2}}\] and we can calculate confidence intervals and perform hypothesis tests on \(p_1 - p_2\).

\hypertarget{confidence-intervals-for-two-proportions}{%
\subsection{Confidence Intervals for Two Proportions}\label{confidence-intervals-for-two-proportions}}

A \(100(1-\alpha)\%\) confidence interval for \(p_1-p_2\) is

\[\hat{p_1} - \hat{p_2} \pm z_{\alpha/2} \times \sqrt{\frac{\hat{p_1}(1-\hat{p_1})}{n_1} + \frac{\hat{p_2}(1-\hat{p_2})}{n_2}}\]

\hypertarget{critical-values-test-statistics-and-p-values}{%
\subsection{Critical Values, Test Statistics, and P-Values}\label{critical-values-test-statistics-and-p-values}}

Often, we are interested in checking whether \(p_1 = p_2\), which results in a null hypothesis of \(H_0: p_1 - p_2 = 0\) (where the null value is zero). In this case, we use a \emph{pooled proportion} to estimate \(p\) in the standard error.

This pooled proportion is calculated as \[\hat{p}_{\text{pooled}} = \frac{\text{total number of successes}}{\text{total number of cases}} = \frac{\hat{p_1}n_1 + \hat{p_2}n_2}{n_1 + n_2}\]
which makes the standard error in this case
\[ \text{Standard Error} = \sqrt{\frac{\hat{p}_{\text{pooled}}(1-\hat{p}_{\text{pooled}})}{n_1} + \frac{\hat{p}_{\text{pooled}}(1-\hat{p}_{\text{pooled}})}{n_2}}\]

The critical value is \(z_{\alpha/2}\). The test statistic is \[z = \frac{\hat{p_1}-\hat{p_2}}{\sqrt{\frac{\hat{p}_{\text{pooled}}(1-\hat{p}_{\text{pooled}})}{n_1} + \frac{\hat{p}_{\text{pooled}}(1-\hat{p}_{\text{pooled}})}{n_2}}}\]and the p-value is \[2P(Z > |z|)\] where \(z\) is the test statistic.

\textbf{Steps:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State the null and alternative hypotheses.
\item
  Determine the significance level \(\alpha\). Check assumptions, \(n_1p_1 > 10\) and \(n_1(1-p_1)>10\) \textbf{and} \(n_2p_2 > 10\) and \(n_2(1-p_2)>10\).
\item
  Compute the value of the test statistic.
\item
  Determine the critical value or p-value.
\item
  For the \emph{critical value approach}: If the test statistic is in the rejection region, reject the null hypothesis. For the \emph{p-value approach}: If \(\text{p-value} < \alpha\), reject the null hypothesis. Otherwise, do not reject.
\item
  Interpret results.
\end{enumerate}

\hypertarget{r-hypothesis-tests-for-two-proportions}{%
\subsection*{R: Hypothesis Tests for Two Proportions}\label{r-hypothesis-tests-for-two-proportions}}
\addcontentsline{toc}{subsection}{R: Hypothesis Tests for Two Proportions}

To compare two proportions, we will use the command \texttt{prop.test}. This is similar to \texttt{binom.test}, but the latter command does not allow us to compare two proportions. We will need the following arguments:

\begin{itemize}
\tightlist
\item
  \texttt{x}: a listing of the numbers of successes in each of the two groups. This will take the form \texttt{x\ =\ c(x1,\ x2)}.
\item
  \texttt{n}: a listing of the numbers of trials for each group. This will take the form \texttt{n\ =\ c(n1,\ n2)}.
\item
  \texttt{conf.level}: the confidence level (\(1-\alpha\)).
\end{itemize}

Note that order matters in \texttt{c(x1,\ x2)} and \texttt{c(n1,\ n2)}. Make sure to keep track of which variable you have set as 1 an which as 2. This test also assumes a null hypothesis of \(p_1 = p_2\).

This test has a few behind-the-scenes tweaks relative to what we do by hand. This means that the results might be slightly different than the results you get when running these tests by hand. That's ok!

The \texttt{sleep} dataset in R contains data on two groups (10 in each) of patients given soporific drugs (drugs designed to induce sleep). We want to examine whether the \emph{proportion} of patients who experienced an increase in hours of sleep differs between the two groups.

I have this set up with two variables, \texttt{d1} and \texttt{d2}, which represent drug 1 and drug 2. Each variable is \(1\) if the patient experienced an increase in hours of sleep and \(0\) if they did not. Let's print these out and find out how many successes were in each group.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 1 0 0 0 0 1 1 1 0 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 1 1 1 1 0 1 1 1 1 1
\end{verbatim}

We can find the total number of successes for each by summing the values in each variable. Let's do that in R using the \texttt{sum} command:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sum}\NormalTok{(d1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sum}\NormalTok{(d2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 9
\end{verbatim}

So the numbers of successes are \(x_1 = 5\) and \(x_2 = 9\) for group sizes \(n_1 = n_2 = 10\). For the \texttt{prop.test} command, this will look like \texttt{x\ =\ c(5,\ 9)} and \texttt{n\ =\ c(10,10)}. We will use an \(\alpha=0.1\) level of significance. Then

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{prop.test}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{9}\NormalTok{), }\AttributeTok{n =} \FunctionTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{,}\DecValTok{10}\NormalTok{), }\AttributeTok{conf.level =} \FloatTok{0.9}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  2-sample test for equality of proportions with continuity correction
## 
## data:  c(5, 9) out of c(10, 10)
## X-squared = 2.1429, df = 1, p-value = 0.1432
## alternative hypothesis: two.sided
## 90 percent confidence interval:
##  -0.803296023  0.003296023
## sample estimates:
## prop 1 prop 2 
##    0.5    0.9
\end{verbatim}

The output of this test is (top to bottom)

\begin{itemize}
\tightlist
\item
  The data provided in the input.
\item
  A test statistic and degrees of freedom (these are part of the behind-the-scenes tweaks and you can ignore them!) along with a p-value.
\item
  When a hypothesis test says ``two sided'' that means the null hypothesis represents the ``not equal'' condition that we work with.
\item
  The requested confidence interval.
\item
  The sample proportions.
\end{itemize}

Although the sample proportions appear to be different, the sample sizes are very small! Therefore it is unsurprising that the data provide insufficient evidence to conclude that the drugs differ in their ability to increase hours slept (\(p=0.143\) and the confidence interval includes \(0\)).

\hypertarget{hypothesis-tests-for-two-means}{%
\section{Hypothesis Tests for Two Means}\label{hypothesis-tests-for-two-means}}

What if we wanted to compare two means? We begin by discussing paired samples. This will feel very familiar, since it's essentially the same as hypothesis testing for a single mean. Then we will move on to independent samples, which will require a couple of adjustments.

\hypertarget{paired-samples}{%
\subsection{Paired Samples}\label{paired-samples}}

Sometimes there is a special correspondence between two sets of observations. We say that two sets of observations are \textbf{paired} if each observation has a natural connection with exactly one observation in the other data set. Consider the following data from 30 students given a pre- and post-test on a course concept:

\begin{longtable}[]{@{}ccc@{}}
\toprule()
Student & Pre-Test & Post-Test \\
\midrule()
\endhead
1 & 52 & 70 \\
2 & 71 & 98 \\
3 & 13 & 65 \\
\(\dots\) & \(\dots\) & \(\dots\) \\
30 & 48 & 81 \\
\bottomrule()
\end{longtable}

The natural connection between ``pre-test'' and ``post-test'' is the student who took each test! Often, paired data will involve similar measures taken on the \emph{same item or individual}. We \emph{pair} these data because we want to compare two means, but we also want to account for the pairing.

Why? Consider: If a student got a 13\% on the pre-test, I would love to see them get a 60\% on the post-test - that's a huge improvement! But if a student got an 82\% on the pre-test, I would \emph{not} like to see them get a 60\% on the post-test. Pairing the data lets us account for this connection.

So what do we do with paired data? Fortunately, this part is easy! We start by taking the difference between the two sets of observations. In the pre- and post-test example, I will take the pre-test score and subtract the post-test score:

\begin{longtable}[]{@{}cccc@{}}
\toprule()
Student & Pre-Test & Post-Test & \textbf{Difference} \\
\midrule()
\endhead
1 & 52 & 70 & \textbf{18} \\
2 & 71 & 98 & \textbf{27} \\
3 & 13 & 65 & \textbf{52} \\
\(\dots\) & \(\dots\) & \(\dots\) & \(\dots\) \\
30 & 48 & 81 & \textbf{33} \\
\bottomrule()
\end{longtable}

Then, we do a test of a \emph{single mean} on the differences where

\begin{itemize}
\tightlist
\item
  \(H_0: \mu_{\text{d}} = 0\)
\item
  \(H_A: \mu_{\text{d}} \ne 0\)
\end{itemize}

Note that the subscript ``d'' denotes ``difference''. We will use the exact same test(s) as in the previous sections:

\begin{itemize}
\item
  \textbf{Large Sample Setting}: \(\mu_{\text{d}}\) is target parameter, \(n_{\text{d}} \ge 30\), \[z = \frac{\bar{x}_{\text{d}}}{s_{\text{d}}/\sqrt{n_{\text{d}}}}\] and the p-value is \[2P(Z > |z|)\] where \(z\) is the test statistic.
\item
  \textbf{Small Sample Setting}: \(\mu_{\text{d}}\) is target parameter, \(n_{\text{d}} < 30\), \[t = \frac{\bar{x}_{\text{d}}}{s_{\text{d}}/\sqrt{n_{\text{d}}}}\] and the p-value is \[2P(t_{df} > |t|)\] where \(t\) is the test statistic.
\end{itemize}

Here, \(n_{\text{d}}\) is the number of pairs.

Steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State the null and alternative hypotheses.
\item
  Determine the significance level \(\alpha\). Check assumptions (decide which setting to use).
\item
  Compute the value of the test statistic.
\item
  Determine the critical values or p-value.
\item
  For the \emph{critical value approach}: If the test statistic is in the rejection region, reject the null hypothesis. For the \emph{p-value approach}: If \(\text{p-value} < \alpha\), reject the null hypothesis. Otherwise, do not reject.
\item
  Interpret results.
\end{enumerate}

\hypertarget{independent-samples}{%
\subsection{Independent Samples}\label{independent-samples}}

In \textbf{independent samples}, the sample from one population does not impact the sample from the other population. In short, we take two \emph{separate samples} and compare them.

\begin{itemize}
\tightlist
\item
  \(H_0: \mu_1 = \mu_2 \quad \rightarrow \quad H_0: \mu_1 - \mu_2 = 0\)
\item
  \(H_A: \mu_1 \ne \mu_2 \quad \rightarrow \quad H_A: \mu_1 - \mu_2 \ne 0\)
\end{itemize}

If we use \(\bar{x}\) to estimate \(\mu\), intuitively we might use \(\bar{x}_1-\bar{x}_2\) to estimate \(\mu_1 - \mu_2\). To do this, we need to know something about the sampling distribution of \(\bar{x}_1-\bar{x}_2\).

Consider: if \(X_1\) is Normal(\(\mu_1\), \(\sigma_1\)) and \(X_2\) is Normal(\(\mu_2\),\(\sigma_2\)) with \(\sigma_1\) and \(\sigma_2\) are known, then for independent samples of size \(n_1\) and \(n_2\),

\begin{itemize}
\tightlist
\item
  \(\bar{X}_1-\bar{X}_2\) is Normal(\(\mu_{\bar{X}_1-\bar{X}_2}\), \(\sigma_{\bar{X}_1-\bar{X}_2}\)).
\item
  \(\mu_{\bar{X}_1-\bar{X}_2} = \mu_1 - \mu_2\)
\item
  \(\sigma_{\bar{X}_1-\bar{X}_2} = \sigma_1 - \sigma_2\)
\end{itemize}

so then \[Z = \frac{(\bar{X}_1 - \bar{X}_2) - (\mu_1 - \mu_2)}{\sqrt{\sigma_1/n_1 - \sigma_2/n_2}}\] has a standard normal distribution. But, as we mentioned earlier, we rarely work in that setting where the population standard deviation is known. Instead, we will use \(s_1\) and \(s_2\) to estimate \(\sigma_1\) and \(\sigma_2\). For independent samples of size \(n_1\) and \(n_2\), \[t = \frac{(\bar{X}_1 - \bar{X}_2) - (\mu_1 - \mu_2)}{\sqrt{s_1/n_1 - s_2/n_2}}\] has a t-distribution with degrees of freedom \[\Delta = \frac{[(s_1^2/n_1) + (s_2^2/n_2)]^2}{\frac{(s_1^2/n_1)^2}{n_1-1} + \frac{(s_2^2/n_2)^2}{n_2-1}}\] rounded \emph{down} to the nearest whole number. (Note that \(\Delta\) is the uppercase Greek letter, ``delta''.) If \(n_1 = n_2\), this simplifies to \[\Delta = (n-1)\left(\frac{(s_1^2 + s_2^2)^2}{s_1^4 + s_2^4}\right)\]

\textbf{Tip:} Generally, people do not calculate \(\Delta\) by hand. Instead, we use a computer to do these kinds of tests.

\textbf{The Two-Sample T-Test}

Assumptions:

\begin{itemize}
\tightlist
\item
  Simple random samples.
\item
  Independent samples.
\item
  Normal populations or large (\(n \ge 30\)) samples.
\end{itemize}

\textbf{Steps for Critical Value Approach}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(H_0: \mu_1 - \mu_2 = 0\) and \(H_A: \mu_1 - \mu_2 \ne 0\)
\item
  Check assumptions; select the significance level \(\alpha\).
\item
  Compute the test statistic \[t = \frac{\bar{x}_1 - \bar{x}_2}{\sqrt{s_1/n_1 - s_2/n_2}}\] Note that we assume under the null hypothesis that \(\mu_1 - \mu_2 = 0\), which is why we replace this quantity with \(0\) in the test statistic.
\item
  The critical value is \(\pm t_{df, \alpha/2}\) with \(df = \Delta\).
\item
  If the test statistic falls in the rejection region, reject the null hypothesis.
\item
  Interpret in the context of the problem.
\end{enumerate}

\textbf{Steps for P-Value Approach}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(H_0: \mu_1 - \mu_2 = 0\) and \(H_A: \mu_1 - \mu_2 \ne 0\)
\item
  Check assumptions; select the significance level \(\alpha\).
\item
  Compute the test statistic \[t = \frac{\bar{x}_1 - \bar{x}_2}{\sqrt{s_1/n_1 - s_2/n_2}}\] Note that we assume under the null hypothesis that \(\mu_1 - \mu_2 = 0\), which is why we replace this quantity with \(0\) in the test statistic.
\item
  The p-value is \(2P(t_{df} > |t|)\) with \(df = \Delta\).
\item
  If \(\text{p-value}<\alpha\), reject the null hypothesis.
\item
  Interpret in the context of the problem.
\end{enumerate}

Notice that the only difference between the critical value and p-value approaches are steps 4 and 5.

\begin{quote}
\emph{Example}: Researchers wanted to detemine whether a dymanic or static approach would impact the time needed to complete neurosurgeries. The experiment resulted in the following data from simple random samples of patients:

\begin{longtable}[]{@{}cc@{}}
\toprule()
Dynamic & Static \\
\midrule()
\endhead
\(\bar{x}_1 = 394.6\) & \(\bar{x}_2 = 468.3\) \\
\(s_1 = 84.7\) & \(s_2 = 38.2\) \\
\(n_1 = 14\) & \(n_2 = 6\) \\
\bottomrule()
\end{longtable}

Times are measured in minutes. Assume \(X_1\) and \(X_2\) are reasonably normal.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(H_0: \mu_1 = \mu_2\) and \(H_A: \mu_1\ne\mu_2\)
\item
  Let \(\alpha=0.05\) (this will be our default when a significance level is not given)

  \begin{itemize}
  \tightlist
  \item
    We are told these are simple random samples.
  \item
    There's no reason that time for a neurosurgery with the dynamic system would impact time for the static system (or vice versa), so it's reasonable to assume these samples are independent.
  \item
    We are told to assume that \(X_1\) and \(X_2\) are reasonably normal.
  \end{itemize}
\item
  The test statistic is \[t = \frac{394.6-468.3}{84.7^2/14 + 38.2^2/6} = -2.681\]
\item
  Then \[df = \Delta = \frac{(84.7^2/14) + (38.2^2/6)^2}{\frac{(84.7^2/14)^2}{14-1} + \frac{(38.2^2/6)^2}{6-1}} = 17\] when rounded down. The critical value is \[t_{17, 0.025} = 2.110\] and the p-value is \[2P(t_{17}>|-2.681|)=2(0.0079)=0.0158\]
\item
  For the critical value approach,
\end{enumerate}
\end{quote}

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-96-1.pdf}

\begin{quote}
Since the test statistic is in the rejection region, we reject the null hypothesis. For the p-value approach, since \(\text{p-value}=0.158 < \alpha =0.05\), reject the null hypothesis.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  At the 0.05 level of significance, the data provide sufficient evidence to conclude that the mean time for the dynamic system is less than the mean time for the static system.
\end{enumerate}
\end{quote}

We can also construct a \textbf{\((1-\alpha)100\%\) confidence interval} for the difference of the two population means: \[(\bar{x}_1-\bar{x}_2) \pm t_{df, \alpha/2}\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}\] which we interpret as we interpret other confidence intervals, including in our interpretation that we are now considering the \textbf{difference of two means}.

\hypertarget{r-hypothesis-tests-for-two-means}{%
\subsection*{R: Hypothesis Tests for Two Means}\label{r-hypothesis-tests-for-two-means}}
\addcontentsline{toc}{subsection}{R: Hypothesis Tests for Two Means}

The math has only gotten more cumbersome! Let's use R to quickly run these types of tests without having to do any calculations by hand.

There is data built into R that shows the effect of Vitamin C on tooth growth in guinea pigs through (A) ascorbic acid or (B) orange juice. (Each guinea pig was randomly assigned to either ascorbic acid \emph{or} orange juice.) We want to compare the ascorbic acid group to the orange juice group to see if one has more tooth growth than the other. This is currently in a data set called \texttt{teeth}, which contains two variables: \texttt{aa}, the tooth length for guinea pigs in the ascorbic acid group and \texttt{oj} the tooth length for the orange juice group.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{attach}\NormalTok{(teeth)}
\end{Highlighting}
\end{Shaded}

To run a two-sample test comparing means in R, we continue to use the command \texttt{t.test}. The arguments we need in this case are:

\begin{itemize}
\tightlist
\item
  \texttt{x}: the first variable.
\item
  \texttt{y}: the other variable.
\item
  \texttt{mu}: the null value, usually \(\mu_1-\mu_2=0\).
\item
  \texttt{paired}: set this equal to \texttt{TRUE} for paired t tests; set it equal to \texttt{FALSE} for independent samples.
\item
  \texttt{conf.level}: the desired confidence level (\(1-\alpha\)).
\end{itemize}

In this case, we are interested in variables \texttt{x\ =\ aa} and \texttt{y\ =\ oj}. The null value is \texttt{mu\ =\ 0}. Guinea pigs were randomly assigned to each treatment group, so these are independent samples and \texttt{paired\ =\ FALSE}. Finally, we will go ahead and test this at a 0.05 level of significance, so \texttt{conf.level\ =\ 0.95}. Putting that all together, the R command looks like

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(}\AttributeTok{x =}\NormalTok{ aa, }\AttributeTok{y =}\NormalTok{ oj, }\AttributeTok{mu =} \DecValTok{0}\NormalTok{, }\AttributeTok{paired =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{conf.level =} \FloatTok{0.95}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Welch Two Sample t-test
## 
## data:  aa and oj
## t = -1.9153, df = 55.309, p-value = 0.06063
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -7.5710156  0.1710156
## sample estimates:
## mean of x mean of y 
##  16.96333  20.66333
\end{verbatim}

The R output shows (top to bottom)

\begin{itemize}
\tightlist
\item
  variables entered.
\item
  the test statistic, degrees of freedom, and p-value.
\item
  the alternative hypothesis.
\item
  a confidence interval for the difference of the two means.
\item
  sample means for each variable.
\end{itemize}

Based on the output, at the 0.05 level of significance, the data provide insufficient evidence to conclude that the mean tooth length for guinea pigs receiving ascorbic acid differs from the guinea pigs receiving orange juice (\(p = 0.061\) and the confidence interval includes 0).

\hypertarget{chi-square-tests}{%
\chapter{Chi-Square Tests}\label{chi-square-tests}}

\hypertarget{chapter-overview-9}{%
\section{Chapter Overview}\label{chapter-overview-9}}

In this chapter, we will continue our discussion on statistical inference with a discussion on hypothesis testing. In hypothesis testing, we take a more active approach to our data by asking questions about population parameters and developing a framework to answer those questions. We will root this discussion in confidence intervals before learning about several other approaches to hypothesis testing.

\textbf{Chapter Learning Outcomes/Objectives}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Perform and interpret inference for

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    a population variance.
  \item
    the ratio of two variances.
  \item
    tests of goodness of fit and contingency tables.
  \end{enumerate}
\end{enumerate}

This chapter's outcomes correspond to course outcomes (6) apply statistical inference techniques of parameter estimation such as point estimation and confidence interval estimation and (7) apply techniques of testing various statistical hypotheses concerning population parameters.

\hypertarget{inference-for-a-population-variance}{%
\section{Inference for a Population Variance}\label{inference-for-a-population-variance}}

Sometimes, it may be of interest to examine directly the variability of a population. Why? Suppose we have some medication that comes in a pill form. We know that each pill has an average of 10mg of active ingredient. For this medication to be consistently effective, we want to make sure that the amount of active ingredient does not \emph{vary} too much from one pill to the next. We examine this using tests for population variance.

\hypertarget{the-chi-square-distribution}{%
\subsection{The Chi-Square Distribution}\label{the-chi-square-distribution}}

\hypertarget{the-ratio-of-two-variances}{%
\section{The Ratio of Two Variances}\label{the-ratio-of-two-variances}}

\hypertarget{goodness-of-fit}{%
\section{Goodness of Fit}\label{goodness-of-fit}}

\hypertarget{contingency-tables}{%
\section{Contingency Tables}\label{contingency-tables}}

\hypertarget{anova}{%
\chapter{ANOVA}\label{anova}}

\hypertarget{chapter-overview-10}{%
\section{Chapter Overview}\label{chapter-overview-10}}

In this chapter, we extend the concepts from Chapter 6 to answer questions like ``is there a difference between these means?'' We will also consider hypothesis tests for whether a sample represents the population or closely matches a particular distribution.

\textbf{Chapter Learning Outcomes/Objectives}

Perform and interpret inference for

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Interpret an ANOVA.
\item
  Use the Bonferroni correction to conduct multiple comparisons.
\end{enumerate}

\hypertarget{what-is-the-analysis-of-variance-anova}{%
\section{What is the Analysis of Variance (ANOVA)}\label{what-is-the-analysis-of-variance-anova}}

Now that we've examined tests for one and two means, it's natural to wonder about three or more means. For example, we might want to compare three different medications: treatment 1 (\(t_1\)), treatment 2 (\(t_2\)), and treatment 3 (\(t_3\)). Based on what we've learned so far, we might think to do pairwise comparisons, examining \(t_1\) vs \(t_2\), then \(t_2\) vs \(t_3\), then \(t_1\) vs \(t_3\). Unfortunately, this tends to increase our Type I error!

Think of it this way: if I set my confidence level to 95\%, I'm setting my Type I error rate to \(\alpha=0.05\). In general terms, this means that about 1 out of every 20 times I run my experiment, I would make a type I error. If I went ahead and ran, say, 20 tests comparing two means, my \emph{overall} Type I error rate is going to increase - there's a pretty significant chance that at least one of those comparisons will results in a Type I error!

Instead, we will use a test that allows us to ask: ``Are all these means the same?'' This is called the \textbf{an}alysis \textbf{o}f \textbf{va}riance, or ANOVA.

\begin{itemize}
\tightlist
\item
  \(H_0\): The mean outcome is the same across all groups.
\item
  \(H_A\): At least one mean differs from the rest.
\end{itemize}

In statistical notation, these hypotheses look like:

\begin{itemize}
\tightlist
\item
  \(H_0: \mu_1 = \mu_2 = \dots = \mu_k\)
\item
  \(H_A: \mu_i \ne \mu_j\) for at least one pair \((i, j)\)
\end{itemize}

where \(k\) is the number of means being compared and the notation \(\mu_i\) represents the mean for the \(i\)th group (\(i\) can take on any whole number value between 1 and \(k\)).

For ANOVA, we have three key conditions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Observations are independent within and across groups.
\end{enumerate}

Independence within groups is the way we've been thinking about independence already. We want to convince ourselves that for any particular group, the observations do not impact each other. For independence across groups, we want to convince ourselves that the groups do not impact each other. Note: if we have a simple random sample, this assumption is always satisfied.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Data within each group are approximately normal.
\end{enumerate}

If you make a histogram of the data for each group, each histogram will look approximately bell-shaped.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Variability is approximately equal across groups.
\end{enumerate}

Take the standard deviation for each group and check if they are approximately equal. A boxplot is an appropriate way to do this visually.

\textbf{Why Variance?}

You may have seen the name ``analysis of variance'' and wondered what the variance has to do with comparing many means. Consider the following boxplots:

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-100-1.pdf}

Is there a difference in the means for Experiment 1? What about Experiment 2?

In fact, the means are \(\mu_1 = \mu_4 = 2\), \(\mu_2 = \mu_5 = 1\), and \(\mu_3 = \mu_6 = 0.5\). But the variances for the Experiment 1 groups are much larger than for the Experiment 2 groups! The larger variances in Experiment 1 obscure any differences between the group means. It is for this reason that we analyze variance as part of our test for differences in means.

\begin{quote}
Aside: Why can't we look at the data first and just test the two means that have the largest difference?

When we look at the data \emph{and then choose a test}, this inflates our Type I error rate! It's bad practice and not something we want to engage in as scientists.
\end{quote}

In order to perform an ANOVA, we need to consider whether the sample means differ more than we would expect them to based on natural variation (remember that we expect random samples to produce slightly different sample statistics each time!). This type of variation is called \textbf{mean square between groups} or \(MSG\). It has associated degrees of freedom \(df_G = k-1\) where \(k\) is the number of groups. Note that \[MSG = \frac{SSG}{df_G}\] where \(SSE\) is the \textbf{sum of squares group}. If the null hypothesis is true, variation in the sample means is due to chance. In this case, we would expect the MSG to be relatively small.

When I say ``relatively small'', I mean we need to compare this quantity to something. We need some quantity that will give us an idea of how much variability to expect if the null hypothesis is true. This is the \textbf{mean square error} or \(MSE\), which has degrees of freedom \(df_E = n-k\). Again, we have the relationship that \[MSE = \frac{SSE}{df_E}\] where \(SSE\) is the \textbf{sum of squares error}. These calculations are very similar to the calculation for variance (and standard deviation)! (Note: we will not calculate these quantities by hand, but if you are interested in the mathematical details they are available in the OpenIntro Statistics textbook in the footnote on page 289.)

We compare these two quantities by examining their ratio: \[F = \frac{MSG}{MSE}\] This is the test statistic for the ANOVA.

\hypertarget{the-f-distribution}{%
\section{The F-Distribution}\label{the-f-distribution}}

The \(\boldsymbol{F}\)\textbf{-test} relies on something called the \(F\) distribution. The \(F\) distribution has two parameters: \(df_1=df_G\) and \(df_1=df_E\). The \(F\) distribution always takes on positive values, so an \emph{extreme} or \emph{unusual} value for the \(F\) distribution will correspond to a large (positive) number.

When we run an ANOVA, we almost always use the p-value approach. If you are using \texttt{R} for your distributions, the command is \texttt{pf(F,\ df1,\ df2,\ lower.tail=FALSE)} where \texttt{F} is the test statistic.

\begin{quote}
\emph{Example:} Suppose I have a test with 100 observations and 5 groups. I find \(MSG = 0.041\) and \(MSE = 0.023\). Then \[df_G = k-1 = 5-1 = 4\] and \[df_E = n-k = 100-5 = 95\] The test statistic is \[f = \frac{0.041}{0.023} = 1.7826\] To find the p-value using \texttt{R}, I would write the command
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pf}\NormalTok{(}\FloatTok{1.7826}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{95}\NormalTok{, }\AttributeTok{lower.tail=}\ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1387132
\end{verbatim}

\begin{quote}
and find a p-value of 0.1387.
\end{quote}

Here is a nice F-distribution applet. For this applet, \(\nu_1 = df_1\) and \(\nu_2 = df_2\). Plug in your \(F\) test statistic where it indicates ``x ='' and your p=value will appear in the red box next to ``P(X\textgreater x)''. When you enter your degrees of freedom, a visualization will appear similar to those in the Rossman and Chance applets we used previously.

\textbf{The ANOVA Table}

Generally, when we run an ANOVA, we create an ANOVA table (or we have software create one for us!). This table looks something like this

\begin{longtable}[]{@{}llllll@{}}
\toprule()
& df & Sum of Squares & Mean Squares & F Value & P-Value \\
\midrule()
\endhead
group & \(df_G\) & \(SSG\) & \(MSG\) & \(F\) & p-value \\
error & \(df_E\) & \(SSE\) & \(MSE\) & & \\
\bottomrule()
\end{longtable}

\begin{quote}
\emph{Example:} chick weights

\texttt{R} has data on the weights of chicks fed six different feeds (diets). Assume these data are based on a random sample of chicks. There are \(n=71\) total observations and \(k=6\) different feeds. Let's assume we want to test with a 0.05 level of significance.

The ANOVA hypotheses are

\begin{itemize}
\tightlist
\item
  \(H_0\): the mean weight is the same for all six feeds.
\item
  \(H_A\): at least one feed has a mean weight that differs.
\end{itemize}

The summaries for these data are
\end{quote}

\begin{verbatim}
##         casein horsebean linseed meatmeal soybean sunflower
## n        12.00     10.00   12.00    11.00   14.00     12.00
## Mean    323.58    160.20  218.75   276.91  246.43    328.92
## Std Dev  64.43     38.63   52.24    64.90   54.13     48.84
\end{verbatim}

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-102-1.pdf} \includegraphics{IntroStats_files/figure-latex/unnamed-chunk-102-2.pdf}

\begin{quote}
The group sizes are relatively small, so it's difficult to determine how far from normality these data are based on the histograms. We may also run into some issues with constant variance. However, for the sake of the example, let's push ahead with the ANOVA! Since we usually use software to calculate ANOVAs, I've used \texttt{R} to create the following ANOVA table:
\end{quote}

\begin{verbatim}
## Analysis of Variance Table
## 
## Response: chickwts$weight
##               Df Sum Sq Mean Sq F value    Pr(>F)    
## chickwts$feed  5 231129   46226  15.365 5.936e-10 ***
## Residuals     65 195556    3009                      
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{quote}
From the table, we can confirm that \(df_G = 6-1 = 5\) and \(df_E = 71 - 6 = 65\). The F test statistic is \[MSG/MSE = 46226 / 3009 = 15.365\] Finally, the p-value is \(5.936\times10^{-10}\). Clearly \(5.936\times10^{-10} < \alpha = 0.05\), so we will reject the null hypothesis and conclude that at least one of the feed groups has a mean weight that differs.
\end{quote}

\hypertarget{multiple-comparisons-and-type-i-error-rate}{%
\section{Multiple Comparisons and Type I Error Rate}\label{multiple-comparisons-and-type-i-error-rate}}

Let's return for a moment to our ANOVA hypotheses:

\begin{itemize}
\tightlist
\item
  \(H_0\): The mean outcome is the same across all groups.
\item
  \(H_A\): At least one mean differs from the rest.
\end{itemize}

If we reject \(H_0\) and conclude that ``at least one mean differs from the rest'', how do we determine which mean(s) differ? \emph{If} we reject \(H_0\), we will perform a series of two-sample t-tests. But wait! What about the Type I error? Isn't this exactly what we decided we couldn't do when we introduced ANOVA?

In order to avoid this increased Type I error rate, we run these \textbf{mulitple comparisons} with a modified significance level. There are several ways to do this, but the most common way is with the \textbf{Bonferroni correction}. Here, if we want to test at the \(100(1-\alpha)\) level of significance, we run each of our pairwise comparisons with \[\alpha^* = \alpha/K\] where \(K\) is the number of comparisons being considered. For \(k\) groups, there are \[K = \frac{k(k-1)}{2}\] possible pairwise comparisons.

For these comparisons, we use a special pooled estimate of the standard deviation, \(s_{\text{pooled}}\) in place of \(s_1\) and \(s_2\): \[\text{standard error} = \sqrt{\frac{s_{\text{pooled}}^2}{n_1} + \frac{s_{\text{pooled}}^2}{n_2}}\] Other than changing \(\alpha\) to \(\alpha^*\) and the standard error to this new formula, the test is exactly the same as that discussed in the previous section. Note that \[s_{\text{pooled}} = \sqrt{MSE}\] and the degrees of freedom is \(df_E\).

\begin{quote}
\emph{Example}: chick weights

Let's extend our discussion on the chick weights to multiple comparisons. Since we were able to conclude that at least one feed has a weight that differs, we want to find out where the difference(s) lie!

We will test all possible pairwise comparisons. This will require \(K = \frac{6(6-1)}{2} = 15\) tests. The pooled standard deviation is \(s_{pooled} = \sqrt{3009} \approx 54.85\). Let's walk through the test of casein \((\bar{x}_1 = 323.58, n=12)\) vs horsebean \((\bar{x}_2 = 160.20, n=10)\):

\begin{itemize}
\tightlist
\item
  \(H_0: \mu_1 = \mu_2\)
\item
  \(H_A: \mu_1 \ne \mu_2\)
\end{itemize}

The estimated difference and standard error are \[\bar{x}_1 - \bar{x}_2 = 323.58 - 160.20 = 163.38 \quad\quad SE = \sqrt{\frac{54.85^2}{11}+\frac{54.85^2}{9}} = 25.65\] which results in a test statistic of \(t=6.37\) and a p-value of \(1.11\times10^{-8}\). We then compare this to \(\alpha^* = 0.05/15 = 0.0033\). Since the p-value of \(1.11\times10^{-8} < \alpha^* = 0.0033\), we reject the null hypothesis and conclude there is a significant difference in mean chick weight between the casein and horsebean feeds.

In order to complete the pairwise comparisons, we would then run the remaining 14 tests. I will leave this as an optional exercise for the particularly motivated student.
\end{quote}

Note: occasionally, we may reject \(H_0\) in the ANOVA but may fail to find any statistically significant differences when performing multiple comparisons with the Bonferroni correction. This is ok! It just means we were unable to identify which specific groups differ.

\hypertarget{appendices}{%
\chapter*{Appendices}\label{appendices}}
\addcontentsline{toc}{chapter}{Appendices}

\hypertarget{appendix-a-important-links-and-additional-resources}{%
\section*{Appendix A: Important Links and Additional Resources}\label{appendix-a-important-links-and-additional-resources}}
\addcontentsline{toc}{section}{Appendix A: Important Links and Additional Resources}

\hypertarget{applets}{%
\subsection*{Applets}\label{applets}}
\addcontentsline{toc}{subsection}{Applets}

\begin{itemize}
\tightlist
\item
  \href{https://lgpcappiello.shinyapps.io/normcalc/}{Normal Distribution Calculator}
\item
  \href{http://www.rossmanchance.com/applets/}{Rossman and Chance Applets}
\item
  \href{https://lgpcappiello.shinyapps.io/SimulateCLT/}{Simulating the Central Limit Theorem}
\end{itemize}

\hypertarget{run-r-online}{%
\subsection*{Run R Online}\label{run-r-online}}
\addcontentsline{toc}{subsection}{Run R Online}

\begin{itemize}
\tightlist
\item
  \href{https://rdrr.io/snippets/}{Run R Online}
\item
  \href{https://login.rstudio.cloud/}{RStudio Cloud}
\end{itemize}

\hypertarget{appendix-b-average-deviance}{%
\section*{Appendix B: Average Deviance}\label{appendix-b-average-deviance}}
\addcontentsline{toc}{section}{Appendix B: Average Deviance}

The deviance of an observation from its mean is \(x - \bar{x}\). We denote the deviation for the \(i\)th observation as \(x_i - \bar{x}\). So the sum over all \(n\) deviances is

\begin{align}
\text{Sum of Deviances} &= \Sigma_{i=1}^n (x_i - \bar{x}) \\
&= (x_1 - \bar{x}) + (x_2 - \bar{x}) + \dots + (x_{n-1} - \bar{x}) + (x_n - \bar{x}) \\
&= x_1 - \bar{x} + x_2 - \bar{x} + \dots + x_{n-1} - \bar{x} + x_n - \bar{x} \\
&= x_1 + x_2 + \dots + x_{n-1} + x_n - \bar{x} - \bar{x} - \dots - \bar{x} - \bar{x} \\
&= (x_1 + x_2 + \dots + x_{n-1} + x_n) - (\bar{x} + \bar{x} + \dots + \bar{x} + \bar{x})
\end{align}

where the first half is the sum over all of the \(x\) values and the term (\(\bar{x}\)) appears \(n\) times. So we can rewrite this as

\[
\text{Sum of Deviances} = \Sigma_{i=1}^n (x_i) - n\bar{x}
\]
Now notice that, because \(\bar{x} = \frac{\Sigma_{i=1}^n (x_i)}{n}\), we can multiply both sides by \(n\) to get \(n\bar{x} = \Sigma_{i=1}^n (x_i)\) and rewrite the sum over the deviances as

\begin{align}
\text{Sum of Deviances} &= n\bar{x} - n\bar{x} \\ &=0
\end{align}

\hypertarget{appendix-c-deriving-a-confidence-interval}{%
\section*{Appendix C: Deriving a Confidence Interval}\label{appendix-c-deriving-a-confidence-interval}}
\addcontentsline{toc}{section}{Appendix C: Deriving a Confidence Interval}

Assume we are taking a sample from a normal distribution with mean \(\mu\) and standard deviation \(\sigma\). We will assume the value of \(\sigma\) is known to us. Then \(\bar{X}\) is Normal(\(\mu, \sigma/\sqrt{n}\)). If we standardize \(\bar{X}\), we get \[Z = \frac{\bar{X}-\mu}{\sigma/\sqrt{n}}.\]

We want some interval \((a,b)\). We will start by considering \(a < Z < b\), so \(a < Z\) and \(Z < b\) (or \(b > Z\)). Then

\[
\begin{aligned}
Z &< b\\
\frac{\bar{X}-\mu}{\sigma/\sqrt{n}} &< b\\
\bar{X}-\mu &< b\sigma/\sqrt{n} \\
\bar{X}-b\sigma/\sqrt{n} &< \mu 
\end{aligned}
\]

and

\[
\begin{aligned}
a &< Z  \\
a &< \frac{\bar{X}-\mu}{\sigma/\sqrt{n}} \\
a\sigma/\sqrt{n} &< \bar{X}-\mu \\
\mu &< \bar{X}-a\sigma/\sqrt{n}
\end{aligned}
\]

putting these together, \[ \bar{X}-b\frac{\sigma}{\sqrt{n}} < \mu <  \bar{X}-a\frac{\sigma}{\sqrt{n}}.\] If we want to be 95\% confident, then we want \(P(a < Z < b)=0.95\): \[P\left(\bar{X}-b\frac{\sigma}{\sqrt{n}} < \mu <  \bar{X}-a\frac{\sigma}{\sqrt{n}}\right) = 0.95.\] To calculate the 95\% confidence interval, we need to find \(a\) and \(b\) such that \(P(a < Z < b)=0.95\).

We want this interval to be as narrow (small) as possible. Why? Narrower intervals are more informative. If I say I'm 95\% confident that tomorrow's high will be between -100 and 200 degrees Fahrenheit, that's a useless interval. If I change it to between 70 and 100, that's a little better. Changing it to between 85 and 90 is even better. This is what we mean by more informative.

It turns out that with a symmetric distribution like the normal distribution, the way to make a confidence interval as narrow as possible is to take advantage of this symmetry. Each of the plots below show a shaded area of 0.95. The narrowest interval (along the horizontal axis) is the first interval, which is shaded on \((-1.96 < Z < 1.96)\).

\includegraphics{IntroStats_files/figure-latex/unnamed-chunk-104-1.pdf}

Using the symmetry of the normal distribution, we find that the narrowest interval uses \(a = -1.96\) and \(b = 1.96\), which results in the 95\% confidence interval \[\left(\bar{x} - z_*\frac{\sigma}{\sqrt{n}}, \bar{x} + z_*\frac{\sigma}{\sqrt{n}}\right)\] where \(z_* = 1.96\).

  \bibliography{book.bib,packages.bib}

\end{document}
